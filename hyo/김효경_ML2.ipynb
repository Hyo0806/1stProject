{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41a43fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.text_cell_render.rendered_html{font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.text_cell_render.rendered_html{font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09745b67",
   "metadata": {},
   "source": [
    "# 1. 한식만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691ca872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2711\n",
      "[LightGBM] [Info] Number of data points in the train set: 88705, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2080467.000000\n",
      "Fold 1 MAE: 126,532\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2753\n",
      "[LightGBM] [Info] Number of data points in the train set: 177406, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2119460.000000\n",
      "Fold 2 MAE: 318,519\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 266107, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2140878.000000\n",
      "Fold 3 MAE: 222,968\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 354808, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108242.500000\n",
      "Fold 4 MAE: 219,465\n",
      "\n",
      "====================\n",
      "CV 평균 MAE: 221,871\n",
      "====================\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2847\n",
      "[LightGBM] [Info] Number of data points in the train set: 443509, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2107993.000000\n",
      "최종 모델 학습 완료\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# ===== (1) 날짜 컬럼 찾기/통일 =====\n",
    "date_col = \"DATE\" if \"DATE\" in df.columns else (\"TA_YMD\" if \"TA_YMD\" in df.columns else None)\n",
    "if date_col is None:\n",
    "    raise ValueError(\"날짜 컬럼이 DATE 또는 TA_YMD로 존재해야 합니다.\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "TARGET = \"AMT\"\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"타겟 컬럼 {TARGET} 이(가) 없습니다. 컬럼명 확인 필요.\")\n",
    "\n",
    "# ===== (2) DONG 문자열 처리 =====\n",
    "cat_cols = []\n",
    "if \"DONG\" in df.columns:\n",
    "    df[\"DONG\"] = df[\"DONG\"].astype(\"category\")\n",
    "    cat_cols.append(\"DONG\")\n",
    "\n",
    "# ===== (3) 피처 엔지니어링 =====\n",
    "def make_features(data):\n",
    "    d = data.copy()\n",
    "\n",
    "    # 날짜 파생 (datetime 제거 목적)\n",
    "    d[\"year\"] = d[date_col].dt.year\n",
    "    d[\"month\"] = d[date_col].dt.month\n",
    "    d[\"day\"] = d[date_col].dt.day\n",
    "    d[\"dow\"] = d[date_col].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # Lag / Rolling (AMT 기반)\n",
    "    for lag in [1, 7, 14]:\n",
    "        d[f\"lag_{lag}\"] = d[TARGET].shift(lag)\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        d[f\"roll_mean_{win}\"] = d[TARGET].shift(1).rolling(win).mean()\n",
    "        d[f\"roll_std_{win}\"] = d[TARGET].shift(1).rolling(win).std()\n",
    "\n",
    "    return d\n",
    "\n",
    "df = make_features(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# ===== (4) 학습용 X/y 구성 =====\n",
    "drop_cols = [TARGET, date_col]\n",
    "FEATURES = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# LightGBM이 category dtype을 처리할 수 있도록 유지\n",
    "# (단, 혹시 object 남아있으면 category로 바꿔줌)\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == \"object\":\n",
    "        X[c] = X[c].astype(\"category\")\n",
    "        if c not in cat_cols:\n",
    "            cat_cols.append(c)\n",
    "\n",
    "# ===== (5) TimeSeries CV =====\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "mae_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"l1\",\n",
    "        categorical_feature=cat_cols if len(cat_cols) > 0 else \"auto\",\n",
    "        callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    mae_scores.append(mae)\n",
    "    print(f\"Fold {fold+1} MAE: {mae:,.0f}\")\n",
    "\n",
    "print(\"\\n====================\")\n",
    "print(f\"CV 평균 MAE: {np.mean(mae_scores):,.0f}\")\n",
    "print(\"====================\")\n",
    "\n",
    "# ===== (6) 최종 모델 =====\n",
    "final_model = lgb.LGBMRegressor(\n",
    "    objective=\"regression_l1\",\n",
    "    n_estimators=int(np.mean([model.best_iteration_ for _ in range(1)]) or 1500),\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=0.3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "final_model.fit(X, y, categorical_feature=cat_cols if len(cat_cols) > 0 else \"auto\")\n",
    "\n",
    "print(\"최종 모델 학습 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8646e41",
   "metadata": {},
   "source": [
    "# 2. 배달, 한식, 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d211dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [han] 학습 시작 (mode=temp) =====\n",
      "[han] 핵심 날씨 컬럼 후보: ['TEMP']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2711\n",
      "[LightGBM] [Info] Number of data points in the train set: 88583, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2083101.000000\n",
      "[han] Fold 1 MAE: 121,904  (best_iter=3499)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2753\n",
      "[LightGBM] [Info] Number of data points in the train set: 177164, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2120076.500000\n",
      "[han] Fold 2 MAE: 297,366  (best_iter=3500)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 265745, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2141769.000000\n",
      "[han] Fold 3 MAE: 199,292  (best_iter=3500)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 354326, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108547.500000\n",
      "[han] Fold 4 MAE: 197,593  (best_iter=3500)\n",
      "[han] CV 평균 MAE: 204,039 / 최종 n_estimators=3499\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2847\n",
      "[LightGBM] [Info] Number of data points in the train set: 442907, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108401.000000\n",
      "✅ [han] 최고 성능 모델 저장 완료\n",
      " - model: data/models\\han_best_model.pkl\n",
      " - meta : data/models\\han_best_meta.pkl\n",
      "\n",
      "===== [delivery] 학습 시작 (mode=rain) =====\n",
      "[delivery] 핵심 날씨 컬럼 후보: ['RAIN']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2439\n",
      "[LightGBM] [Info] Number of data points in the train set: 7485, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 65015.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 1 MAE: 7,871  (best_iter=2586)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2475\n",
      "[LightGBM] [Info] Number of data points in the train set: 14966, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 68111.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 2 MAE: 6,819  (best_iter=3497)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2514\n",
      "[LightGBM] [Info] Number of data points in the train set: 22447, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 61920.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 3 MAE: 7,027  (best_iter=1969)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2523\n",
      "[LightGBM] [Info] Number of data points in the train set: 29928, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 58824.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 4 MAE: 5,838  (best_iter=3500)\n",
      "[delivery] CV 평균 MAE: 6,889 / 최종 n_estimators=2888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2562\n",
      "[LightGBM] [Info] Number of data points in the train set: 37409, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 56966.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [delivery] 최고 성능 모델 저장 완료\n",
      " - model: data/models\\delivery_best_model.pkl\n",
      " - meta : data/models\\delivery_best_meta.pkl\n",
      "\n",
      "===== 전체 완료 =====\n",
      "한식 CV MAE: 204,039\n",
      "배달 CV MAE: 6,889\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# 경로/설정\n",
    "# =========================\n",
    "HAN_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DELIVERY_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "OUT_DIR = \"data/models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "# =========================\n",
    "# 유틸: 날씨 컬럼 자동 탐지 (temp/rain)\n",
    "# =========================\n",
    "def find_weather_cols(df, mode: str):\n",
    "    cols = list(df.columns)\n",
    "    low = {c: str(c).lower() for c in cols}\n",
    "\n",
    "    if mode == \"temp\":\n",
    "        keys = [\"temp\", \"tavg\", \"tmean\", \"기온\", \"평균기온\", \"최고기온\", \"최저기온\"]\n",
    "    elif mode == \"rain\":\n",
    "        keys = [\"rain\", \"precip\", \"prcp\", \"강수\", \"강수량\", \"강우\", \"mm\"]\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'temp' or 'rain'\")\n",
    "\n",
    "    picked = []\n",
    "    for c in cols:\n",
    "        s = low[c]\n",
    "        if any(k in s for k in keys):\n",
    "            picked.append(c)\n",
    "\n",
    "    # 숫자형만 유지\n",
    "    picked = [c for c in picked if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return picked\n",
    "\n",
    "# =========================\n",
    "# 피처 생성\n",
    "#  - 날짜 파생(연/월/일/요일 등)\n",
    "#  - 동(DONG) 있으면 category로 사용\n",
    "#  - lag/rolling (동별이면 groupby 적용)\n",
    "# =========================\n",
    "def make_features(df: pd.DataFrame, group_col: str | None = \"DONG\"):\n",
    "    d = df.copy()\n",
    "\n",
    "    if DATE_COL not in d.columns:\n",
    "        raise ValueError(f\"날짜 컬럼 {DATE_COL} 없음\")\n",
    "    if TARGET not in d.columns:\n",
    "        raise ValueError(f\"타겟 컬럼 {TARGET} 없음\")\n",
    "\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # 날짜 파생 (datetime dtype 제거 목적)\n",
    "    d[\"year\"] = d[DATE_COL].dt.year\n",
    "    d[\"month\"] = d[DATE_COL].dt.month\n",
    "    d[\"day\"] = d[DATE_COL].dt.day\n",
    "    d[\"dow\"] = d[DATE_COL].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    cat_cols = []\n",
    "    if group_col and group_col in d.columns:\n",
    "        d[group_col] = d[group_col].astype(\"category\")\n",
    "        cat_cols.append(group_col)\n",
    "    else:\n",
    "        group_col = None\n",
    "\n",
    "    # Lag/Rolling\n",
    "    def add_lag_roll(x):\n",
    "        for lag in [1, 7, 14]:\n",
    "            x[f\"lag_{lag}\"] = x[TARGET].shift(lag)\n",
    "        for win in [7, 14]:\n",
    "            x[f\"roll_mean_{win}\"] = x[TARGET].shift(1).rolling(win).mean()\n",
    "            x[f\"roll_std_{win}\"] = x[TARGET].shift(1).rolling(win).std()\n",
    "        return x\n",
    "\n",
    "    if group_col:\n",
    "        d = d.groupby(group_col, group_keys=False).apply(add_lag_roll)\n",
    "    else:\n",
    "        d = add_lag_roll(d)\n",
    "\n",
    "    # object 남아있으면 category로 (LightGBM dtype 에러 방지)\n",
    "    for c in d.columns:\n",
    "        if d[c].dtype == \"object\" and c != TARGET:\n",
    "            d[c] = d[c].astype(\"category\")\n",
    "            if c not in cat_cols:\n",
    "                cat_cols.append(c)\n",
    "\n",
    "    return d, cat_cols, group_col\n",
    "\n",
    "# =========================\n",
    "# 학습 + 최고 성능 모델 저장\n",
    "#  - TS CV 4 folds\n",
    "#  - 각 fold의 best_iteration_ 평균으로 최종 모델 재학습\n",
    "#  - CV 평균 MAE가 더 낮으면 파일 덮어쓰기\n",
    "# =========================\n",
    "def train_and_save_best(df: pd.DataFrame, name: str, mode: str):\n",
    "    \"\"\"\n",
    "    name: 저장 파일 prefix (예: 'han', 'delivery')\n",
    "    mode: 'temp' or 'rain' (컬럼 자동 탐지용)\n",
    "    \"\"\"\n",
    "    weather_cols = find_weather_cols(df, mode=mode)\n",
    "\n",
    "    d, cat_cols, group_col = make_features(df, group_col=\"DONG\")\n",
    "\n",
    "    # 학습 피처: 타겟/날짜 제외 전부 (날씨 컬럼이 포함되어 있으면 자동 포함)\n",
    "    drop_cols = [TARGET, DATE_COL]\n",
    "    feature_cols = [c for c in d.columns if c not in drop_cols]\n",
    "\n",
    "    # NA 제거\n",
    "    X = d[feature_cols].copy()\n",
    "    y = d[TARGET].copy()\n",
    "    mask = ~X.isna().any(axis=1) & ~y.isna()\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "\n",
    "    # TS CV\n",
    "    tscv = TimeSeriesSplit(n_splits=4)\n",
    "    maes, best_iters = [], []\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=3500,          # early stopping으로 자동 컷\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== [{name}] 학습 시작 (mode={mode}) =====\")\n",
    "    if len(weather_cols) == 0:\n",
    "        print(f\"⚠️ [{name}] {mode} 관련 날씨 컬럼 자동 탐지 실패(그래도 학습은 진행).\")\n",
    "    else:\n",
    "        print(f\"[{name}] 핵심 날씨 컬럼 후보: {weather_cols}\")\n",
    "\n",
    "    for fold, (tr, va) in enumerate(tscv.split(X), start=1):\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X.iloc[tr], y.iloc[tr],\n",
    "            eval_set=[(X.iloc[va], y.iloc[va])],\n",
    "            eval_metric=\"l1\",\n",
    "            categorical_feature=cat_cols if len(cat_cols) else \"auto\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "        pred = model.predict(X.iloc[va])\n",
    "        mae = mean_absolute_error(y.iloc[va], pred)\n",
    "        maes.append(mae)\n",
    "        best_iters.append(model.best_iteration_)\n",
    "        print(f\"[{name}] Fold {fold} MAE: {mae:,.0f}  (best_iter={model.best_iteration_})\")\n",
    "\n",
    "    cv_mae = float(np.mean(maes))\n",
    "    final_estimators = int(np.clip(np.mean(best_iters), 300, 3500))\n",
    "    print(f\"[{name}] CV 평균 MAE: {cv_mae:,.0f} / 최종 n_estimators={final_estimators}\")\n",
    "\n",
    "    # 최종 모델 재학습\n",
    "    final_model = lgb.LGBMRegressor(**{**params, \"n_estimators\": final_estimators})\n",
    "    final_model.fit(X, y, categorical_feature=cat_cols if len(cat_cols) else \"auto\")\n",
    "\n",
    "    # 저장 경로\n",
    "    model_path = os.path.join(OUT_DIR, f\"{name}_best_model.pkl\")\n",
    "    meta_path  = os.path.join(OUT_DIR, f\"{name}_best_meta.pkl\")\n",
    "\n",
    "    # 기존 저장 모델이 있으면 성능 비교 후 더 좋을 때만 덮어쓰기\n",
    "    should_save = True\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            old_meta = joblib.load(meta_path)\n",
    "            old_mae = float(old_meta.get(\"cv_mae\", np.inf))\n",
    "            if cv_mae >= old_mae:\n",
    "                should_save = False\n",
    "                print(f\"[{name}] 기존 모델이 더 좋거나 동일함: old_CV_MAE={old_mae:,.0f} <= new_CV_MAE={cv_mae:,.0f}\")\n",
    "        except Exception:\n",
    "            # 메타가 깨져있으면 새로 저장\n",
    "            should_save = True\n",
    "\n",
    "    if should_save:\n",
    "        joblib.dump(final_model, model_path)\n",
    "        meta = {\n",
    "            \"cv_mae\": cv_mae,\n",
    "            \"date_col\": DATE_COL,\n",
    "            \"target_col\": TARGET,\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"cat_cols\": cat_cols,\n",
    "            \"group_col\": group_col,\n",
    "            \"mode\": mode,\n",
    "            \"weather_cols_detected\": weather_cols,\n",
    "            \"n_estimators\": final_estimators,\n",
    "        }\n",
    "        joblib.dump(meta, meta_path)\n",
    "        print(f\"✅ [{name}] 최고 성능 모델 저장 완료\")\n",
    "        print(f\" - model: {model_path}\")\n",
    "        print(f\" - meta : {meta_path}\")\n",
    "\n",
    "    return final_model, cv_mae\n",
    "\n",
    "# =========================\n",
    "# 실행\n",
    "# =========================\n",
    "han_df = pd.read_csv(HAN_PATH)\n",
    "del_df = pd.read_csv(DELIVERY_PATH)\n",
    "\n",
    "# 한식 = 기온 중심\n",
    "han_model, han_cv_mae = train_and_save_best(han_df, name=\"han\", mode=\"temp\")\n",
    "\n",
    "# 배달 = 강수 중심\n",
    "del_model, del_cv_mae = train_and_save_best(del_df, name=\"delivery\", mode=\"rain\")\n",
    "\n",
    "print(\"\\n===== 전체 완료 =====\")\n",
    "print(f\"한식 CV MAE: {han_cv_mae:,.0f}\")\n",
    "print(f\"배달 CV MAE: {del_cv_mae:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944adc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 평가: data/models/han_best_model.pkl ====\n",
      "Samples: train=354,325 / test=88,582\n",
      "MAE  : 151,157\n",
      "RMSE : 1,679,193\n",
      "R2   : 0.9814\n",
      "WMAPE: 2.8072%\n",
      "\n",
      "[동별 MAE TOP 10 (좋은 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 권선구 세류1동  1444.0   5575.550229  0.071540\n",
      "수원시 권선구 세류3동  2065.0  13811.382568  0.010179\n",
      "수원시 영통구 매탄4동  2043.0  13914.801772  0.010554\n",
      "수원시 장안구 정자1동  2028.0  14649.504522  0.008968\n",
      "수원시 장안구 조원2동  1470.0  15033.683022  0.014717\n",
      "수원시 팔달구 화서1동  2055.0  17188.697282  0.009409\n",
      "수원시 권선구 입북동   1865.0  21462.478350  0.015297\n",
      "수원시 팔달구 매교동   2039.0  21764.044537  0.024374\n",
      "수원시 팔달구 고등동   2056.0  22144.977556  0.010232\n",
      "수원시 권선구 권선2동  2062.0  22380.597920  0.011487\n",
      "\n",
      "[동별 MAE WORST 10 (나쁜 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 영통구 매탄2동  2028.0  9.722877e+04  0.063856\n",
      "수원시 권선구 권선1동  2070.0  9.830294e+04  0.015604\n",
      "수원시 영통구 매탄3동  2060.0  1.079290e+05  0.015677\n",
      "수원시 영통구 광교1동  2059.0  1.771252e+05  0.016428\n",
      "수원시 영통구 원천동   2059.0  2.036054e+05  0.025479\n",
      "수원시 팔달구 행궁동   2060.0  2.375625e+05  0.020081\n",
      "수원시 영통구 영통2동  2060.0  2.384573e+05  0.029192\n",
      "수원시 영통구 영통3동  2060.0  2.391902e+05  0.019575\n",
      "수원시 팔달구 인계동   2060.0  6.738748e+05  0.021521\n",
      "수원시 팔달구 매산동   2060.0  3.267689e+06  0.072903\n",
      "\n",
      "==== 평가: data/models/delivery_best_model.pkl ====\n",
      "Samples: train=29,927 / test=7,482\n",
      "MAE  : 3,922\n",
      "RMSE : 57,061\n",
      "R2   : 0.9382\n",
      "WMAPE: 3.1086%\n",
      "\n",
      "[동별 MAE TOP 10 (좋은 순)]\n",
      "                  n         mae     wmape\n",
      "DONG                                     \n",
      "수원시 장안구 파장동    53.0   68.988621  0.004048\n",
      "수원시 장안구 조원2동   22.0   91.533046  0.002397\n",
      "수원시 권선구 곡선동   123.0  128.249513  0.004408\n",
      "수원시 영통구 매탄2동   32.0  145.901346  0.003445\n",
      "수원시 팔달구 매교동   145.0  163.913706  0.006012\n",
      "수원시 영통구 망포1동  889.0  219.821207  0.007710\n",
      "수원시 권선구 권선1동    4.0  226.300932  0.025989\n",
      "수원시 장안구 영화동   110.0  334.305410  0.006539\n",
      "수원시 권선구 권선2동   41.0  410.547076  0.009164\n",
      "수원시 장안구 정자1동  116.0  526.441465  0.012804\n",
      "\n",
      "[동별 MAE WORST 10 (나쁜 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 장안구 정자3동   201.0   1298.538403  0.008402\n",
      "수원시 팔달구 인계동   1171.0   1573.279183  0.016786\n",
      "수원시 권선구 세류3동   563.0   1678.198905  0.029074\n",
      "수원시 권선구 금곡동    325.0   2746.872868  0.012294\n",
      "수원시 권선구 호매실동    38.0   2850.075530  0.018004\n",
      "수원시 영통구 영통3동  1151.0   3565.511468  0.024360\n",
      "수원시 영통구 광교1동   384.0   5231.107485  0.017843\n",
      "수원시 영통구 광교2동   393.0   6446.216827  0.037606\n",
      "수원시 권선구 구운동    599.0   6690.896536  0.164116\n",
      "수원시 영통구 매탄3동  1049.0  11630.545347  0.045529\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# =====================\n",
    "# 경로\n",
    "# =====================\n",
    "HAN_MODEL_PATH = r\"data/models/han_best_model.pkl\"\n",
    "HAN_META_PATH  = r\"data/models/han_best_meta.pkl\"\n",
    "HAN_DATA_PATH  = r\"data/수원시 한식 데이터백업.csv\"   # 네 환경 경로로 맞춰\n",
    "\n",
    "DEL_MODEL_PATH = r\"data/models/delivery_best_model.pkl\"\n",
    "DEL_META_PATH  = r\"data/models/delivery_best_meta.pkl\"\n",
    "DEL_DATA_PATH  = r\"data/수원시 배달 데이터백업.csv\"  # 네 환경 경로로 맞춰\n",
    "\n",
    "# =====================\n",
    "# 공통: 피처 생성 (학습 코드와 동일해야 함)\n",
    "# =====================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "def make_features(df: pd.DataFrame, group_col: str | None = \"DONG\"):\n",
    "    d = df.copy()\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    d[\"year\"] = d[DATE_COL].dt.year\n",
    "    d[\"month\"] = d[DATE_COL].dt.month\n",
    "    d[\"day\"] = d[DATE_COL].dt.day\n",
    "    d[\"dow\"] = d[DATE_COL].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    cat_cols = []\n",
    "    if group_col and group_col in d.columns:\n",
    "        d[group_col] = d[group_col].astype(\"category\")\n",
    "        cat_cols.append(group_col)\n",
    "    else:\n",
    "        group_col = None\n",
    "\n",
    "    def add_lag_roll(x):\n",
    "        for lag in [1, 7, 14]:\n",
    "            x[f\"lag_{lag}\"] = x[TARGET].shift(lag)\n",
    "        for win in [7, 14]:\n",
    "            x[f\"roll_mean_{win}\"] = x[TARGET].shift(1).rolling(win).mean()\n",
    "            x[f\"roll_std_{win}\"] = x[TARGET].shift(1).rolling(win).std()\n",
    "        return x\n",
    "\n",
    "    if group_col:\n",
    "        d = d.groupby(group_col, group_keys=False).apply(add_lag_roll)\n",
    "    else:\n",
    "        d = add_lag_roll(d)\n",
    "\n",
    "    # object -> category (LightGBM dtype 방지)\n",
    "    for c in d.columns:\n",
    "        if d[c].dtype == \"object\" and c not in [TARGET]:\n",
    "            d[c] = d[c].astype(\"category\")\n",
    "            if c not in cat_cols:\n",
    "                cat_cols.append(c)\n",
    "\n",
    "    return d\n",
    "\n",
    "def wmape(y_true, y_pred, eps=1e-9):\n",
    "    denom = np.sum(np.abs(y_true)) + eps\n",
    "    return np.sum(np.abs(y_true - y_pred)) / denom\n",
    "\n",
    "# =====================\n",
    "# 평가 함수\n",
    "# =====================\n",
    "def evaluate_saved_model(model_path, meta_path, data_path, holdout_ratio=0.2, print_by_dong=True):\n",
    "    model = joblib.load(model_path)\n",
    "    meta = joblib.load(meta_path)\n",
    "\n",
    "    feature_cols = meta[\"feature_cols\"]\n",
    "    cat_cols = meta.get(\"cat_cols\", [])\n",
    "    group_col = meta.get(\"group_col\", None)\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    d = make_features(df, group_col=group_col)\n",
    "\n",
    "    # 학습 때처럼 NA 제거\n",
    "    X = d[feature_cols].copy()\n",
    "    y = d[TARGET].copy()\n",
    "    mask = ~X.isna().any(axis=1) & ~y.isna()\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "\n",
    "    # 시계열 holdout: 뒤쪽 20%를 test로\n",
    "    n = len(X)\n",
    "    split = int(n * (1 - holdout_ratio))\n",
    "    X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "    # 예측\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    # 지표\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    w = wmape(y_test.values, pred)\n",
    "\n",
    "    print(f\"\\n==== 평가: {model_path} ====\")\n",
    "    print(f\"Samples: train={len(X_train):,} / test={len(X_test):,}\")\n",
    "    print(f\"MAE  : {mae:,.0f}\")\n",
    "    print(f\"RMSE : {rmse:,.0f}\")\n",
    "    print(f\"R2   : {r2:.4f}\")\n",
    "    print(f\"WMAPE: {w:.4%}\")\n",
    "\n",
    "        # (선택) 동별 MAE\n",
    "    if print_by_dong and group_col and group_col in d.columns:\n",
    "        d_masked = d.loc[mask].reset_index(drop=True)\n",
    "        d_test = d_masked.iloc[split:].copy()\n",
    "        d_test[\"pred\"] = pred\n",
    "\n",
    "        # 샘플 수가 1 이상인 동만\n",
    "        grp = d_test.groupby(group_col, observed=True)\n",
    "        stats = grp.apply(lambda g: pd.Series({\n",
    "            \"n\": len(g),\n",
    "            \"mae\": mean_absolute_error(g[TARGET].values, g[\"pred\"].values) if len(g) > 0 else np.nan,\n",
    "            \"wmape\": (np.sum(np.abs(g[TARGET].values - g[\"pred\"].values)) / (np.sum(np.abs(g[TARGET].values)) + 1e-9)) if len(g) > 0 else np.nan\n",
    "        }))\n",
    "        stats = stats.dropna().sort_values(\"mae\")\n",
    "\n",
    "        if len(stats) == 0:\n",
    "            print(\"\\n[동별 MAE] 테스트 구간에 유효한 동 데이터가 없습니다. (표본 부족)\")\n",
    "        else:\n",
    "            print(\"\\n[동별 MAE TOP 10 (좋은 순)]\")\n",
    "            print(stats.head(10))\n",
    "            print(\"\\n[동별 MAE WORST 10 (나쁜 순)]\")\n",
    "            print(stats.tail(10))\n",
    "\n",
    "# =====================\n",
    "# 실행\n",
    "# =====================\n",
    "han_res = evaluate_saved_model(HAN_MODEL_PATH, HAN_META_PATH, HAN_DATA_PATH)\n",
    "del_res = evaluate_saved_model(DEL_MODEL_PATH, DEL_META_PATH, DEL_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ca233",
   "metadata": {},
   "source": [
    "# 3. 매출 데이터 바탕으로 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8057c",
   "metadata": {},
   "source": [
    "## 점유율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44cd0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =====================================================\n",
    "# 상수 / 경로\n",
    "# =====================================================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "MY_DATE_COL = \"ta_ymd\"\n",
    "MY_AMT_COL = \"store_amt\"\n",
    "\n",
    "GRID_XLSX_PATH = \"data/수원시 격자.xlsx\"\n",
    "\n",
    "HAN_DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DEL_DATA_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "HAN_MODEL_PATH = \"data/models/han_best_model.pkl\"\n",
    "DEL_MODEL_PATH = \"data/models/delivery_best_model.pkl\"\n",
    "\n",
    "HAN_META_PATH = \"data/models/han_best_meta.pkl\"\n",
    "DEL_META_PATH = \"data/models/delivery_best_meta.pkl\"\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 0) 수원시 격자 로드\n",
    "# =====================================================\n",
    "def load_suwon_grid_map(xlsx_path=GRID_XLSX_PATH):\n",
    "    df = pd.read_excel(xlsx_path, header=None)\n",
    "    df = df.dropna(subset=[0, 1, 2])\n",
    "\n",
    "    grid = {}\n",
    "    for _, r in df.iterrows():\n",
    "        try:\n",
    "            grid[str(r[0]).strip()] = (int(r[1]), int(r[2]))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not grid:\n",
    "        raise ValueError(\"격자 엑셀에서 nx, ny를 읽지 못했습니다.\")\n",
    "    return grid\n",
    "\n",
    "\n",
    "def resolve_nxny_from_input_dong(dong, grid_map):\n",
    "    if dong not in grid_map:\n",
    "        raise ValueError(f\"'{dong}'이(가) 격자 엑셀에 없습니다.\")\n",
    "    return grid_map[dong][0], grid_map[dong][1]\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 1) 기상청 단기예보 + fallback (과거 평균 날씨)\n",
    "# =====================================================\n",
    "def _parse_pcp(v):\n",
    "    if v in [None, \"강수없음\", \"-\", \"없음\"]:\n",
    "        return 0.0\n",
    "    s = str(v).replace(\"mm\", \"\").strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        nums = re.findall(r\"[\\d.]+\", s)\n",
    "        return float(nums[0]) if nums else 0.0\n",
    "\n",
    "\n",
    "def fetch_daily_weather_kma(\n",
    "    nx, ny, target_date, RAIN_ID, base_df, dong\n",
    "):\n",
    "    \"\"\"\n",
    "    1) 기상청 단기예보 가능 → 사용\n",
    "    2) NO_DATA / 과거·미래 → 과거 평균 날씨 대체\n",
    "    \"\"\"\n",
    "\n",
    "    d = datetime.strptime(target_date, \"%Y-%m-%d\").date()\n",
    "    base_date = (d - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": RAIN_ID,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"numOfRows\": 3000,\n",
    "        \"pageNo\": 1,\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": \"2300\",\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        js = r.json()\n",
    "        body = js.get(\"response\", {}).get(\"body\")\n",
    "\n",
    "        if body:\n",
    "            items = body.get(\"items\", {}).get(\"item\", [])\n",
    "            want = d.strftime(\"%Y%m%d\")\n",
    "\n",
    "            temps, rains = [], []\n",
    "            for it in items:\n",
    "                if it.get(\"fcstDate\") != want:\n",
    "                    continue\n",
    "                if it.get(\"category\") == \"TMP\":\n",
    "                    temps.append(float(it.get(\"fcstValue\")))\n",
    "                elif it.get(\"category\") == \"PCP\":\n",
    "                    rains.append(_parse_pcp(it.get(\"fcstValue\")))\n",
    "\n",
    "            if temps:\n",
    "                return {\n",
    "                    \"temp_mean\": float(np.mean(temps)),\n",
    "                    \"rain_mean\": float(np.mean(rains)) if rains else 0.0,\n",
    "                    \"rain_peak\": float(np.max(rains)) if rains else 0.0,\n",
    "                    \"source\": \"기상청 단기예보\",\n",
    "                }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # =============================\n",
    "    # fallback: 과거 평균 날씨\n",
    "    # =============================\n",
    "    hist = base_df.copy()\n",
    "    hist[DATE_COL] = pd.to_datetime(hist[DATE_COL])\n",
    "    hist = hist[hist[\"DONG\"] == dong]\n",
    "\n",
    "    temp_cols = [c for c in hist.columns if \"temp\" in c.lower()]\n",
    "    rain_cols = [c for c in hist.columns if \"rain\" in c.lower()]\n",
    "\n",
    "    return {\n",
    "        \"temp_mean\": float(hist[temp_cols[0]].mean()) if temp_cols else np.nan,\n",
    "        \"rain_mean\": float(hist[rain_cols[0]].mean()) if rain_cols else 0.0,\n",
    "        \"rain_peak\": float(hist[rain_cols[0]].max()) if rain_cols else 0.0,\n",
    "        \"source\": \"과거 평균 날씨(대체)\",\n",
    "    }\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2) 공휴일 (천문연구원)\n",
    "# =====================================================\n",
    "def fetch_holiday_flag_kasi(target_date, HOLIDAY_ID):\n",
    "    d = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    url = \"https://apis.data.go.kr/B090041/openapi/service/SpcdeInfoService/getRestDeInfo\"\n",
    "    params = {\n",
    "        \"serviceKey\": HOLIDAY_ID,\n",
    "        \"solYear\": d.year,\n",
    "        \"solMonth\": f\"{d.month:02d}\",\n",
    "        \"_type\": \"json\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        items = (\n",
    "            r.json()\n",
    "            .get(\"response\", {})\n",
    "            .get(\"body\", {})\n",
    "            .get(\"items\", {})\n",
    "            .get(\"item\", [])\n",
    "        )\n",
    "\n",
    "        if isinstance(items, dict):\n",
    "            items = [items]\n",
    "\n",
    "        for it in items:\n",
    "            if str(it.get(\"locdate\")) == d.strftime(\"%Y%m%d\"):\n",
    "                return {\"is_holiday\": 1}\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return {\"is_holiday\": 0}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3) 예측용 feature 생성 (❗ DONG 제거됨)\n",
    "# =====================================================\n",
    "def build_feature_row_for_prediction(\n",
    "    base_df, feature_cols, dong, target_date, weather, is_holiday\n",
    "):\n",
    "    d = base_df.copy()\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d[d[\"DONG\"] == dong].sort_values(DATE_COL)\n",
    "\n",
    "    td = pd.to_datetime(target_date)\n",
    "    hist = d[d[DATE_COL] < td]\n",
    "\n",
    "    row = {\n",
    "        \"year\": td.year,\n",
    "        \"month\": td.month,\n",
    "        \"day\": td.day,\n",
    "        \"dow\": td.weekday(),\n",
    "        \"weekend\": int(td.weekday() >= 5),\n",
    "    }\n",
    "\n",
    "    for lag in [1, 7, 14]:\n",
    "        row[f\"lag_{lag}\"] = hist[TARGET].iloc[-lag] if len(hist) >= lag else np.nan\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        row[f\"roll_mean_{win}\"] = hist[TARGET].tail(win).mean() if len(hist) >= win else np.nan\n",
    "        row[f\"roll_std_{win}\"] = hist[TARGET].tail(win).std() if len(hist) >= win else np.nan\n",
    "\n",
    "    row[\"temp_mean\"] = weather[\"temp_mean\"]\n",
    "    row[\"rain_mean\"] = weather[\"rain_mean\"]\n",
    "    row[\"rain_peak\"] = weather[\"rain_peak\"]\n",
    "    row[\"is_holiday\"] = is_holiday\n",
    "\n",
    "    return pd.DataFrame([{c: row.get(c, np.nan) for c in feature_cols}])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4) 점유율 + 보정계수\n",
    "# =====================================================\n",
    "def compute_store_share_and_adj(\n",
    "    my_store_df, base_df, dong, target_date, lookback_days=60\n",
    "):\n",
    "    d0 = datetime.strptime(target_date, \"%Y-%m-%d\").date()\n",
    "    start = d0 - timedelta(days=lookback_days)\n",
    "\n",
    "    ms = my_store_df.copy()\n",
    "    ms[MY_DATE_COL] = pd.to_datetime(ms[MY_DATE_COL])\n",
    "    store_amt_60 = ms[\n",
    "        (ms[MY_DATE_COL].dt.date >= start) &\n",
    "        (ms[MY_DATE_COL].dt.date < d0)\n",
    "    ][MY_AMT_COL].sum()\n",
    "\n",
    "    bd = base_df.copy()\n",
    "    bd[DATE_COL] = pd.to_datetime(bd[DATE_COL])\n",
    "    dong_amt_60 = bd[\n",
    "        (bd[\"DONG\"] == dong) &\n",
    "        (bd[DATE_COL].dt.date >= start) &\n",
    "        (bd[DATE_COL].dt.date < d0)\n",
    "    ][TARGET].sum()\n",
    "\n",
    "    share = (store_amt_60 / dong_amt_60 * 100) if dong_amt_60 > 0 else 0.0\n",
    "    return share, 1.0\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5) 거래건수 추정\n",
    "# =====================================================\n",
    "def estimate_txn_count(my_store_df, target_date, pred_store_amt):\n",
    "    pseudo_ticket = max(15000.0, pred_store_amt / 20.0)\n",
    "    return int(pred_store_amt / pseudo_ticket)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6) 메인 함수\n",
    "# =====================================================\n",
    "def predict_my_store_sales(\n",
    "    my_store_csv,\n",
    "    dong,\n",
    "    date_str,\n",
    "    service_type,\n",
    "    RAIN_ID,\n",
    "    HOLIDAY_ID,\n",
    "    grid_xlsx_path=GRID_XLSX_PATH,\n",
    "):\n",
    "    if service_type == \"delivery\":\n",
    "        model_path, meta_path, data_path = (\n",
    "            DEL_MODEL_PATH, DEL_META_PATH, DEL_DATA_PATH\n",
    "        )\n",
    "    else:\n",
    "        model_path, meta_path, data_path = (\n",
    "            HAN_MODEL_PATH, HAN_META_PATH, HAN_DATA_PATH\n",
    "        )\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    meta = joblib.load(meta_path)\n",
    "\n",
    "    # ❗ DONG 제거\n",
    "    feature_cols = [c for c in meta[\"feature_cols\"] if c != \"DONG\"]\n",
    "\n",
    "    base_df = pd.read_csv(data_path)\n",
    "    my_store_df = pd.read_csv(my_store_csv)\n",
    "\n",
    "    grid_map = load_suwon_grid_map(grid_xlsx_path)\n",
    "    nx, ny = resolve_nxny_from_input_dong(dong, grid_map)\n",
    "\n",
    "    weather = fetch_daily_weather_kma(\n",
    "        nx, ny, date_str, RAIN_ID, base_df, dong\n",
    "    )\n",
    "    holi = fetch_holiday_flag_kasi(date_str, HOLIDAY_ID)\n",
    "\n",
    "    X_pred = build_feature_row_for_prediction(\n",
    "        base_df, feature_cols, dong, date_str, weather, holi[\"is_holiday\"]\n",
    "    )\n",
    "\n",
    "    pred_dong_amt = float(model.predict(X_pred)[0])\n",
    "    share, adj = compute_store_share_and_adj(\n",
    "        my_store_df, base_df, dong, date_str\n",
    "    )\n",
    "\n",
    "    pred_store_amt = pred_dong_amt * (share / 100.0) * adj\n",
    "    pred_cnt = estimate_txn_count(my_store_df, date_str, pred_store_amt)\n",
    "\n",
    "    # =========================\n",
    "    # 출력\n",
    "    # =========================\n",
    "    print(f\"📍지역(동): {dong}\")\n",
    "    print(f\"📅날짜: {date_str}\")\n",
    "    print(f\"🌦️날씨(출처): {weather['source']}\")\n",
    "    print(f\"   - 평균기온: {weather['temp_mean']:.1f}°C\")\n",
    "    print(f\"   - 강수(평균/피크): {weather['rain_mean']:.2f} / {weather['rain_peak']:.2f} mm\")\n",
    "    print(f\"👥예상 매출건수: {pred_cnt}건\")\n",
    "    print(f\"💰예상 동 전체 일매출: {pred_dong_amt:,.0f} 원\")\n",
    "    print(f\"🏪가게 점유율(최근 60일): {share:.2f}%\")\n",
    "    print(f\"🏪예상 가게 일매출: {pred_store_amt:,.0f} 원\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e431b73",
   "metadata": {},
   "source": [
    "## 사용예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "687af6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍지역(동): 행궁동\n",
      "📅날짜: 2025-12-30\n",
      "🌦️날씨(출처): 과거 평균 날씨(대체)\n",
      "   - 평균기온: 13.4°C\n",
      "   - 강수(평균/피크): 0.16 / 37.30 mm\n",
      "👥예상 매출건수: 0건\n",
      "💰예상 동 전체 일매출: 84,208 원\n",
      "🏪가게 점유율(최근 60일): 0.00%\n",
      "🏪예상 가게 일매출: 0 원\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "holiday_key_env = os.getenv(\"HOLIDAY_ID\")\n",
    "rain_key_env = os.getenv(\"RAIN_ID\")\n",
    "\n",
    "predict_my_store_sales(\n",
    "    my_store_csv=\"data/my_store.csv\",\n",
    "    dong=\"행궁동\",\n",
    "    date_str=\"2025-12-30\",\n",
    "    service_type=\"han\",\n",
    "    RAIN_ID=rain_key_env,\n",
    "    HOLIDAY_ID=holiday_key_env,\n",
    "    grid_xlsx_path=\"data/수원시 격자.xlsx\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528355d",
   "metadata": {},
   "source": [
    "# 4. 모델 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b67ea58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAIN START: han =====\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2760\n",
      "[LightGBM] [Info] Number of data points in the train set: 355094, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 2108030.000000\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 210080\n",
      "✅ VALID MAE = 210,080.03\n",
      "💾 model saved: data/models/han_best_model.pkl\n",
      "💾 meta  saved: data/models/han_best_meta.pkl\n",
      "\n",
      "===== TRAIN START: delivery =====\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2500\n",
      "[LightGBM] [Info] Number of data points in the train set: 30234, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 58824.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4998]\tvalid_0's l1: 4223.44\n",
      "✅ VALID MAE = 4,223.44\n",
      "💾 model saved: data/models/delivery_best_model.pkl\n",
      "💾 meta  saved: data/models/delivery_best_meta.pkl\n",
      "\n",
      "===== DONE =====\n",
      "HAN MAE     : 210,080.03\n",
      "DELIVERY MAE: 4,223.44\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import timedelta\n",
    "\n",
    "# =====================================================\n",
    "# 설정\n",
    "# =====================================================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "HAN_DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DEL_DATA_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "OUT_DIR = \"data/models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# 공통 전처리\n",
    "# =====================================================\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "    df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # ---------- 날짜 파생 ----------\n",
    "    df[\"year\"] = df[DATE_COL].dt.year\n",
    "    df[\"month\"] = df[DATE_COL].dt.month\n",
    "    df[\"day\"] = df[DATE_COL].dt.day\n",
    "    df[\"dow\"] = df[DATE_COL].dt.weekday\n",
    "    df[\"weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # ---------- DONG 제거 ----------\n",
    "    if \"DONG\" in df.columns:\n",
    "        df = df.drop(columns=[\"DONG\"])\n",
    "\n",
    "    # ---------- lag / rolling ----------\n",
    "    for lag in [1, 7, 14]:\n",
    "        df[f\"lag_{lag}\"] = df[TARGET].shift(lag)\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        df[f\"roll_mean_{win}\"] = df[TARGET].rolling(win).mean()\n",
    "        df[f\"roll_std_{win}\"] = df[TARGET].rolling(win).std()\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 학습 함수\n",
    "# =====================================================\n",
    "def train_and_save(\n",
    "    data_path: str,\n",
    "    model_name: str,\n",
    "):\n",
    "    print(f\"\\n===== TRAIN START: {model_name} =====\")\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = preprocess(df)\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in [DATE_COL, TARGET]]\n",
    "\n",
    "    # ---------- time split ----------\n",
    "    split_date = df[DATE_COL].quantile(0.8)\n",
    "    train_df = df[df[DATE_COL] <= split_date]\n",
    "    valid_df = df[df[DATE_COL] > split_date]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[TARGET]\n",
    "    X_val = valid_df[feature_cols]\n",
    "    y_val = valid_df[TARGET]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=30,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"l1\",\n",
    "        callbacks=[lgb.early_stopping(300, verbose=True)],\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "\n",
    "    print(f\"✅ VALID MAE = {mae:,.2f}\")\n",
    "\n",
    "    # ---------- 저장 ----------\n",
    "    model_path = f\"{OUT_DIR}/{model_name}_best_model.pkl\"\n",
    "    meta_path = f\"{OUT_DIR}/{model_name}_best_meta.pkl\"\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(\n",
    "        {\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"valid_mae\": mae,\n",
    "            \"n_train\": len(train_df),\n",
    "            \"n_valid\": len(valid_df),\n",
    "        },\n",
    "        meta_path,\n",
    "    )\n",
    "\n",
    "    print(f\"💾 model saved: {model_path}\")\n",
    "    print(f\"💾 meta  saved: {meta_path}\")\n",
    "\n",
    "    return mae\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 실행\n",
    "# =====================================================\n",
    "if __name__ == \"__main__\":\n",
    "    han_mae = train_and_save(HAN_DATA_PATH, \"han\")\n",
    "    del_mae = train_and_save(DEL_DATA_PATH, \"delivery\")\n",
    "\n",
    "    print(\"\\n===== DONE =====\")\n",
    "    print(f\"HAN MAE     : {han_mae:,.2f}\")\n",
    "    print(f\"DELIVERY MAE: {del_mae:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d48c8",
   "metadata": {},
   "source": [
    "# 5. 정님 모델 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c4e03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] XGB 사용 여부: True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model\n",
      "\n",
      "[DEBUG] >>> 학습 루프 시작 직전\n",
      "\n",
      "[DEBUG] >>> HOUR=01 루프 진입\n",
      "[DEBUG] HOUR=01 rows=42419\n",
      "[DEBUG] HOUR=01 enc shapes: tr=(30541, 267), val=(3394, 267), test=(8484, 267)\n",
      "[DEBUG] HOUR=01 DONE | MAE=16813.879\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_01\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=02 루프 진입\n",
      "[DEBUG] HOUR=02 rows=42498\n",
      "[DEBUG] HOUR=02 enc shapes: tr=(30598, 267), val=(3400, 267), test=(8500, 267)\n",
      "[DEBUG] HOUR=02 DONE | MAE=8036.923\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_02\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=03 루프 진입\n",
      "[DEBUG] HOUR=03 rows=44644\n",
      "[DEBUG] HOUR=03 enc shapes: tr=(32143, 267), val=(3572, 267), test=(8929, 267)\n",
      "[DEBUG] HOUR=03 DONE | MAE=8344.425\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_03\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=04 루프 진입\n",
      "[DEBUG] HOUR=04 rows=45451\n",
      "[DEBUG] HOUR=04 enc shapes: tr=(32724, 267), val=(3636, 267), test=(9091, 267)\n",
      "[DEBUG] HOUR=04 DONE | MAE=4265.749\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_04\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=05 루프 진입\n",
      "[DEBUG] HOUR=05 rows=45510\n",
      "[DEBUG] HOUR=05 enc shapes: tr=(32767, 267), val=(3641, 267), test=(9102, 267)\n",
      "[DEBUG] HOUR=05 DONE | MAE=5370.516\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_05\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=06 루프 진입\n",
      "[DEBUG] HOUR=06 rows=45501\n",
      "[DEBUG] HOUR=06 enc shapes: tr=(32760, 267), val=(3640, 267), test=(9101, 267)\n",
      "[DEBUG] HOUR=06 DONE | MAE=7684.002\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_06\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=07 루프 진입\n",
      "[DEBUG] HOUR=07 rows=45516\n",
      "[DEBUG] HOUR=07 enc shapes: tr=(32770, 267), val=(3642, 267), test=(9104, 267)\n",
      "[DEBUG] HOUR=07 DONE | MAE=6052.627\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_07\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=08 루프 진입\n",
      "[DEBUG] HOUR=08 rows=45439\n",
      "[DEBUG] HOUR=08 enc shapes: tr=(32715, 267), val=(3636, 267), test=(9088, 267)\n",
      "[DEBUG] HOUR=08 DONE | MAE=7437.397\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_08\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=09 루프 진입\n",
      "[DEBUG] HOUR=09 rows=44986\n",
      "[DEBUG] HOUR=09 enc shapes: tr=(32389, 267), val=(3599, 267), test=(8998, 267)\n",
      "[DEBUG] HOUR=09 DONE | MAE=12911.622\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_09\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=10 루프 진입\n",
      "[DEBUG] HOUR=10 rows=41559\n",
      "[DEBUG] HOUR=10 enc shapes: tr=(29922, 267), val=(3325, 267), test=(8312, 267)\n",
      "[DEBUG] HOUR=10 DONE | MAE=20040.728\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_10\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] 루프 종료: trained=10, skipped=0\n",
      "[DEBUG] SAVE_DIR 최종 파일 리스트 일부: ['HOUR_01', 'HOUR_02', 'HOUR_03', 'HOUR_04', 'HOUR_05', 'HOUR_06', 'HOUR_07', 'HOUR_08', 'HOUR_09', 'HOUR_10', 'results_by_hour.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n사용 예:\\nbundle = load_best_bundle(5)\\nsample = X_all.iloc[[0]].copy()          # 컬럼 구조 동일해야 함\\nyhat = predict_with_bundle(bundle, sample)\\nprint(yhat)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# XGB 사용 가능 여부 체크\n",
    "# =============================\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "HANSIK_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DELIV_PATH  = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model\"           \n",
    "MAX_HOURS = list(range(1, 11))    # 1~10\n",
    "TEST_RATIO = 0.2                  # 마지막 20% 테스트\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 원핫 폭발 방지(가능한 sklearn 버전에서만 적용)\n",
    "OHE_MIN_FREQUENCY = 20\n",
    "OHE_MAX_CATEGORIES = 200\n",
    "\n",
    "# UNIT lag/rolling\n",
    "LAGS = [1, 7, 14]\n",
    "ROLL_WINDOWS = [7, 14]\n",
    "\n",
    "# XGB 파라미터(너무 느리면 max_depth/early stopping 줄이세요)\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=5000,          # 크게 주고 early stopping으로 컷\n",
    "    learning_rate=0.03,\n",
    "    max_depth=8,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    objective=\"reg:absoluteerror\",  # MAE 직접 최적화(지원 버전 필요)\n",
    "    tree_method=\"hist\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "ET_PARAMS = dict(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# 누수 위험(예측 시점에 모르는 값) 기본 제외\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate_reg(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return float(mae), float(rmse), float(r2)\n",
    "\n",
    "\n",
    "def safe_onehot_encoder():\n",
    "    \"\"\"\n",
    "    sklearn 버전에 따라 min_frequency / max_categories 미지원일 수 있어 예외 처리.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES\n",
    "        )\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "def build_preprocess(X: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    X의 dtype 기반으로 ColumnTransformer 생성\n",
    "    \"\"\"\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", safe_onehot_encoder())\n",
    "    ])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1) Load\n",
    "# =============================\n",
    "hansik = pd.read_csv(HANSIK_PATH)\n",
    "deliv  = pd.read_csv(DELIV_PATH)\n",
    "\n",
    "required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"UNIT\"}\n",
    "missing = required - set(hansik.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"[한식 데이터] 필수 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "missing2 = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\"} - set(deliv.columns)\n",
    "if missing2:\n",
    "    raise ValueError(f\"[배달 데이터] 필수 컬럼이 없습니다: {missing2}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 2) Merge DELIV as features\n",
    "# =============================\n",
    "merge_keys = [\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\"]\n",
    "\n",
    "deliv_feat = deliv.copy()\n",
    "for c in deliv_feat.columns:\n",
    "    if c not in merge_keys:\n",
    "        deliv_feat.rename(columns={c: f\"DELIV_{c}\"}, inplace=True)\n",
    "\n",
    "df = hansik.merge(deliv_feat, on=merge_keys, how=\"left\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3) Date features\n",
    "# =============================\n",
    "df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df = df[df[\"DATE\"].notna()].copy()\n",
    "\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"DAY_OF_MONTH\"] = df[\"DATE\"].dt.day\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 4) Lag/Rolling features for UNIT (no leakage if time-split)\n",
    "#    group key: (DONG, HOUR)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"]).reset_index(drop=True)\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"], sort=False)\n",
    "\n",
    "for lag in LAGS:\n",
    "    df[f\"UNIT_LAG_{lag}\"] = grp[\"UNIT\"].shift(lag)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"UNIT_ROLL_MEAN_{w}\"] = grp[\"UNIT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).mean()\n",
    "    df[f\"UNIT_ROLL_STD_{w}\"]  = grp[\"UNIT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).std()\n",
    "\n",
    "df[\"UNIT_LAG_1_MISSING\"] = df[\"UNIT_LAG_1\"].isna().astype(int)\n",
    "\n",
    "# 타겟\n",
    "y_all = df[\"UNIT\"].copy()\n",
    "\n",
    "# 피처\n",
    "X_all = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5) Train per HOUR\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "hours = sorted(pd.unique(df[\"HOUR\"].dropna()))\n",
    "hours = [int(h) for h in hours if int(h) in MAX_HOURS]\n",
    "\n",
    "print(f\"[INFO] XGB 사용 여부: {HAS_XGB}\")\n",
    "print(f\"[INFO] 학습 시간대: {hours}\")\n",
    "print(f\"[INFO] 저장 폴더: {SAVE_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "all_results = []\n",
    "t0 = time.time()\n",
    "\n",
    "print(\"\\n[DEBUG] >>> 학습 루프 시작 직전\")\n",
    "for h in hours:\n",
    "    idx = (df[\"HOUR\"].astype(int) == h)\n",
    "    X_h = X_all.loc[idx].copy()\n",
    "    y_h = y_all.loc[idx].copy()\n",
    "    d_h = df.loc[idx, \"DATE\"].copy()\n",
    "\n",
    "    if len(X_h) < 800:\n",
    "        print(f\"\\n[SKIP] HOUR={h:02d}: 데이터가 너무 적음 (n={len(X_h)})\")\n",
    "        continue\n",
    "\n",
    "    # 날짜 정렬\n",
    "    order = np.argsort(d_h.values)\n",
    "    X_h = X_h.iloc[order].reset_index(drop=True)\n",
    "    y_h = y_h.iloc[order].reset_index(drop=True)\n",
    "\n",
    "    n = len(X_h)\n",
    "    split = int(n * (1 - TEST_RATIO))\n",
    "    X_train, X_test = X_h.iloc[:split], X_h.iloc[split:]\n",
    "    y_train, y_test = y_h.iloc[:split], y_h.iloc[split:]\n",
    "\n",
    "    # XGB early stopping용 valid (train의 마지막 10%)\n",
    "    split2 = int(len(X_train) * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2], X_train.iloc[split2:]\n",
    "    y_tr, y_val = y_train.iloc[:split2], y_train.iloc[split2:]\n",
    "\n",
    "    print(f\"\\n===== HOUR={h:02d} | train={len(X_train)} test={len(X_test)} =====\")\n",
    "\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{h:02d}\")\n",
    "    os.makedirs(hour_dir, exist_ok=True)\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # -------------------------\n",
    "    # A방식: preprocess를 먼저 fit/transform 후 모델 학습\n",
    "    # -------------------------\n",
    "    preprocess = build_preprocess(X_train)\n",
    "    pre = clone(preprocess)\n",
    "\n",
    "    X_tr_enc  = pre.fit_transform(X_tr)\n",
    "    X_val_enc = pre.transform(X_val)\n",
    "    X_test_enc = pre.transform(X_test)\n",
    "\n",
    "    # --- 1) XGB (있으면 가장 강력) ---\n",
    "    if HAS_XGB:\n",
    "        # objective가 버전에 따라 에러날 수 있어 fallback 처리\n",
    "        xgb_params = dict(XGB_PARAMS)\n",
    "        try:\n",
    "            model_xgb = XGBRegressor(**xgb_params)\n",
    "            model_xgb.fit(\n",
    "                X_tr_enc, y_tr,\n",
    "                eval_set=[(X_val_enc, y_val)],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # fallback: objective를 reg:squarederror로\n",
    "            xgb_params[\"objective\"] = \"reg:squarederror\"\n",
    "            model_xgb = XGBRegressor(**xgb_params)\n",
    "            model_xgb.fit(\n",
    "                X_tr_enc, y_tr,\n",
    "                eval_set=[(X_val_enc, y_val)],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200\n",
    "            )\n",
    "\n",
    "        pred = model_xgb.predict(X_test_enc)\n",
    "        mae, rmse, r2 = evaluate_reg(y_test, pred)\n",
    "        print(f\"- XGB       | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "\n",
    "        bundle = {\"preprocess\": pre, \"model\": model_xgb, \"model_name\": \"XGB\"}\n",
    "        candidates.append((\"XGB\", bundle, mae, rmse, r2))\n",
    "\n",
    "    # --- 2) ExtraTrees (빠른/안정 대안) ---\n",
    "    model_et = ExtraTreesRegressor(**ET_PARAMS)\n",
    "    model_et.fit(X_tr_enc, y_tr)  # valid는 사용 안 함(빠름)\n",
    "\n",
    "    pred = model_et.predict(X_test_enc)\n",
    "    mae, rmse, r2 = evaluate_reg(y_test, pred)\n",
    "    print(f\"- EXTRATREE | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "\n",
    "    bundle = {\"preprocess\": pre, \"model\": model_et, \"model_name\": \"EXTRATREE\"}\n",
    "    candidates.append((\"EXTRATREE\", bundle, mae, rmse, r2))\n",
    "\n",
    "    # --- pick best by MAE ---\n",
    "    candidates.sort(key=lambda x: x[2])\n",
    "    best_name, best_bundle, best_mae, best_rmse, best_r2 = candidates[0]\n",
    "\n",
    "    best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    joblib.dump(best_bundle, best_path)\n",
    "\n",
    "    meta = {\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": best_name,\n",
    "        \"MAE\": float(best_mae),\n",
    "        \"RMSE\": float(best_rmse),\n",
    "        \"R2\": float(best_r2),\n",
    "        \"TRAIN_SIZE\": int(len(X_train)),\n",
    "        \"TEST_SIZE\": int(len(X_test)),\n",
    "        \"FEATURE_COLS\": list(X_all.columns),\n",
    "        \"DROP_COLS\": DROP_COLS,\n",
    "        \"LAGS\": LAGS,\n",
    "        \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "        \"HAS_XGB\": bool(HAS_XGB),\n",
    "        \"XGB_PARAMS\": XGB_PARAMS if HAS_XGB else None,\n",
    "        \"ET_PARAMS\": ET_PARAMS,\n",
    "    }\n",
    "    with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ BEST => {best_name} | MAE={best_mae:,.3f} 저장: {best_path}\")\n",
    "\n",
    "    all_results.append({\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": best_name,\n",
    "        \"MAE\": float(best_mae),\n",
    "        \"RMSE\": float(best_rmse),\n",
    "        \"R2\": float(best_r2),\n",
    "        \"SAVE_PATH\": best_path\n",
    "    })\n",
    "\n",
    "# 결과 저장\n",
    "res_df = pd.DataFrame(all_results).sort_values([\"HOUR\"])\n",
    "res_path = os.path.join(SAVE_DIR, \"results_by_hour.csv\")\n",
    "res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[DONE] 학습 완료\")\n",
    "print(f\"- 결과표 저장: {res_path}\")\n",
    "print(f\"- 모델 저장 폴더: {SAVE_DIR}\")\n",
    "print(f\"- 총 소요(초): {time.time() - t0:,.1f}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 6) 로드/예측 헬퍼\n",
    "# =============================\n",
    "def load_best_bundle(hour: int):\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{hour:02d}\")\n",
    "    path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "\"\"\"\n",
    "사용 예:\n",
    "bundle = load_best_bundle(5)\n",
    "sample = X_all.iloc[[0]].copy()          # 컬럼 구조 동일해야 함\n",
    "yhat = predict_with_bundle(bundle, sample)\n",
    "print(yhat)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bdea1",
   "metadata": {},
   "source": [
    "## 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41dc0469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TA_YMD  DAY  HOUR BEST_MODEL  TEMP  RAIN  PRED_UNIT  KMA_BASE_USED\n",
      "0  20251230    2     1        XGB -0.86   0.0      38545  20251230-1100\n",
      "1  20251230    2     2        XGB -0.86   0.0      11827  20251230-1100\n",
      "2  20251230    2     3        XGB -0.86   0.0      16934  20251230-1100\n",
      "3  20251230    2     4  EXTRATREE  1.00   0.0      25555  20251230-1100\n",
      "4  20251230    2     5  EXTRATREE  1.50   0.0      24160  20251230-1100\n",
      "5  20251230    2     6        XGB  1.00   0.0      23881  20251230-1100\n",
      "6  20251230    2     7  EXTRATREE -0.50   0.0      33109  20251230-1100\n",
      "7  20251230    2     8  EXTRATREE -2.00   0.0      38352  20251230-1100\n",
      "8  20251230    2     9        XGB -3.00   0.0      51828  20251230-1100\n",
      "9  20251230    2    10        XGB -4.00   0.0      51965  20251230-1100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "SERVICE_KEY = os.getenv(\"RAIN_ID\")  # 기상청 단기예보 서비스키\n",
    "\n",
    "MODELS_ROOT = \"data/model\"\n",
    "BEST_MAP_PATH = os.path.join(MODELS_ROOT, \"results_by_hour.csv\")  \n",
    "BUNDLE_PATH_TEMPLATE = os.path.join(MODELS_ROOT, \"HOUR_{hour:02d}\", \"best_model.joblib\")  \n",
    "\n",
    "# 학습 때 로그타겟이면 True\n",
    "USE_LOG_TARGET = False\n",
    "\n",
    "# (학습 때와 동일해야 함) 안전장치로 drop\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "# =========================\n",
    "# 항목요약 시간대(01~10)\n",
    "# =========================\n",
    "HOUR_BINS = {\n",
    "    1: list(range(0, 7)),     \n",
    "    2: [7, 8],\n",
    "    3: [9, 10],\n",
    "    4: [11, 12],\n",
    "    5: [13, 14],\n",
    "    6: [15, 16],\n",
    "    7: [17, 18],\n",
    "    8: [19, 20],\n",
    "    9: [21, 22],\n",
    "    10: [23],\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 날짜 파생 + DAY(01=월..07=일)\n",
    "# =========================\n",
    "def compute_day_code(date_yyyymmdd: str) -> int:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return int(dt.weekday() + 1)\n",
    "\n",
    "def make_date_features(date_yyyymmdd: str) -> dict:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return {\n",
    "        \"TA_YMD\": str(date_yyyymmdd),\n",
    "        \"YEAR\": int(dt.year),\n",
    "        \"MONTH\": int(dt.month),\n",
    "        \"DAY_OF_MONTH\": int(dt.day),\n",
    "        \"WEEKOFYEAR\": int(dt.isocalendar().week),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 기상청 단기예보 TMP/PCP\n",
    "# =========================\n",
    "def _parse_pcp_to_mm(x):\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s in (\"강수없음\", \"없음\", \"\", \"nan\", \"NaN\"):\n",
    "        return 0.0\n",
    "    if \"미만\" in s:\n",
    "        try:\n",
    "            num = float(s.replace(\"mm\", \"\").replace(\"미만\", \"\").strip())\n",
    "            return max(0.0, num * 0.5)\n",
    "        except:\n",
    "            return 0.0\n",
    "    if \"~\" in s:\n",
    "        try:\n",
    "            a, b = s.replace(\"mm\", \"\").split(\"~\")\n",
    "            return (float(a) + float(b)) / 2.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    try:\n",
    "        return float(s.replace(\"mm\", \"\"))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def fetch_vilage_fcst_json(service_key, base_date, base_time, nx, ny, num_rows=3000):\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_hourly_tmp_pcp_for_date(service_key, target_yyyymmdd, nx, ny):\n",
    "    base_times = [\"2300\",\"2000\",\"1700\",\"1400\",\"1100\",\"0800\",\"0500\",\"0200\"]\n",
    "    now = datetime.now()\n",
    "    today = now.strftime(\"%Y%m%d\")\n",
    "    yesterday = (now - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    last_err = None\n",
    "    for bd in [today, yesterday]:\n",
    "        for bt in base_times:\n",
    "            try:\n",
    "                js = fetch_vilage_fcst_json(service_key, bd, bt, nx, ny)\n",
    "                items = js[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "                df = pd.DataFrame(items)\n",
    "\n",
    "                df = df[df[\"category\"].isin([\"TMP\", \"PCP\"])]\n",
    "                df = df[df[\"fcstDate\"].astype(str) == str(target_yyyymmdd)]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                df[\"fcstTime\"] = df[\"fcstTime\"].astype(str).str.zfill(4)\n",
    "                piv = df.pivot_table(\n",
    "                    index=\"fcstTime\",\n",
    "                    columns=\"category\",\n",
    "                    values=\"fcstValue\",\n",
    "                    aggfunc=\"first\"\n",
    "                ).reset_index()\n",
    "\n",
    "                if \"TMP\" not in piv.columns:\n",
    "                    continue\n",
    "\n",
    "                piv[\"TMP\"] = piv[\"TMP\"].astype(float)\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].apply(_parse_pcp_to_mm) if \"PCP\" in piv.columns else 0.0\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].astype(float)\n",
    "                piv[\"base_used\"] = f\"{bd}-{bt}\"\n",
    "                return piv.sort_values(\"fcstTime\").reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(f\"해당 날짜의 기상청 예보 데이터를 찾지 못했습니다. last_err={last_err}\")\n",
    "\n",
    "def aggregate_weather_to_item_slots(hourly_df: pd.DataFrame) -> dict:\n",
    "    hourly_df = hourly_df.copy()\n",
    "    hourly_df[\"HOUR_OF_DAY\"] = hourly_df[\"fcstTime\"].str[:2].astype(int)\n",
    "\n",
    "    out = {}\n",
    "    for h, hours in HOUR_BINS.items():\n",
    "        sub = hourly_df[hourly_df[\"HOUR_OF_DAY\"].isin(hours)]\n",
    "        out[h] = {\n",
    "            \"TEMP\": float(sub[\"TMP\"].mean()) if not sub.empty else np.nan,\n",
    "            \"RAIN\": float(sub[\"PCP\"].sum()) if not sub.empty else 0.0,\n",
    "        }\n",
    "\n",
    "    temps = [v[\"TEMP\"] for v in out.values() if not pd.isna(v[\"TEMP\"])]\n",
    "    fill_temp = float(np.mean(temps)) if temps else 0.0\n",
    "    for h in out:\n",
    "        if pd.isna(out[h][\"TEMP\"]):\n",
    "            out[h][\"TEMP\"] = fill_temp\n",
    "\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# 모델 로드 + \"모델이 기대하는 컬럼\" 자동 맞춤\n",
    "# (학습 저장물: best_model.joblib = {\"preprocess\": ColumnTransformer, \"model\": estimator, \"model_name\": str})\n",
    "# =========================\n",
    "def load_best_model_map():\n",
    "    if not os.path.exists(BEST_MAP_PATH):\n",
    "        raise FileNotFoundError(f\"결과 파일이 없습니다: {BEST_MAP_PATH}\")\n",
    "    df = pd.read_csv(BEST_MAP_PATH)\n",
    "    required_cols = {\"HOUR\", \"BEST_MODEL\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"{BEST_MAP_PATH}에 필요한 컬럼이 없습니다. 필요: {required_cols}, 현재: {set(df.columns)}\")\n",
    "    return df.set_index(\"HOUR\")[\"BEST_MODEL\"].to_dict()\n",
    "\n",
    "def load_best_bundle(hour: int):\n",
    "    path = BUNDLE_PATH_TEMPLATE.format(hour=hour)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"번들 파일이 없습니다: {path}\")\n",
    "    bundle = joblib.load(path)\n",
    "    # 최소 검증\n",
    "    if not isinstance(bundle, dict) or \"preprocess\" not in bundle or \"model\" not in bundle:\n",
    "        raise ValueError(f\"번들 포맷이 예상과 다릅니다: {path}\")\n",
    "    return bundle\n",
    "\n",
    "def get_required_feature_names_from_preprocess(preprocess) -> list:\n",
    "    \"\"\"\n",
    "    ColumnTransformer가 기대하는 원본 feature명 추출\n",
    "    (fit 이후 preprocess.transformers_에 cols가 들어있음)\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    for name, trans, cols in getattr(preprocess, \"transformers_\", []):\n",
    "        if cols is None:\n",
    "            continue\n",
    "        if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "            names.extend(list(cols))\n",
    "    return sorted(set([str(x) for x in names]))\n",
    "\n",
    "def align_row_to_required_columns(X_row: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    required_cols에 없는 컬럼은 제거, 없는 컬럼은 생성(NaN).\n",
    "    imputer가 있으니 NaN 생성이 안전.\n",
    "    \"\"\"\n",
    "    X = X_row.copy()\n",
    "\n",
    "    if required_cols:\n",
    "        # 제거\n",
    "        drop_cols = [c for c in X.columns if c not in required_cols]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        # 생성\n",
    "        for c in required_cols:\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "\n",
    "        # 순서 고정\n",
    "        X = X[required_cols]\n",
    "\n",
    "    return X\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "# =========================\n",
    "# 핵심: 날짜 1개 → 1~10시간대 예측\n",
    "# - 기상청 TEMP/RAIN을 항목요약 시간대로 집계해서 넣음\n",
    "# - 시간대별 \"best_model.joblib\" 자동 로드\n",
    "# - preprocess가 기대하는 컬럼 자동 보정\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather(\n",
    "    date_yyyymmdd: str,\n",
    "    nx: int,\n",
    "    ny: int,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)가 비어있습니다. .env 또는 환경변수를 확인하세요.\")\n",
    "\n",
    "    day_code = compute_day_code(date_yyyymmdd)\n",
    "\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    best_map = load_best_model_map()\n",
    "    rows = []\n",
    "\n",
    "    for hour in range(1, 11):\n",
    "        if hour not in best_map:\n",
    "            continue\n",
    "\n",
    "        # 학습 코드에서는 best_model.joblib 한 개만 저장 (BEST_MODEL 명은 meta용)\n",
    "        bundle = load_best_bundle(hour)\n",
    "        best_name = str(best_map[hour])\n",
    "\n",
    "        # 기본 row: 날짜/시간대/요일 + 날씨\n",
    "        row = {}\n",
    "        row.update(make_date_features(date_yyyymmdd))\n",
    "        row.update({\n",
    "            \"HOUR\": int(hour),\n",
    "            \"DAY\": int(day_code),\n",
    "\n",
    "            # (학습 데이터에 존재한다면 자동 사용될 수 있게)\n",
    "            \"TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "\n",
    "            # merge된 배달 피처명을 대비해서도 넣어둠(학습에서 DELIV_*가 있었음)\n",
    "            \"DELIV_TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"DELIV_RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "        })\n",
    "\n",
    "        X_row = pd.DataFrame([row])\n",
    "        X_row = X_row.drop(columns=[c for c in DROP_COLS if c in X_row.columns], errors=\"ignore\")\n",
    "\n",
    "        # preprocess가 기대하는 원본 컬럼으로 맞춤\n",
    "        required = get_required_feature_names_from_preprocess(bundle[\"preprocess\"])\n",
    "        X_row_aligned = align_row_to_required_columns(X_row, required)\n",
    "\n",
    "        pred = float(predict_with_bundle(bundle, X_row_aligned)[0])\n",
    "        if USE_LOG_TARGET:\n",
    "            pred = float(np.expm1(pred))\n",
    "\n",
    "        rows.append({\n",
    "            \"TA_YMD\": str(date_yyyymmdd),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"HOUR\": int(hour),\n",
    "            \"BEST_MODEL\": best_name,             \n",
    "            \"TEMP\": round(float(row[\"TEMP\"]), 2),\n",
    "            \"RAIN\": float(row[\"RAIN\"]),\n",
    "            \"PRED_UNIT\": int(round(pred)),\n",
    "            \"KMA_BASE_USED\": str(hourly_weather.loc[0, \"base_used\"]),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# 사용 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df_pred = predict_date_all_hours_with_weather(\n",
    "        date_yyyymmdd=\"20251230\",\n",
    "        nx=61,\n",
    "        ny=121\n",
    "    )\n",
    "    print(df_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816a827",
   "metadata": {},
   "source": [
    "## 동입력후 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dcacc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMP(slot_hour=5): -4.0 °C\n",
      "     TA_YMD  DONG  DAY  HOUR BEST_MODEL  TEMP  RAIN  PRED_UNIT  KMA_BASE_USED\n",
      "0  20251231  정자2동    3     1        XGB -5.43   0.0      38545  20251230-1100\n",
      "1  20251231  정자2동    3     2        XGB -8.00   0.0      11827  20251230-1100\n",
      "2  20251231  정자2동    3     3        XGB -7.00   0.0      16934  20251230-1100\n",
      "3  20251231  정자2동    3     4  EXTRATREE -5.50   0.0      25851  20251230-1100\n",
      "4  20251231  정자2동    3     5  EXTRATREE -4.00   0.0      24799  20251230-1100\n",
      "5  20251231  정자2동    3     6        XGB -4.00   0.0      20502  20251230-1100\n",
      "6  20251231  정자2동    3     7  EXTRATREE -5.50   0.0      32290  20251230-1100\n",
      "7  20251231  정자2동    3     8  EXTRATREE -6.50   0.0      38535  20251230-1100\n",
      "8  20251231  정자2동    3     9        XGB -8.00   0.0      34702  20251230-1100\n",
      "9  20251231  정자2동    3    10        XGB -9.00   0.0      51965  20251230-1100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "SERVICE_KEY = os.getenv(\"RAIN_ID\")  # 기상청 단기예보 서비스키\n",
    "\n",
    "MODELS_ROOT = \"data/model\"\n",
    "BEST_MAP_PATH = os.path.join(MODELS_ROOT, \"results_by_hour.csv\")\n",
    "BUNDLE_PATH_TEMPLATE = os.path.join(MODELS_ROOT, \"HOUR_{hour:02d}\", \"best_model.joblib\")\n",
    "\n",
    "# 학습 때 로그타겟이면 True\n",
    "USE_LOG_TARGET = False\n",
    "\n",
    "# (학습 때와 동일해야 함) 안전장치로 drop\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "# 격자 엑셀\n",
    "GRID_XLSX_PATH = \"data/수원시 격자.xlsx\"\n",
    "_GRID_CACHE = None  # 반복 로딩 방지 캐시\n",
    "\n",
    "# =========================\n",
    "# 항목요약 시간대(01~10)\n",
    "# =========================\n",
    "HOUR_BINS = {\n",
    "    1: list(range(0, 7)),     # 00~06\n",
    "    2: [7, 8],\n",
    "    3: [9, 10],\n",
    "    4: [11, 12],\n",
    "    5: [13, 14],\n",
    "    6: [15, 16],\n",
    "    7: [17, 18],\n",
    "    8: [19, 20],\n",
    "    9: [21, 22],\n",
    "    10: [23],\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 날짜 파생 + DAY(01=월..07=일)\n",
    "# =========================\n",
    "def compute_day_code(date_yyyymmdd: str) -> int:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return int(dt.weekday() + 1)\n",
    "\n",
    "def make_date_features(date_yyyymmdd: str) -> dict:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return {\n",
    "        \"TA_YMD\": str(date_yyyymmdd),\n",
    "        \"YEAR\": int(dt.year),\n",
    "        \"MONTH\": int(dt.month),\n",
    "        \"DAY_OF_MONTH\": int(dt.day),\n",
    "        \"WEEKOFYEAR\": int(dt.isocalendar().week),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 격자: DONG -> (nx, ny)\n",
    "# =========================\n",
    "def load_grid_table(path: str = GRID_XLSX_PATH) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    data/수원시 격자.xlsx\n",
    "    - A열: 동이름, B열: nx, C열: ny (사용자 설명 기준)\n",
    "    - header가 없다고 가정하고 A/B/C를 고정으로 읽음\n",
    "      (만약 실제 파일에 헤더가 있으면 header=None -> header=0 으로 변경)\n",
    "    \"\"\"\n",
    "    global _GRID_CACHE\n",
    "    if _GRID_CACHE is not None:\n",
    "        return _GRID_CACHE\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"격자 파일이 없습니다: {path}\")\n",
    "\n",
    "    df = pd.read_excel(path, header=None)\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(f\"격자 파일 컬럼 수가 부족합니다(A,B,C 필요): {path}\")\n",
    "\n",
    "    df = df.iloc[:, :3].copy()\n",
    "    df.columns = [\"DONG_NAME\", \"nx\", \"ny\"]\n",
    "\n",
    "    df[\"DONG_NAME\"] = df[\"DONG_NAME\"].astype(str).str.strip()\n",
    "    df[\"nx\"] = pd.to_numeric(df[\"nx\"], errors=\"coerce\")\n",
    "    df[\"ny\"] = pd.to_numeric(df[\"ny\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"nx\", \"ny\"]).copy()\n",
    "    df[\"nx\"] = df[\"nx\"].astype(int)\n",
    "    df[\"ny\"] = df[\"ny\"].astype(int)\n",
    "\n",
    "    _GRID_CACHE = df\n",
    "    return df\n",
    "\n",
    "def find_grid_by_dong(dong: str, path: str = GRID_XLSX_PATH) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    사용자가 입력한 dong(예: '행궁동', '수원시 팔달구 행궁동')으로\n",
    "    엑셀 A열(DONG_NAME)에서 격자(nx,ny) 찾기.\n",
    "    - 완전일치 우선, 없으면 포함검색(양방향)\n",
    "    - 후보가 여러 개면 DONG_NAME 길이가 짧은 것을 우선 선택\n",
    "    \"\"\"\n",
    "    dong = str(dong).strip()\n",
    "    if not dong:\n",
    "        raise ValueError(\"dong(동) 입력이 비었습니다.\")\n",
    "\n",
    "    grid = load_grid_table(path)\n",
    "\n",
    "    # 1) 완전일치\n",
    "    exact = grid[grid[\"DONG_NAME\"] == dong]\n",
    "    if not exact.empty:\n",
    "        r = exact.iloc[0]\n",
    "        return int(r[\"nx\"]), int(r[\"ny\"])\n",
    "\n",
    "    # 2) 포함검색(양방향)\n",
    "    a = grid[\"DONG_NAME\"].str.contains(dong, na=False)\n",
    "    b = grid[\"DONG_NAME\"].apply(lambda x: str(dong).find(str(x)) >= 0)\n",
    "    cand = grid[a | b].copy()\n",
    "\n",
    "    # fallback: 마지막 토큰(보통 'OO동')로 재시도\n",
    "    if cand.empty:\n",
    "        tok = dong.split()[-1]\n",
    "        cand = grid[grid[\"DONG_NAME\"].str.contains(tok, na=False)].copy()\n",
    "\n",
    "    if cand.empty:\n",
    "        raise ValueError(f\"'{dong}'에 해당하는 격자(nx,ny)를 찾지 못했습니다. 엑셀 A열 동이름을 확인하세요.\")\n",
    "\n",
    "    cand[\"name_len\"] = cand[\"DONG_NAME\"].str.len()\n",
    "    cand = cand.sort_values([\"name_len\"]).reset_index(drop=True)\n",
    "    r = cand.iloc[0]\n",
    "    return int(r[\"nx\"]), int(r[\"ny\"])\n",
    "\n",
    "# =========================\n",
    "# 기상청 단기예보 TMP/PCP\n",
    "# =========================\n",
    "def _parse_pcp_to_mm(x):\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s in (\"강수없음\", \"없음\", \"\", \"nan\", \"NaN\"):\n",
    "        return 0.0\n",
    "    if \"미만\" in s:\n",
    "        try:\n",
    "            num = float(s.replace(\"mm\", \"\").replace(\"미만\", \"\").strip())\n",
    "            return max(0.0, num * 0.5)\n",
    "        except:\n",
    "            return 0.0\n",
    "    if \"~\" in s:\n",
    "        try:\n",
    "            a, b = s.replace(\"mm\", \"\").split(\"~\")\n",
    "            return (float(a) + float(b)) / 2.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    try:\n",
    "        return float(s.replace(\"mm\", \"\"))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def fetch_vilage_fcst_json(service_key, base_date, base_time, nx, ny, num_rows=3000):\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_hourly_tmp_pcp_for_date(service_key, target_yyyymmdd, nx, ny):\n",
    "    base_times = [\"2300\", \"2000\", \"1700\", \"1400\", \"1100\", \"0800\", \"0500\", \"0200\"]\n",
    "    now = datetime.now()\n",
    "    today = now.strftime(\"%Y%m%d\")\n",
    "    yesterday = (now - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    last_err = None\n",
    "    for bd in [today, yesterday]:\n",
    "        for bt in base_times:\n",
    "            try:\n",
    "                js = fetch_vilage_fcst_json(service_key, bd, bt, nx, ny)\n",
    "                items = js[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "                df = pd.DataFrame(items)\n",
    "\n",
    "                df = df[df[\"category\"].isin([\"TMP\", \"PCP\"])]\n",
    "                df = df[df[\"fcstDate\"].astype(str) == str(target_yyyymmdd)]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                df[\"fcstTime\"] = df[\"fcstTime\"].astype(str).str.zfill(4)\n",
    "                piv = df.pivot_table(\n",
    "                    index=\"fcstTime\",\n",
    "                    columns=\"category\",\n",
    "                    values=\"fcstValue\",\n",
    "                    aggfunc=\"first\"\n",
    "                ).reset_index()\n",
    "\n",
    "                if \"TMP\" not in piv.columns:\n",
    "                    continue\n",
    "\n",
    "                piv[\"TMP\"] = piv[\"TMP\"].astype(float)\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].apply(_parse_pcp_to_mm) if \"PCP\" in piv.columns else 0.0\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].astype(float)\n",
    "                piv[\"base_used\"] = f\"{bd}-{bt}\"\n",
    "                return piv.sort_values(\"fcstTime\").reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(f\"해당 날짜의 기상청 예보 데이터를 찾지 못했습니다. last_err={last_err}\")\n",
    "\n",
    "def aggregate_weather_to_item_slots(hourly_df: pd.DataFrame) -> dict:\n",
    "    hourly_df = hourly_df.copy()\n",
    "    hourly_df[\"HOUR_OF_DAY\"] = hourly_df[\"fcstTime\"].str[:2].astype(int)\n",
    "\n",
    "    out = {}\n",
    "    for h, hours in HOUR_BINS.items():\n",
    "        sub = hourly_df[hourly_df[\"HOUR_OF_DAY\"].isin(hours)]\n",
    "        out[h] = {\n",
    "            \"TEMP\": float(sub[\"TMP\"].mean()) if not sub.empty else np.nan,\n",
    "            \"RAIN\": float(sub[\"PCP\"].sum()) if not sub.empty else 0.0,\n",
    "        }\n",
    "\n",
    "    temps = [v[\"TEMP\"] for v in out.values() if not pd.isna(v[\"TEMP\"])]\n",
    "    fill_temp = float(np.mean(temps)) if temps else 0.0\n",
    "    for h in out:\n",
    "        if pd.isna(out[h][\"TEMP\"]):\n",
    "            out[h][\"TEMP\"] = fill_temp\n",
    "\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# ✅ 동 입력 -> 그 동의 \"기온\" 반환(원하는 함수)\n",
    "# =========================\n",
    "def get_temperature_by_dong(\n",
    "    date_yyyymmdd: str,\n",
    "    dong: str,\n",
    "    slot_hour: int = 5,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    dong 입력 -> 격자(nx,ny) 조회 -> 기상청 예보 -> slot_hour(1~10)의 TEMP 반환\n",
    "    \"\"\"\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)가 비어있습니다. .env 또는 환경변수를 확인하세요.\")\n",
    "    if not (1 <= int(slot_hour) <= 10):\n",
    "        raise ValueError(\"slot_hour는 1~10 이어야 합니다.\")\n",
    "\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    temp = float(weather_by_hour[int(slot_hour)][\"TEMP\"])\n",
    "    return round(temp, 1)  # ✅ 소수점 1자리 반환\n",
    "\n",
    "# =========================\n",
    "# 모델 로드 + \"모델이 기대하는 컬럼\" 자동 맞춤\n",
    "# (학습 저장물: best_model.joblib = {\"preprocess\": ColumnTransformer, \"model\": estimator, \"model_name\": str})\n",
    "# =========================\n",
    "def load_best_model_map():\n",
    "    if not os.path.exists(BEST_MAP_PATH):\n",
    "        raise FileNotFoundError(f\"결과 파일이 없습니다: {BEST_MAP_PATH}\")\n",
    "    df = pd.read_csv(BEST_MAP_PATH)\n",
    "    required_cols = {\"HOUR\", \"BEST_MODEL\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"{BEST_MAP_PATH}에 필요한 컬럼이 없습니다. 필요: {required_cols}, 현재: {set(df.columns)}\")\n",
    "    return df.set_index(\"HOUR\")[\"BEST_MODEL\"].to_dict()\n",
    "\n",
    "def load_best_bundle(hour: int):\n",
    "    path = BUNDLE_PATH_TEMPLATE.format(hour=hour)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"번들 파일이 없습니다: {path}\")\n",
    "    bundle = joblib.load(path)\n",
    "    if not isinstance(bundle, dict) or \"preprocess\" not in bundle or \"model\" not in bundle:\n",
    "        raise ValueError(f\"번들 포맷이 예상과 다릅니다: {path}\")\n",
    "    return bundle\n",
    "\n",
    "def get_required_feature_names_from_preprocess(preprocess) -> list:\n",
    "    names = []\n",
    "    for name, trans, cols in getattr(preprocess, \"transformers_\", []):\n",
    "        if cols is None:\n",
    "            continue\n",
    "        if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "            names.extend(list(cols))\n",
    "    return sorted(set([str(x) for x in names]))\n",
    "\n",
    "def align_row_to_required_columns(X_row: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
    "    X = X_row.copy()\n",
    "\n",
    "    if required_cols:\n",
    "        drop_cols = [c for c in X.columns if c not in required_cols]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        for c in required_cols:\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "\n",
    "        X = X[required_cols]\n",
    "\n",
    "    return X\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "# =========================\n",
    "# 핵심: 날짜 1개 → 1~10시간대 예측\n",
    "# - (nx, ny 직접 입력 버전)\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather(\n",
    "    date_yyyymmdd: str,\n",
    "    nx: int,\n",
    "    ny: int,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)가 비어있습니다. .env 또는 환경변수를 확인하세요.\")\n",
    "\n",
    "    day_code = compute_day_code(date_yyyymmdd)\n",
    "\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    best_map = load_best_model_map()\n",
    "    rows = []\n",
    "\n",
    "    for hour in range(1, 11):\n",
    "        if hour not in best_map:\n",
    "            continue\n",
    "\n",
    "        bundle = load_best_bundle(hour)\n",
    "        best_name = str(best_map[hour])\n",
    "\n",
    "        row = {}\n",
    "        row.update(make_date_features(date_yyyymmdd))\n",
    "        row.update({\n",
    "            \"HOUR\": int(hour),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "            \"DELIV_TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"DELIV_RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "        })\n",
    "\n",
    "        X_row = pd.DataFrame([row])\n",
    "        X_row = X_row.drop(columns=[c for c in DROP_COLS if c in X_row.columns], errors=\"ignore\")\n",
    "\n",
    "        required = get_required_feature_names_from_preprocess(bundle[\"preprocess\"])\n",
    "        X_row_aligned = align_row_to_required_columns(X_row, required)\n",
    "\n",
    "        pred = float(predict_with_bundle(bundle, X_row_aligned)[0])\n",
    "        if USE_LOG_TARGET:\n",
    "            pred = float(np.expm1(pred))\n",
    "\n",
    "        rows.append({\n",
    "            \"TA_YMD\": str(date_yyyymmdd),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"HOUR\": int(hour),\n",
    "            \"BEST_MODEL\": best_name,\n",
    "            \"TEMP\": round(float(row[\"TEMP\"]), 2), \n",
    "            \"RAIN\": float(row[\"RAIN\"]),\n",
    "            \"PRED_UNIT\": int(round(pred)),\n",
    "            \"KMA_BASE_USED\": str(hourly_weather.loc[0, \"base_used\"]),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# ✅ 핵심: 사용자 DONG 입력 버전 (요청한 흐름)\n",
    "# - DONG으로 nx,ny 자동 추출\n",
    "# - 그 nx,ny로 예측 수행\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather_by_dong(\n",
    "    date_yyyymmdd: str,\n",
    "    dong: str,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "    df_pred = predict_date_all_hours_with_weather(\n",
    "        date_yyyymmdd=date_yyyymmdd,\n",
    "        nx=nx,\n",
    "        ny=ny,\n",
    "        service_key=service_key\n",
    "    )\n",
    "    df_pred.insert(1, \"DONG\", str(dong))\n",
    "#     df_pred.insert(2, \"nx\", int(nx))\n",
    "#     df_pred.insert(3, \"ny\", int(ny))\n",
    "    return df_pred\n",
    "\n",
    "# =========================\n",
    "# 사용 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 동의 특정 시간대(항목요약) 기온만 뽑기\n",
    "#     temp = get_temperature_by_dong(\"20251231\", dong=\"정자2동\", slot_hour=5)\n",
    "#     print(\"TEMP(slot_hour=5):\", temp, \"°C\")\n",
    "\n",
    "    # 2) 동 입력으로 1~10 시간대 매출(UNIT) 예측\n",
    "    df_pred = predict_date_all_hours_with_weather_by_dong(\n",
    "        date_yyyymmdd=\"20251231\",\n",
    "        dong=\"정자2동\"\n",
    "    )\n",
    "    print(df_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38cf7af",
   "metadata": {},
   "source": [
    "## 매출 데이터 입력 후 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55d23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28211d39",
   "metadata": {},
   "source": [
    "# 6. 휴게소, 빵집 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# XGB 사용 가능 여부\n",
    "# =============================\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "BASE_PATH   = \"data/수원시 한식 데이터백업.csv\"     \n",
    "DELIV_PATH  = \"data/수원시 배달 데이터백업.csv\"\n",
    "BAKERY_PATH = \"data/수원시 빵집 데이터백업.csv\"\n",
    "REST_PATH   = \"data/수원시 휴게소 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model_amt_fast\"\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "TARGET_COL = \"AMT\"\n",
    "\n",
    "# Lag / Rolling (AMT 기준)\n",
    "LAGS = [1, 7, 14]\n",
    "ROLL_WINDOWS = [7, 14]\n",
    "\n",
    "# OneHot 제한\n",
    "OHE_MIN_FREQUENCY = 20\n",
    "OHE_MAX_CATEGORIES = 200\n",
    "\n",
    "# XGB (속도 최적)\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=1800,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"mae\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "ET_PARAMS = dict(\n",
    "    n_estimators=600,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"AMT\", \"DATE\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate(y, p):\n",
    "    return (\n",
    "        mean_absolute_error(y, p),\n",
    "        mean_squared_error(y, p, squared=False),\n",
    "        r2_score(y, p),\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES,\n",
    "            sparse_output=True\n",
    "        )\n",
    "    except:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "def build_preprocess(X):\n",
    "    cat = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num = [c for c in X.columns if c not in cat]\n",
    "\n",
    "    return ColumnTransformer([\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", safe_ohe())\n",
    "        ]), cat)\n",
    "    ])\n",
    "\n",
    "\n",
    "def prefix(df, name, keys):\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if c not in keys:\n",
    "            out.rename(columns={c: f\"{name}_{c}\"}, inplace=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Load & Merge\n",
    "# =============================\n",
    "merge_keys = [\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\"]\n",
    "\n",
    "base   = pd.read_csv(BASE_PATH)\n",
    "deliv  = prefix(pd.read_csv(DELIV_PATH),  \"DELIV\",  merge_keys)\n",
    "bakery = prefix(pd.read_csv(BAKERY_PATH), \"BAKERY\", merge_keys)\n",
    "rest   = prefix(pd.read_csv(REST_PATH),   \"REST\",   merge_keys)\n",
    "\n",
    "df = (\n",
    "    base\n",
    "    .merge(deliv, on=merge_keys, how=\"left\")\n",
    "    .merge(bakery, on=merge_keys, how=\"left\")\n",
    "    .merge(rest, on=merge_keys, how=\"left\")\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# Date features\n",
    "# =============================\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"].astype(str), format=\"%Y%m%d\")\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# =============================\n",
    "# Lag / Rolling (AMT)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"])\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"])\n",
    "\n",
    "for l in LAGS:\n",
    "    df[f\"AMT_LAG_{l}\"] = grp[TARGET_COL].shift(l)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"AMT_ROLL_MEAN_{w}\"] = grp[TARGET_COL].shift(1).rolling(w, min_periods=3).mean()\n",
    "    df[f\"AMT_ROLL_STD_{w}\"]  = grp[TARGET_COL].shift(1).rolling(w, min_periods=3).std()\n",
    "\n",
    "# =============================\n",
    "# Target / Feature split\n",
    "# =============================\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "\n",
    "# 시간 순 분할\n",
    "order = np.argsort(df[\"DATE\"].values)\n",
    "X, y = X.iloc[order], y.iloc[order]\n",
    "\n",
    "split = int(len(X) * (1 - TEST_RATIO))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# =============================\n",
    "# Encode\n",
    "# =============================\n",
    "pre = build_preprocess(X_train)\n",
    "X_tr = pre.fit_transform(X_train)\n",
    "X_te = pre.transform(X_test)\n",
    "\n",
    "# =============================\n",
    "# Train\n",
    "# =============================\n",
    "candidates = []\n",
    "\n",
    "if HAS_XGB:\n",
    "    xgb = XGBRegressor(**XGB_PARAMS)\n",
    "    xgb.fit(\n",
    "        X_tr, y_train,\n",
    "        eval_set=[(X_te, y_test)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    p = xgb.predict(X_te)\n",
    "    candidates.append((\"XGB\", xgb, *evaluate(y_test, p)))\n",
    "\n",
    "et = ExtraTreesRegressor(**ET_PARAMS)\n",
    "et.fit(X_tr, y_train)\n",
    "p = et.predict(X_te)\n",
    "candidates.append((\"ET\", et, *evaluate(y_test, p)))\n",
    "\n",
    "best = sorted(candidates, key=lambda x: x[2])[0]\n",
    "\n",
    "# =============================\n",
    "# Save\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\"model\": best[1], \"preprocess\": pre, \"name\": best[0]},\n",
    "    os.path.join(SAVE_DIR, \"best_model_amt.joblib\")\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "[DONE]\n",
    "MODEL = {best[0]}\n",
    "MAE   = {best[2]:,.2f}\n",
    "RMSE  = {best[3]:,.2f}\n",
    "R2    = {best[4]:.4f}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de26376",
   "metadata": {},
   "source": [
    "# 7. 하루매출, 동별 매출을 중요시한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "649a4ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF 사용 여부(HOUR=1 딥러닝): True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model_amt\n",
      "\n",
      "[DEBUG] >>> 학습 루프 시작\n",
      "\n",
      "===== HOUR=01 | train=33935 test=8484 =====\n",
      "- DL(H1)    | MAE=1,422,978.290 RMSE=5,048,622.482 R2=0.3067\n",
      "✅ BEST => DL_MLP_DONG_EMB 저장: data/model_amt\\HOUR_01\\best_model.joblib\n",
      "\n",
      "===== HOUR=02 | train=33998 test=8500 =====\n",
      "- XGB       | MAE=248,367.541 RMSE=953,382.150 R2=0.4135\n",
      "✅ BEST => XGB | MAE=248,367.541 저장: data/model_amt\\HOUR_02\\best_model.joblib\n",
      "\n",
      "===== HOUR=03 | train=35715 test=8929 =====\n",
      "- XGB       | MAE=645,458.140 RMSE=2,432,873.733 R2=0.3735\n",
      "✅ BEST => XGB | MAE=645,458.140 저장: data/model_amt\\HOUR_03\\best_model.joblib\n",
      "\n",
      "===== HOUR=04 | train=36360 test=9091 =====\n",
      "- XGB       | MAE=1,821,287.913 RMSE=5,831,440.797 R2=0.8165\n",
      "✅ BEST => XGB | MAE=1,821,287.913 저장: data/model_amt\\HOUR_04\\best_model.joblib\n",
      "\n",
      "===== HOUR=05 | train=36408 test=9102 =====\n",
      "- XGB       | MAE=2,171,725.638 RMSE=6,673,013.365 R2=0.8493\n",
      "✅ BEST => XGB | MAE=2,171,725.638 저장: data/model_amt\\HOUR_05\\best_model.joblib\n",
      "\n",
      "===== HOUR=06 | train=36400 test=9101 =====\n",
      "- XGB       | MAE=1,698,310.130 RMSE=5,559,996.672 R2=0.8802\n",
      "✅ BEST => XGB | MAE=1,698,310.130 저장: data/model_amt\\HOUR_06\\best_model.joblib\n",
      "\n",
      "===== HOUR=07 | train=36412 test=9104 =====\n",
      "- XGB       | MAE=1,987,626.198 RMSE=7,092,356.954 R2=0.8113\n",
      "✅ BEST => XGB | MAE=1,987,626.198 저장: data/model_amt\\HOUR_07\\best_model.joblib\n",
      "\n",
      "===== HOUR=08 | train=36351 test=9088 =====\n",
      "- XGB       | MAE=2,663,182.395 RMSE=7,076,861.935 R2=0.7882\n",
      "✅ BEST => XGB | MAE=2,663,182.395 저장: data/model_amt\\HOUR_08\\best_model.joblib\n",
      "\n",
      "===== HOUR=09 | train=35988 test=8998 =====\n",
      "- XGB       | MAE=1,461,944.132 RMSE=3,571,435.462 R2=0.7866\n",
      "✅ BEST => XGB | MAE=1,461,944.132 저장: data/model_amt\\HOUR_09\\best_model.joblib\n",
      "\n",
      "===== HOUR=10 | train=33247 test=8312 =====\n",
      "- XGB       | MAE=572,524.849 RMSE=1,386,120.188 R2=0.6984\n",
      "✅ BEST => XGB | MAE=572,524.849 저장: data/model_amt\\HOUR_10\\best_model.joblib\n",
      "\n",
      "[DONE] 학습 완료\n",
      "- 결과표 저장: data/model_amt\\results_by_hour.csv\n",
      "- 모델 저장 폴더: data/model_amt\n",
      "- 총 소요(초): 42.7\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# TensorFlow 사용 가능 여부 체크 (HOUR=1 딥러닝용)\n",
    "# =============================\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    HAS_TF = True\n",
    "except Exception:\n",
    "    HAS_TF = False\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "HANSIK_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model_amt\"\n",
    "MAX_HOURS = list(range(1, 11))    # 1~10\n",
    "TEST_RATIO = 0.2                  # 마지막 20% 테스트(시간순)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "OHE_MIN_FREQUENCY = 20\n",
    "OHE_MAX_CATEGORIES = 200\n",
    "\n",
    "LAGS = [1, 7, 14]\n",
    "ROLL_WINDOWS = [7, 14]\n",
    "\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=4000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=7,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "DL_CFG = dict(\n",
    "    epochs=60,\n",
    "    batch_size=1024,\n",
    "    patience=8,\n",
    "    lr=1e-3,\n",
    "    hidden_units=[128, 64],\n",
    "    dropout=0.10\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"DATE\", \"AMT\", \"CNT\", \"UNIT\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate_reg(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return float(mae), float(rmse), float(r2)\n",
    "\n",
    "def safe_onehot_encoder():\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES\n",
    "        )\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "def build_preprocess_xgb(X: pd.DataFrame):\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", safe_onehot_encoder())\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "def build_preprocess_dl_numeric(X: pd.DataFrame):\n",
    "    num_cols = [c for c in X.columns if X[c].dtype != \"object\"]\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    return pipe, num_cols\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if HAS_TF:\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =============================\n",
    "# 1) Load\n",
    "# =============================\n",
    "df = pd.read_csv(HANSIK_PATH)\n",
    "\n",
    "required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"AMT\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"[한식 데이터] 필수 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "# =============================\n",
    "# 2) Date features\n",
    "# =============================\n",
    "df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df = df[df[\"DATE\"].notna()].copy()\n",
    "\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"DAY_OF_MONTH\"] = df[\"DATE\"].dt.day\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# =============================\n",
    "# 3) Lag/Rolling features for AMT (no leakage if time-split)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"]).reset_index(drop=True)\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"], sort=False)\n",
    "\n",
    "for lag in LAGS:\n",
    "    df[f\"AMT_LAG_{lag}\"] = grp[\"AMT\"].shift(lag)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"AMT_ROLL_MEAN_{w}\"] = grp[\"AMT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).mean()\n",
    "    df[f\"AMT_ROLL_STD_{w}\"]  = grp[\"AMT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).std()\n",
    "\n",
    "df[\"AMT_LAG_1_MISSING\"] = df[\"AMT_LAG_1\"].isna().astype(int)\n",
    "\n",
    "# =============================\n",
    "# 3-1) DONG 중요도 강화 (에러 수정 버전)\n",
    "# - transform 기반: df 인덱스에 1:1로 안전하게 붙음\n",
    "# - 누수 방지: shift(1) 이후 expanding\n",
    "# =============================\n",
    "def expanding_mean_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).mean()\n",
    "\n",
    "def expanding_median_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).median()\n",
    "\n",
    "df[\"DONGHOUR_AMT_EXP_MEAN\"] = grp[\"AMT\"].transform(lambda s: expanding_mean_shift1(s, min_periods=5))\n",
    "df[\"DONGHOUR_AMT_EXP_MED\"]  = grp[\"AMT\"].transform(lambda s: expanding_median_shift1(s, min_periods=5))\n",
    "\n",
    "grp_d = df.groupby(\"DONG\", sort=False)\n",
    "df[\"DONG_AMT_EXP_MEAN\"] = grp_d[\"AMT\"].transform(lambda s: expanding_mean_shift1(s, min_periods=10))\n",
    "\n",
    "# =============================\n",
    "# Target / Features\n",
    "# =============================\n",
    "y_all = df[\"AMT\"].copy()\n",
    "X_all = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# 4) Train per HOUR\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "hours = sorted(pd.unique(df[\"HOUR\"].dropna()))\n",
    "hours = [int(h) for h in hours if int(h) in MAX_HOURS]\n",
    "\n",
    "print(f\"[INFO] TF 사용 여부(HOUR=1 딥러닝): {HAS_TF}\")\n",
    "print(f\"[INFO] 학습 시간대: {hours}\")\n",
    "print(f\"[INFO] 저장 폴더: {SAVE_DIR}\")\n",
    "\n",
    "all_results = []\n",
    "t0 = time.time()\n",
    "\n",
    "def train_hour1_deep_learning(X_train, y_train, X_test, y_test, hour_dir):\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow가 없어 딥러닝 학습 불가\")\n",
    "\n",
    "    # train/val split\n",
    "    ntr = len(X_train)\n",
    "    split2 = int(ntr * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2].copy(), X_train.iloc[split2:].copy()\n",
    "    y_tr, y_val = y_train.iloc[:split2].copy(), y_train.iloc[split2:].copy()\n",
    "\n",
    "    # DONG integer encoding\n",
    "    dong_train = X_tr[\"DONG\"].astype(str).values\n",
    "    dong_val   = X_val[\"DONG\"].astype(str).values\n",
    "    dong_test  = X_test[\"DONG\"].astype(str).values\n",
    "\n",
    "    uniq = pd.Index(pd.unique(dong_train))\n",
    "    dong2id = {k: i+1 for i, k in enumerate(uniq)}  # 0: OOV\n",
    "\n",
    "    def map_dong(arr):\n",
    "        return np.array([dong2id.get(x, 0) for x in arr], dtype=np.int32)\n",
    "\n",
    "    dong_tr_id = map_dong(dong_train)\n",
    "    dong_val_id = map_dong(dong_val)\n",
    "    dong_test_id = map_dong(dong_test)\n",
    "\n",
    "    # numeric preprocess\n",
    "    X_tr_num = X_tr.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_val_num = X_val.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_test_num = X_test.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "\n",
    "    num_pipe, num_cols = build_preprocess_dl_numeric(X_tr_num)\n",
    "    X_tr_num_s = num_pipe.fit_transform(X_tr_num[num_cols])\n",
    "    X_val_num_s = num_pipe.transform(X_val_num[num_cols])\n",
    "    X_test_num_s = num_pipe.transform(X_test_num[num_cols])\n",
    "\n",
    "    # model\n",
    "    vocab_size = int(len(dong2id) + 1)\n",
    "    emb_dim = int(min(32, max(8, round(np.sqrt(vocab_size)))))\n",
    "\n",
    "    inp_dong = keras.Input(shape=(1,), dtype=\"int32\", name=\"dong_id\")\n",
    "    x_dong = layers.Embedding(input_dim=vocab_size+1, output_dim=emb_dim, name=\"dong_emb\")(inp_dong)\n",
    "    x_dong = layers.Flatten()(x_dong)\n",
    "\n",
    "    inp_num = keras.Input(shape=(X_tr_num_s.shape[1],), dtype=\"float32\", name=\"num\")\n",
    "    x = layers.Concatenate()([x_dong, inp_num])\n",
    "\n",
    "    for u in DL_CFG[\"hidden_units\"]:\n",
    "        x = layers.Dense(u, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DL_CFG[\"dropout\"])(x)\n",
    "\n",
    "    out = layers.Dense(1, name=\"amt\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[inp_dong, inp_num], outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=DL_CFG[\"lr\"]),\n",
    "        loss=\"mae\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    cb = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=DL_CFG[\"patience\"], restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\", factor=0.7, patience=3, min_lr=1e-5),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        {\"dong_id\": dong_tr_id, \"num\": X_tr_num_s},\n",
    "        y_tr.values,\n",
    "        validation_data=({\"dong_id\": dong_val_id, \"num\": X_val_num_s}, y_val.values),\n",
    "        epochs=DL_CFG[\"epochs\"],\n",
    "        batch_size=DL_CFG[\"batch_size\"],\n",
    "        verbose=0,\n",
    "        callbacks=cb\n",
    "    )\n",
    "\n",
    "    pred = model.predict({\"dong_id\": dong_test_id, \"num\": X_test_num_s}, verbose=0).reshape(-1)\n",
    "    mae, rmse, r2 = evaluate_reg(y_test.values, pred)\n",
    "\n",
    "    # save\n",
    "    model_path = os.path.join(hour_dir, \"dl_model.keras\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    pre_path = os.path.join(hour_dir, \"dl_preprocess.joblib\")\n",
    "    joblib.dump({\"dong2id\": dong2id, \"num_pipe\": num_pipe, \"num_cols\": num_cols}, pre_path)\n",
    "\n",
    "    bundle = {\"model_name\": \"DL_MLP_DONG_EMB\", \"model_path\": model_path, \"preprocess_path\": pre_path, \"type\": \"DL\"}\n",
    "    return bundle, mae, rmse, r2\n",
    "\n",
    "def predict_hour1_dl(bundle, X_new: pd.DataFrame):\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow가 없어 딥러닝 예측 불가\")\n",
    "\n",
    "    from tensorflow import keras\n",
    "    model = keras.models.load_model(bundle[\"model_path\"])\n",
    "    pre = joblib.load(bundle[\"preprocess_path\"])\n",
    "    dong2id = pre[\"dong2id\"]\n",
    "    num_pipe = pre[\"num_pipe\"]\n",
    "    num_cols = pre[\"num_cols\"]\n",
    "\n",
    "    dong_arr = X_new[\"DONG\"].astype(str).values\n",
    "    dong_id = np.array([dong2id.get(x, 0) for x in dong_arr], dtype=np.int32)\n",
    "\n",
    "    X_num = X_new.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_num_s = num_pipe.transform(X_num[num_cols])\n",
    "\n",
    "    pred = model.predict({\"dong_id\": dong_id, \"num\": X_num_s}, verbose=0).reshape(-1)\n",
    "    return pred\n",
    "\n",
    "print(\"\\n[DEBUG] >>> 학습 루프 시작\")\n",
    "for h in hours:\n",
    "    idx = (df[\"HOUR\"].astype(int) == h)\n",
    "    X_h = X_all.loc[idx].copy()\n",
    "    y_h = y_all.loc[idx].copy()\n",
    "    d_h = df.loc[idx, \"DATE\"].copy()\n",
    "\n",
    "    if len(X_h) < 800:\n",
    "        print(f\"\\n[SKIP] HOUR={h:02d}: 데이터가 너무 적음 (n={len(X_h)})\")\n",
    "        continue\n",
    "\n",
    "    # 시간순 split\n",
    "    order = np.argsort(d_h.values)\n",
    "    X_h = X_h.iloc[order].reset_index(drop=True)\n",
    "    y_h = y_h.iloc[order].reset_index(drop=True)\n",
    "\n",
    "    n = len(X_h)\n",
    "    split = int(n * (1 - TEST_RATIO))\n",
    "    X_train, X_test = X_h.iloc[:split], X_h.iloc[split:]\n",
    "    y_train, y_test = y_h.iloc[:split], y_h.iloc[split:]\n",
    "\n",
    "    print(f\"\\n===== HOUR={h:02d} | train={len(X_train)} test={len(X_test)} =====\")\n",
    "\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{h:02d}\")\n",
    "    os.makedirs(hour_dir, exist_ok=True)\n",
    "\n",
    "    # HOUR=1: 딥러닝\n",
    "    if h == 1 and HAS_TF:\n",
    "        try:\n",
    "            bundle, mae, rmse, r2 = train_hour1_deep_learning(X_train, y_train, X_test, y_test, hour_dir)\n",
    "            best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "            joblib.dump(bundle, best_path)\n",
    "\n",
    "            meta = {\n",
    "                \"HOUR\": int(h),\n",
    "                \"BEST_MODEL\": bundle[\"model_name\"],\n",
    "                \"TYPE\": bundle[\"type\"],\n",
    "                \"MAE\": float(mae),\n",
    "                \"RMSE\": float(rmse),\n",
    "                \"R2\": float(r2),\n",
    "                \"TRAIN_SIZE\": int(len(X_train)),\n",
    "                \"TEST_SIZE\": int(len(X_test)),\n",
    "                \"FEATURE_COLS\": list(X_all.columns),\n",
    "                \"DROP_COLS\": DROP_COLS,\n",
    "                \"LAGS\": LAGS,\n",
    "                \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "                \"DL_CFG\": DL_CFG,\n",
    "                \"HAS_TF\": bool(HAS_TF),\n",
    "            }\n",
    "            with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"- DL(H1)    | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "            print(f\"✅ BEST => {bundle['model_name']} 저장: {best_path}\")\n",
    "\n",
    "            all_results.append({\"HOUR\": int(h), \"BEST_MODEL\": bundle[\"model_name\"], \"TYPE\": bundle[\"type\"],\n",
    "                                \"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"SAVE_PATH\": best_path})\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ HOUR=1 딥러닝 실패 -> XGB로 대체. error={e}\")\n",
    "\n",
    "    # XGB (2~10 + HOUR=1 fallback)\n",
    "    split2 = int(len(X_train) * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2], X_train.iloc[split2:]\n",
    "    y_tr, y_val = y_train.iloc[:split2], y_train.iloc[split2:]\n",
    "\n",
    "    preprocess = build_preprocess_xgb(X_train)\n",
    "    pre = clone(preprocess)\n",
    "\n",
    "    X_tr_enc = pre.fit_transform(X_tr)\n",
    "    X_val_enc = pre.transform(X_val)\n",
    "    X_test_enc = pre.transform(X_test)\n",
    "\n",
    "    xgb_params = dict(XGB_PARAMS)\n",
    "    try:\n",
    "        model_xgb = XGBRegressor(**xgb_params)\n",
    "        model_xgb.fit(X_tr_enc, y_tr, eval_set=[(X_val_enc, y_val)], verbose=False, early_stopping_rounds=200)\n",
    "    except Exception:\n",
    "        xgb_params[\"objective\"] = \"reg:squarederror\"\n",
    "        model_xgb = XGBRegressor(**xgb_params)\n",
    "        model_xgb.fit(X_tr_enc, y_tr, eval_set=[(X_val_enc, y_val)], verbose=False, early_stopping_rounds=200)\n",
    "\n",
    "    pred = model_xgb.predict(X_test_enc)\n",
    "    mae, rmse, r2 = evaluate_reg(y_test, pred)\n",
    "    print(f\"- XGB       | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "\n",
    "    best_bundle = {\"preprocess\": pre, \"model\": model_xgb, \"model_name\": \"XGB\", \"type\": \"XGB\"}\n",
    "    best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    joblib.dump(best_bundle, best_path)\n",
    "\n",
    "    meta = {\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": \"XGB\",\n",
    "        \"TYPE\": \"XGB\",\n",
    "        \"MAE\": float(mae),\n",
    "        \"RMSE\": float(rmse),\n",
    "        \"R2\": float(r2),\n",
    "        \"TRAIN_SIZE\": int(len(X_train)),\n",
    "        \"TEST_SIZE\": int(len(X_test)),\n",
    "        \"FEATURE_COLS\": list(X_all.columns),\n",
    "        \"DROP_COLS\": DROP_COLS,\n",
    "        \"LAGS\": LAGS,\n",
    "        \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "        \"HAS_TF\": bool(HAS_TF),\n",
    "        \"XGB_PARAMS\": XGB_PARAMS,\n",
    "    }\n",
    "    with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ BEST => XGB | MAE={mae:,.3f} 저장: {best_path}\")\n",
    "\n",
    "    all_results.append({\"HOUR\": int(h), \"BEST_MODEL\": \"XGB\", \"TYPE\": \"XGB\",\n",
    "                        \"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"SAVE_PATH\": best_path})\n",
    "\n",
    "# 결과 저장\n",
    "res_df = pd.DataFrame(all_results).sort_values([\"HOUR\"])\n",
    "res_path = os.path.join(SAVE_DIR, \"results_by_hour.csv\")\n",
    "res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[DONE] 학습 완료\")\n",
    "print(f\"- 결과표 저장: {res_path}\")\n",
    "print(f\"- 모델 저장 폴더: {SAVE_DIR}\")\n",
    "print(f\"- 총 소요(초): {time.time() - t0:,.1f}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5) 로드/예측 헬퍼\n",
    "# =============================\n",
    "def load_best_bundle(hour: int):\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{hour:02d}\")\n",
    "    path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    if bundle.get(\"type\") == \"DL\":\n",
    "        return predict_hour1_dl(bundle, X_new)\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5df54a",
   "metadata": {},
   "source": [
    "## mae값 낮추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b65f2945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF 사용 여부(HOUR=1 딥러닝): True\n",
      "[INFO] log 타겟 사용: True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model_amt\n",
      "\n",
      "[DEBUG] >>> 학습 루프 시작\n",
      "\n",
      "===== HOUR=01 | train=33935 test=8484 =====\n",
      "- DL(H1)    | MAE=953,988.580 RMSE=2,532,482.998 R2=0.8255\n",
      "✅ BEST => DL_MLP_DONG_EMB 저장: data/model_amt\\HOUR_01\\best_model.joblib\n",
      "\n",
      "===== HOUR=02 | train=33998 test=8500 =====\n",
      "- XGB       | MAE=228,565.487 RMSE=926,586.537 R2=0.4460  (best_iter=730)\n",
      "✅ BEST => XGB | MAE=228,565.487 저장: data/model_amt\\HOUR_02\\best_model.joblib\n",
      "\n",
      "===== HOUR=03 | train=35715 test=8929 =====\n",
      "- XGB       | MAE=593,664.265 RMSE=2,432,275.543 R2=0.3738  (best_iter=534)\n",
      "✅ BEST => XGB | MAE=593,664.265 저장: data/model_amt\\HOUR_03\\best_model.joblib\n",
      "\n",
      "===== HOUR=04 | train=36360 test=9091 =====\n",
      "- XGB       | MAE=1,648,562.293 RMSE=5,964,550.638 R2=0.8080  (best_iter=1003)\n",
      "✅ BEST => XGB | MAE=1,648,562.293 저장: data/model_amt\\HOUR_04\\best_model.joblib\n",
      "\n",
      "===== HOUR=05 | train=36408 test=9102 =====\n",
      "- XGB       | MAE=1,796,913.895 RMSE=5,741,578.858 R2=0.8884  (best_iter=653)\n",
      "✅ BEST => XGB | MAE=1,796,913.895 저장: data/model_amt\\HOUR_05\\best_model.joblib\n",
      "\n",
      "===== HOUR=06 | train=36400 test=9101 =====\n",
      "- XGB       | MAE=1,423,164.339 RMSE=5,450,647.446 R2=0.8849  (best_iter=585)\n",
      "✅ BEST => XGB | MAE=1,423,164.339 저장: data/model_amt\\HOUR_06\\best_model.joblib\n",
      "\n",
      "===== HOUR=07 | train=36412 test=9104 =====\n",
      "- XGB       | MAE=1,627,266.986 RMSE=5,842,416.692 R2=0.8719  (best_iter=821)\n",
      "✅ BEST => XGB | MAE=1,627,266.986 저장: data/model_amt\\HOUR_07\\best_model.joblib\n",
      "\n",
      "===== HOUR=08 | train=36351 test=9088 =====\n",
      "- XGB       | MAE=2,022,603.005 RMSE=4,807,413.111 R2=0.9023  (best_iter=1839)\n",
      "✅ BEST => XGB | MAE=2,022,603.005 저장: data/model_amt\\HOUR_08\\best_model.joblib\n",
      "\n",
      "===== HOUR=09 | train=35988 test=8998 =====\n",
      "- XGB       | MAE=1,131,956.371 RMSE=2,719,898.336 R2=0.8762  (best_iter=2293)\n",
      "✅ BEST => XGB | MAE=1,131,956.371 저장: data/model_amt\\HOUR_09\\best_model.joblib\n",
      "\n",
      "===== HOUR=10 | train=33247 test=8312 =====\n",
      "- XGB       | MAE=445,193.784 RMSE=1,253,300.294 R2=0.7535  (best_iter=799)\n",
      "✅ BEST => XGB | MAE=445,193.784 저장: data/model_amt\\HOUR_10\\best_model.joblib\n",
      "\n",
      "[DONE] 학습 완료\n",
      "- 결과표 저장: data/model_amt\\results_by_hour.csv\n",
      "- 모델 저장 폴더: data/model_amt\n",
      "- 총 소요(초): 1,482.2\n",
      "CPU times: total: 3h 1min 42s\n",
      "Wall time: 24min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# TensorFlow 사용 가능 여부 체크 (HOUR=1 딥러닝용)\n",
    "# =============================\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    HAS_TF = True\n",
    "except Exception:\n",
    "    HAS_TF = False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "HANSIK_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model_amt\"\n",
    "MAX_HOURS = list(range(1, 11))    # 1~10\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ✅ AMT 꼬리/아웃라이어 많으면 MAE 개선에 도움되는 경우가 많음\n",
    "USE_LOG_TARGET = True            # <-- (추가) log1p 타겟 사용\n",
    "\n",
    "# 원핫 폭발 방지\n",
    "OHE_MIN_FREQUENCY = 10           # <-- (완화) 더 많은 카테고리 반영(동 중요도 ↑)\n",
    "OHE_MAX_CATEGORIES = 500\n",
    "\n",
    "# ✅ 피처 강화(조금만)\n",
    "LAGS = [1, 7, 14, 28]            # <-- (추가)\n",
    "ROLL_WINDOWS = [7, 14, 30, 60]   # <-- (추가)\n",
    "\n",
    "# ✅ 30분 정도 쓸 수 있게 XGB를 강하게(early stopping으로 자동 컷)\n",
    "# - learning_rate ↓, n_estimators ↑, early_stopping ↑\n",
    "# - grow_policy=lossguide + max_leaves로 더 강하게(데이터 크면 효율 좋음)\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=30000,          # <-- 크게\n",
    "    learning_rate=0.01,          # <-- 작게\n",
    "    max_depth=0,                # <-- lossguide에서 depth 대신 leaves로\n",
    "    max_leaves=256,             # <-- (추가) 모델 용량\n",
    "    grow_policy=\"lossguide\",    # <-- (추가)\n",
    "    min_child_weight=1,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_lambda=2.0,\n",
    "    reg_alpha=0.1,\n",
    "    objective=\"reg:squarederror\",   # 안정적으로 + eval_metric=mae로 MAE 최적화\n",
    "    eval_metric=\"mae\",\n",
    "    tree_method=\"hist\",\n",
    "    max_bin=512,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 1500     # <-- (증가) 더 오래 학습\n",
    "\n",
    "# ✅ 딥러닝(HOUR=1)도 조금 더 오래/강하게\n",
    "DL_CFG = dict(\n",
    "    epochs=250,                 # <-- 증가\n",
    "    batch_size=512,             # <-- 조금 작게(성능↑ 가능)\n",
    "    patience=25,                # <-- 증가\n",
    "    lr=5e-4,                    # <-- 조금 낮게\n",
    "    hidden_units=[256, 128, 64],# <-- 증가\n",
    "    dropout=0.15\n",
    ")\n",
    "\n",
    "# 누수 위험 제거\n",
    "DROP_COLS = [\"DATE\", \"AMT\", \"CNT\", \"UNIT\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate_reg(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return float(mae), float(rmse), float(r2)\n",
    "\n",
    "def safe_onehot_encoder():\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES\n",
    "        )\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "def build_preprocess_xgb(X: pd.DataFrame):\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", safe_onehot_encoder())\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "def build_preprocess_dl_numeric(X: pd.DataFrame):\n",
    "    num_cols = [c for c in X.columns if X[c].dtype != \"object\"]\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    return pipe, num_cols\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if HAS_TF:\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "def y_transform(y: pd.Series):\n",
    "    \"\"\"학습용 타겟 변환\"\"\"\n",
    "    if USE_LOG_TARGET:\n",
    "        return np.log1p(np.clip(y.values, 0, None))\n",
    "    return y.values.astype(float)\n",
    "\n",
    "def y_inverse(pred: np.ndarray):\n",
    "    \"\"\"예측값 역변환\"\"\"\n",
    "    if USE_LOG_TARGET:\n",
    "        return np.expm1(pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1) Load\n",
    "# =============================\n",
    "df = pd.read_csv(HANSIK_PATH)\n",
    "\n",
    "required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"AMT\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"[한식 데이터] 필수 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "# =============================\n",
    "# 2) Date features\n",
    "# =============================\n",
    "df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df = df[df[\"DATE\"].notna()].copy()\n",
    "\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"DAY_OF_MONTH\"] = df[\"DATE\"].dt.day\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# =============================\n",
    "# 3) Lag/Rolling features for AMT (no leakage)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"]).reset_index(drop=True)\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"], sort=False)\n",
    "\n",
    "for lag in LAGS:\n",
    "    df[f\"AMT_LAG_{lag}\"] = grp[\"AMT\"].shift(lag)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"AMT_ROLL_MEAN_{w}\"] = grp[\"AMT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).mean()\n",
    "    df[f\"AMT_ROLL_STD_{w}\"]  = grp[\"AMT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).std()\n",
    "\n",
    "df[\"AMT_LAG_1_MISSING\"] = df[\"AMT_LAG_1\"].isna().astype(int)\n",
    "\n",
    "# =============================\n",
    "# 3-1) DONG 중요도 강화(누수 방지, IndexError 방지: transform 사용)\n",
    "# =============================\n",
    "def expanding_mean_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).mean()\n",
    "\n",
    "def expanding_median_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).median()\n",
    "\n",
    "def expanding_std_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).std()\n",
    "\n",
    "df[\"DONGHOUR_AMT_EXP_MEAN\"] = grp[\"AMT\"].transform(lambda s: expanding_mean_shift1(s, 5))\n",
    "df[\"DONGHOUR_AMT_EXP_MED\"]  = grp[\"AMT\"].transform(lambda s: expanding_median_shift1(s, 5))\n",
    "df[\"DONGHOUR_AMT_EXP_STD\"]  = grp[\"AMT\"].transform(lambda s: expanding_std_shift1(s, 5))  # <-- (추가)\n",
    "\n",
    "grp_d = df.groupby(\"DONG\", sort=False)\n",
    "df[\"DONG_AMT_EXP_MEAN\"] = grp_d[\"AMT\"].transform(lambda s: expanding_mean_shift1(s, 10))\n",
    "\n",
    "# =============================\n",
    "# Target / Features\n",
    "# =============================\n",
    "y_all = df[\"AMT\"].copy()\n",
    "X_all = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 4) Train per HOUR\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "hours = sorted(pd.unique(df[\"HOUR\"].dropna()))\n",
    "hours = [int(h) for h in hours if int(h) in MAX_HOURS]\n",
    "\n",
    "print(f\"[INFO] TF 사용 여부(HOUR=1 딥러닝): {HAS_TF}\")\n",
    "print(f\"[INFO] log 타겟 사용: {USE_LOG_TARGET}\")\n",
    "print(f\"[INFO] 학습 시간대: {hours}\")\n",
    "print(f\"[INFO] 저장 폴더: {SAVE_DIR}\")\n",
    "\n",
    "all_results = []\n",
    "t0 = time.time()\n",
    "\n",
    "def train_hour1_deep_learning(X_train, y_train_raw, X_test, y_test_raw, hour_dir):\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow가 없어 딥러닝 학습 불가\")\n",
    "\n",
    "    # train/val split (시간순)\n",
    "    ntr = len(X_train)\n",
    "    split2 = int(ntr * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2].copy(), X_train.iloc[split2:].copy()\n",
    "    y_tr_raw, y_val_raw = y_train_raw.iloc[:split2].copy(), y_train_raw.iloc[split2:].copy()\n",
    "\n",
    "    # y transform\n",
    "    y_tr = y_transform(y_tr_raw)\n",
    "    y_val = y_transform(y_val_raw)\n",
    "\n",
    "    # DONG integer encoding\n",
    "    dong_train = X_tr[\"DONG\"].astype(str).values\n",
    "    dong_val   = X_val[\"DONG\"].astype(str).values\n",
    "    dong_test  = X_test[\"DONG\"].astype(str).values\n",
    "\n",
    "    uniq = pd.Index(pd.unique(dong_train))\n",
    "    dong2id = {k: i+1 for i, k in enumerate(uniq)}  # 0 OOV\n",
    "\n",
    "    def map_dong(arr):\n",
    "        return np.array([dong2id.get(x, 0) for x in arr], dtype=np.int32)\n",
    "\n",
    "    dong_tr_id  = map_dong(dong_train)\n",
    "    dong_val_id = map_dong(dong_val)\n",
    "    dong_test_id= map_dong(dong_test)\n",
    "\n",
    "    # numeric preprocess\n",
    "    X_tr_num = X_tr.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_val_num = X_val.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_test_num = X_test.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "\n",
    "    num_pipe, num_cols = build_preprocess_dl_numeric(X_tr_num)\n",
    "    X_tr_num_s = num_pipe.fit_transform(X_tr_num[num_cols])\n",
    "    X_val_num_s = num_pipe.transform(X_val_num[num_cols])\n",
    "    X_test_num_s = num_pipe.transform(X_test_num[num_cols])\n",
    "\n",
    "    # model\n",
    "    vocab_size = int(len(dong2id) + 1)\n",
    "    emb_dim = int(min(64, max(12, round(np.sqrt(vocab_size) * 2))))  # <-- 약간 확대\n",
    "\n",
    "    inp_dong = keras.Input(shape=(1,), dtype=\"int32\", name=\"dong_id\")\n",
    "    x_dong = layers.Embedding(input_dim=vocab_size+1, output_dim=emb_dim, name=\"dong_emb\")(inp_dong)\n",
    "    x_dong = layers.Flatten()(x_dong)\n",
    "\n",
    "    inp_num = keras.Input(shape=(X_tr_num_s.shape[1],), dtype=\"float32\", name=\"num\")\n",
    "    x = layers.Concatenate()([x_dong, inp_num])\n",
    "\n",
    "    x = layers.LayerNormalization()(x)  # <-- (추가) 안정성/성능 개선\n",
    "\n",
    "    for u in DL_CFG[\"hidden_units\"]:\n",
    "        x = layers.Dense(u, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DL_CFG[\"dropout\"])(x)\n",
    "\n",
    "    out = layers.Dense(1, name=\"amt\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[inp_dong, inp_num], outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=DL_CFG[\"lr\"]),\n",
    "        loss=\"mae\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    cb = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=DL_CFG[\"patience\"], restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\", factor=0.7, patience=5, min_lr=1e-5),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        {\"dong_id\": dong_tr_id, \"num\": X_tr_num_s},\n",
    "        y_tr,\n",
    "        validation_data=({\"dong_id\": dong_val_id, \"num\": X_val_num_s}, y_val),\n",
    "        epochs=DL_CFG[\"epochs\"],\n",
    "        batch_size=DL_CFG[\"batch_size\"],\n",
    "        verbose=0,\n",
    "        callbacks=cb\n",
    "    )\n",
    "\n",
    "    # predict + inverse\n",
    "    pred_t = model.predict({\"dong_id\": dong_test_id, \"num\": X_test_num_s}, verbose=0).reshape(-1)\n",
    "    pred = y_inverse(pred_t)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "\n",
    "    mae, rmse, r2 = evaluate_reg(y_test_raw.values, pred)\n",
    "\n",
    "    # save\n",
    "    model_path = os.path.join(hour_dir, \"dl_model.keras\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    pre_path = os.path.join(hour_dir, \"dl_preprocess.joblib\")\n",
    "    joblib.dump({\"dong2id\": dong2id, \"num_pipe\": num_pipe, \"num_cols\": num_cols}, pre_path)\n",
    "\n",
    "    bundle = {\"model_name\": \"DL_MLP_DONG_EMB\", \"model_path\": model_path, \"preprocess_path\": pre_path, \"type\": \"DL\"}\n",
    "    return bundle, mae, rmse, r2\n",
    "\n",
    "def predict_hour1_dl(bundle, X_new: pd.DataFrame):\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow가 없어 딥러닝 예측 불가\")\n",
    "\n",
    "    from tensorflow import keras\n",
    "    model = keras.models.load_model(bundle[\"model_path\"])\n",
    "    pre = joblib.load(bundle[\"preprocess_path\"])\n",
    "    dong2id = pre[\"dong2id\"]\n",
    "    num_pipe = pre[\"num_pipe\"]\n",
    "    num_cols = pre[\"num_cols\"]\n",
    "\n",
    "    dong_arr = X_new[\"DONG\"].astype(str).values\n",
    "    dong_id = np.array([dong2id.get(x, 0) for x in dong_arr], dtype=np.int32)\n",
    "\n",
    "    X_num = X_new.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_num_s = num_pipe.transform(X_num[num_cols])\n",
    "\n",
    "    pred_t = model.predict({\"dong_id\": dong_id, \"num\": X_num_s}, verbose=0).reshape(-1)\n",
    "    pred = y_inverse(pred_t)\n",
    "    return np.clip(pred, 0, None)\n",
    "\n",
    "print(\"\\n[DEBUG] >>> 학습 루프 시작\")\n",
    "for h in hours:\n",
    "    idx = (df[\"HOUR\"].astype(int) == h)\n",
    "    X_h = X_all.loc[idx].copy()\n",
    "    y_h_raw = y_all.loc[idx].copy()\n",
    "    d_h = df.loc[idx, \"DATE\"].copy()\n",
    "\n",
    "    if len(X_h) < 800:\n",
    "        print(f\"\\n[SKIP] HOUR={h:02d}: 데이터가 너무 적음 (n={len(X_h)})\")\n",
    "        continue\n",
    "\n",
    "    # 시간순 split\n",
    "    order = np.argsort(d_h.values)\n",
    "    X_h = X_h.iloc[order].reset_index(drop=True)\n",
    "    y_h_raw = y_h_raw.iloc[order].reset_index(drop=True)\n",
    "\n",
    "    n = len(X_h)\n",
    "    split = int(n * (1 - TEST_RATIO))\n",
    "    X_train, X_test = X_h.iloc[:split], X_h.iloc[split:]\n",
    "    y_train_raw, y_test_raw = y_h_raw.iloc[:split], y_h_raw.iloc[split:]\n",
    "\n",
    "    print(f\"\\n===== HOUR={h:02d} | train={len(X_train)} test={len(X_test)} =====\")\n",
    "\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{h:02d}\")\n",
    "    os.makedirs(hour_dir, exist_ok=True)\n",
    "\n",
    "    # HOUR=1: 딥러닝\n",
    "    if h == 1 and HAS_TF:\n",
    "        try:\n",
    "            bundle, mae, rmse, r2 = train_hour1_deep_learning(X_train, y_train_raw, X_test, y_test_raw, hour_dir)\n",
    "            best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "            joblib.dump(bundle, best_path)\n",
    "\n",
    "            meta = {\n",
    "                \"HOUR\": int(h),\n",
    "                \"BEST_MODEL\": bundle[\"model_name\"],\n",
    "                \"TYPE\": bundle[\"type\"],\n",
    "                \"MAE\": float(mae),\n",
    "                \"RMSE\": float(rmse),\n",
    "                \"R2\": float(r2),\n",
    "                \"TRAIN_SIZE\": int(len(X_train)),\n",
    "                \"TEST_SIZE\": int(len(X_test)),\n",
    "                \"FEATURE_COLS\": list(X_all.columns),\n",
    "                \"DROP_COLS\": DROP_COLS,\n",
    "                \"LAGS\": LAGS,\n",
    "                \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "                \"DL_CFG\": DL_CFG,\n",
    "                \"USE_LOG_TARGET\": USE_LOG_TARGET,\n",
    "                \"HAS_TF\": bool(HAS_TF),\n",
    "            }\n",
    "            with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"- DL(H1)    | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "            print(f\"✅ BEST => {bundle['model_name']} 저장: {best_path}\")\n",
    "\n",
    "            all_results.append({\"HOUR\": int(h), \"BEST_MODEL\": bundle[\"model_name\"], \"TYPE\": bundle[\"type\"],\n",
    "                                \"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"SAVE_PATH\": best_path})\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ HOUR=1 딥러닝 실패 -> XGB로 대체. error={e}\")\n",
    "\n",
    "    # XGB (2~10 + HOUR=1 fallback)\n",
    "    # train 마지막 10%를 valid로 early stopping\n",
    "    split2 = int(len(X_train) * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2], X_train.iloc[split2:]\n",
    "    y_tr_raw, y_val_raw = y_train_raw.iloc[:split2], y_train_raw.iloc[split2:]\n",
    "\n",
    "    # y transform\n",
    "    y_tr = y_transform(y_tr_raw)\n",
    "    y_val = y_transform(y_val_raw)\n",
    "\n",
    "    preprocess = build_preprocess_xgb(X_train)\n",
    "    pre = clone(preprocess)\n",
    "\n",
    "    X_tr_enc = pre.fit_transform(X_tr)\n",
    "    X_val_enc = pre.transform(X_val)\n",
    "    X_test_enc = pre.transform(X_test)\n",
    "\n",
    "    model_xgb = XGBRegressor(**XGB_PARAMS)\n",
    "    model_xgb.fit(\n",
    "        X_tr_enc, y_tr,\n",
    "        eval_set=[(X_val_enc, y_val)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    "    )\n",
    "\n",
    "    pred_t = model_xgb.predict(X_test_enc)\n",
    "    pred = y_inverse(pred_t)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "\n",
    "    mae, rmse, r2 = evaluate_reg(y_test_raw.values, pred)\n",
    "    print(f\"- XGB       | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}  (best_iter={getattr(model_xgb, 'best_iteration', None)})\")\n",
    "\n",
    "    best_bundle = {\"preprocess\": pre, \"model\": model_xgb, \"model_name\": \"XGB\", \"type\": \"XGB\"}\n",
    "    best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    joblib.dump(best_bundle, best_path)\n",
    "\n",
    "    meta = {\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": \"XGB\",\n",
    "        \"TYPE\": \"XGB\",\n",
    "        \"MAE\": float(mae),\n",
    "        \"RMSE\": float(rmse),\n",
    "        \"R2\": float(r2),\n",
    "        \"TRAIN_SIZE\": int(len(X_train)),\n",
    "        \"TEST_SIZE\": int(len(X_test)),\n",
    "        \"FEATURE_COLS\": list(X_all.columns),\n",
    "        \"DROP_COLS\": DROP_COLS,\n",
    "        \"LAGS\": LAGS,\n",
    "        \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "        \"USE_LOG_TARGET\": USE_LOG_TARGET,\n",
    "        \"HAS_TF\": bool(HAS_TF),\n",
    "        \"XGB_PARAMS\": XGB_PARAMS,\n",
    "        \"EARLY_STOPPING_ROUNDS\": EARLY_STOPPING_ROUNDS,\n",
    "    }\n",
    "    with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ BEST => XGB | MAE={mae:,.3f} 저장: {best_path}\")\n",
    "\n",
    "    all_results.append({\"HOUR\": int(h), \"BEST_MODEL\": \"XGB\", \"TYPE\": \"XGB\",\n",
    "                        \"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"SAVE_PATH\": best_path})\n",
    "\n",
    "# 결과 저장\n",
    "res_df = pd.DataFrame(all_results).sort_values([\"HOUR\"])\n",
    "res_path = os.path.join(SAVE_DIR, \"results_by_hour.csv\")\n",
    "res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[DONE] 학습 완료\")\n",
    "print(f\"- 결과표 저장: {res_path}\")\n",
    "print(f\"- 모델 저장 폴더: {SAVE_DIR}\")\n",
    "print(f\"- 총 소요(초): {time.time() - t0:,.1f}\")\n",
    "\n",
    "# =============================\n",
    "# 5) 로드/예측 헬퍼\n",
    "# =============================\n",
    "def load_best_bundle(hour: int):\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{hour:02d}\")\n",
    "    path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    if bundle.get(\"type\") == \"DL\":\n",
    "        return predict_hour1_dl(bundle, X_new)\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    pred_t = model.predict(X_enc)\n",
    "    pred = y_inverse(pred_t)\n",
    "    return np.clip(pred, 0, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95797b18",
   "metadata": {},
   "source": [
    "## mae값 낮추고 성능 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b78efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF 사용 여부(HOUR=1 딥러닝): False\n",
      "[INFO] log 타겟 사용: True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model_amt_xgb\n",
      "\n",
      "[DEBUG] >>> 학습 루프 시작\n",
      "\n",
      "===== HOUR=01 | train=33935 test=8484 =====\n",
      "  - XGB cand#1  | MAE=822,312.078 RMSE=2,196,652.914 R2=0.8687 (best_iter=26)\n",
      "  - XGB cand#2  | MAE=912,250.451 RMSE=2,852,431.266 R2=0.7787 (best_iter=8)\n",
      "  - XGB cand#3  | MAE=905,422.539 RMSE=2,711,424.367 R2=0.8000 (best_iter=6)\n",
      "  - XGB cand#4  | MAE=836,157.679 RMSE=2,120,713.352 R2=0.8777 (best_iter=19)\n",
      "  - XGB cand#5  | MAE=822,312.078 RMSE=2,196,652.914 R2=0.8687 (best_iter=26)\n",
      "  - XGB cand#6  | MAE=909,725.543 RMSE=3,254,827.312 R2=0.7118 (best_iter=26)\n",
      "  - XGB cand#7  | MAE=909,725.543 RMSE=3,254,827.312 R2=0.7118 (best_iter=26)\n",
      "  - XGB cand#8  | MAE=905,422.539 RMSE=2,711,424.367 R2=0.8000 (best_iter=6)\n",
      "✅ BEST => XGB | MAE=822,312.078 저장: data/model_amt_xgb\\HOUR_01\\best_model.joblib\n",
      "\n",
      "===== HOUR=02 | train=33998 test=8500 =====\n",
      "  - XGB cand#1  | MAE=261,688.021 RMSE=972,487.811 R2=0.3899 (best_iter=3)\n",
      "  - XGB cand#2  | MAE=256,874.565 RMSE=982,558.696 R2=0.3772 (best_iter=4)\n",
      "  - XGB cand#3  | MAE=257,266.445 RMSE=962,346.221 R2=0.4025 (best_iter=14)\n",
      "  - XGB cand#4  | MAE=259,534.302 RMSE=966,739.048 R2=0.3971 (best_iter=2)\n",
      "  - XGB cand#5  | MAE=261,530.333 RMSE=984,631.688 R2=0.3745 (best_iter=9)\n",
      "  - XGB cand#6  | MAE=257,266.445 RMSE=962,346.221 R2=0.4025 (best_iter=14)\n",
      "  - XGB cand#7  | MAE=256,874.565 RMSE=982,558.696 R2=0.3772 (best_iter=4)\n",
      "  - XGB cand#8  | MAE=261,688.021 RMSE=972,487.811 R2=0.3899 (best_iter=3)\n",
      "✅ BEST => XGB | MAE=256,874.565 저장: data/model_amt_xgb\\HOUR_02\\best_model.joblib\n",
      "\n",
      "===== HOUR=03 | train=35715 test=8929 =====\n",
      "  - XGB cand#1  | MAE=682,509.947 RMSE=2,504,715.949 R2=0.3359 (best_iter=12)\n",
      "  - XGB cand#2  | MAE=656,765.666 RMSE=2,479,290.522 R2=0.3493 (best_iter=16)\n",
      "  - XGB cand#3  | MAE=639,996.913 RMSE=2,430,288.909 R2=0.3748 (best_iter=37)\n",
      "  - XGB cand#4  | MAE=639,996.913 RMSE=2,430,288.909 R2=0.3748 (best_iter=37)\n",
      "  - XGB cand#5  | MAE=682,509.947 RMSE=2,504,715.949 R2=0.3359 (best_iter=12)\n",
      "  - XGB cand#6  | MAE=674,131.260 RMSE=2,487,065.349 R2=0.3452 (best_iter=13)\n",
      "  - XGB cand#7  | MAE=655,843.323 RMSE=2,472,861.417 R2=0.3527 (best_iter=24)\n",
      "  - XGB cand#8  | MAE=656,765.666 RMSE=2,479,290.522 R2=0.3493 (best_iter=16)\n",
      "✅ BEST => XGB | MAE=639,996.913 저장: data/model_amt_xgb\\HOUR_03\\best_model.joblib\n",
      "\n",
      "===== HOUR=04 | train=36360 test=9091 =====\n",
      "  - XGB cand#1  | MAE=1,896,642.822 RMSE=6,233,682.771 R2=0.7903 (best_iter=16)\n",
      "  - XGB cand#2  | MAE=1,810,466.572 RMSE=6,149,855.740 R2=0.7959 (best_iter=109)\n",
      "  - XGB cand#3  | MAE=1,815,739.512 RMSE=6,013,522.155 R2=0.8049 (best_iter=47)\n",
      "  - XGB cand#4  | MAE=1,815,739.512 RMSE=6,013,522.155 R2=0.8049 (best_iter=47)\n",
      "  - XGB cand#5  | MAE=1,902,877.303 RMSE=6,222,229.570 R2=0.7911 (best_iter=19)\n",
      "  - XGB cand#6  | MAE=1,810,466.572 RMSE=6,149,855.740 R2=0.7959 (best_iter=109)\n",
      "  - XGB cand#7  | MAE=2,017,260.072 RMSE=6,396,944.316 R2=0.7792 (best_iter=22)\n",
      "  - XGB cand#8  | MAE=1,902,877.303 RMSE=6,222,229.570 R2=0.7911 (best_iter=19)\n",
      "✅ BEST => XGB | MAE=1,810,466.572 저장: data/model_amt_xgb\\HOUR_04\\best_model.joblib\n",
      "\n",
      "===== HOUR=05 | train=36408 test=9102 =====\n",
      "  - XGB cand#1  | MAE=2,239,234.785 RMSE=6,579,767.637 R2=0.8535 (best_iter=22)\n",
      "  - XGB cand#2  | MAE=2,191,414.247 RMSE=6,455,020.560 R2=0.8590 (best_iter=73)\n",
      "  - XGB cand#3  | MAE=2,219,630.020 RMSE=6,402,885.928 R2=0.8613 (best_iter=14)\n",
      "  - XGB cand#4  | MAE=2,239,234.785 RMSE=6,579,767.637 R2=0.8535 (best_iter=22)\n",
      "  - XGB cand#5  | MAE=2,219,630.020 RMSE=6,402,885.928 R2=0.8613 (best_iter=14)\n",
      "  - XGB cand#6  | MAE=2,191,414.247 RMSE=6,455,020.560 R2=0.8590 (best_iter=73)\n",
      "  - XGB cand#7  | MAE=2,233,937.712 RMSE=6,432,692.332 R2=0.8600 (best_iter=22)\n",
      "  - XGB cand#8  | MAE=2,263,873.072 RMSE=6,569,589.344 R2=0.8539 (best_iter=20)\n",
      "✅ BEST => XGB | MAE=2,191,414.247 저장: data/model_amt_xgb\\HOUR_05\\best_model.joblib\n",
      "\n",
      "===== HOUR=06 | train=36400 test=9101 =====\n",
      "  - XGB cand#1  | MAE=1,749,304.105 RMSE=6,218,931.691 R2=0.8502 (best_iter=10)\n",
      "  - XGB cand#2  | MAE=1,776,558.937 RMSE=6,315,536.702 R2=0.8455 (best_iter=15)\n",
      "  - XGB cand#3  | MAE=1,813,905.368 RMSE=6,406,990.111 R2=0.8410 (best_iter=10)\n",
      "  - XGB cand#4  | MAE=1,753,421.795 RMSE=6,343,714.606 R2=0.8441 (best_iter=23)\n",
      "  - XGB cand#5  | MAE=1,749,304.105 RMSE=6,218,931.691 R2=0.8502 (best_iter=10)\n",
      "  - XGB cand#6  | MAE=1,662,010.147 RMSE=5,910,172.609 R2=0.8647 (best_iter=70)\n",
      "  - XGB cand#7  | MAE=1,776,558.937 RMSE=6,315,536.702 R2=0.8455 (best_iter=15)\n",
      "  - XGB cand#8  | MAE=1,662,010.147 RMSE=5,910,172.609 R2=0.8647 (best_iter=70)\n",
      "✅ BEST => XGB | MAE=1,662,010.147 저장: data/model_amt_xgb\\HOUR_06\\best_model.joblib\n",
      "\n",
      "===== HOUR=07 | train=36412 test=9104 =====\n",
      "  - XGB cand#1  | MAE=2,017,875.407 RMSE=6,894,422.481 R2=0.8217 (best_iter=19)\n",
      "  - XGB cand#2  | MAE=1,986,634.301 RMSE=6,689,677.915 R2=0.8321 (best_iter=22)\n",
      "  - XGB cand#3  | MAE=1,991,321.468 RMSE=6,638,859.911 R2=0.8346 (best_iter=34)\n",
      "  - XGB cand#4  | MAE=2,036,838.303 RMSE=7,013,098.926 R2=0.8155 (best_iter=5)\n",
      "  - XGB cand#5  | MAE=1,984,516.378 RMSE=6,613,881.767 R2=0.8359 (best_iter=17)\n",
      "  - XGB cand#6  | MAE=1,986,634.301 RMSE=6,689,677.915 R2=0.8321 (best_iter=22)\n",
      "  - XGB cand#7  | MAE=1,984,516.378 RMSE=6,613,881.767 R2=0.8359 (best_iter=17)\n",
      "  - XGB cand#8  | MAE=2,017,875.407 RMSE=6,894,422.481 R2=0.8217 (best_iter=19)\n",
      "✅ BEST => XGB | MAE=1,984,516.378 저장: data/model_amt_xgb\\HOUR_07\\best_model.joblib\n",
      "\n",
      "===== HOUR=08 | train=36351 test=9088 =====\n",
      "  - XGB cand#1  | MAE=2,525,593.883 RMSE=5,828,232.003 R2=0.8564 (best_iter=35)\n",
      "  - XGB cand#2  | MAE=2,525,593.883 RMSE=5,828,232.003 R2=0.8564 (best_iter=35)\n",
      "  - XGB cand#3  | MAE=2,546,949.459 RMSE=5,741,006.596 R2=0.8607 (best_iter=23)\n",
      "  - XGB cand#4  | MAE=2,461,756.374 RMSE=5,730,910.298 R2=0.8611 (best_iter=17)\n",
      "  - XGB cand#5  | MAE=2,381,864.070 RMSE=5,750,523.376 R2=0.8602 (best_iter=89)\n",
      "  - XGB cand#6  | MAE=2,461,756.374 RMSE=5,730,910.298 R2=0.8611 (best_iter=17)\n",
      "  - XGB cand#7  | MAE=2,621,702.537 RMSE=6,391,309.955 R2=0.8273 (best_iter=9)\n",
      "  - XGB cand#8  | MAE=2,381,864.070 RMSE=5,750,523.376 R2=0.8602 (best_iter=89)\n",
      "✅ BEST => XGB | MAE=2,381,864.070 저장: data/model_amt_xgb\\HOUR_08\\best_model.joblib\n",
      "\n",
      "===== HOUR=09 | train=35988 test=8998 =====\n",
      "  - XGB cand#1  | MAE=1,374,403.441 RMSE=3,149,749.586 R2=0.8342 (best_iter=32)\n",
      "  - XGB cand#2  | MAE=1,413,935.884 RMSE=3,309,787.008 R2=0.8169 (best_iter=13)\n",
      "  - XGB cand#3  | MAE=1,374,403.441 RMSE=3,149,749.586 R2=0.8342 (best_iter=32)\n",
      "  - XGB cand#4  | MAE=1,443,972.008 RMSE=3,374,046.448 R2=0.8097 (best_iter=16)\n",
      "  - XGB cand#5  | MAE=1,304,735.205 RMSE=2,971,363.253 R2=0.8524 (best_iter=58)\n",
      "  - XGB cand#6  | MAE=1,413,935.884 RMSE=3,309,787.008 R2=0.8169 (best_iter=13)\n",
      "  - XGB cand#7  | MAE=1,304,735.205 RMSE=2,971,363.253 R2=0.8524 (best_iter=58)\n",
      "  - XGB cand#8  | MAE=1,402,703.722 RMSE=3,186,885.576 R2=0.8303 (best_iter=45)\n",
      "✅ BEST => XGB | MAE=1,304,735.205 저장: data/model_amt_xgb\\HOUR_09\\best_model.joblib\n",
      "\n",
      "===== HOUR=10 | train=33247 test=8312 =====\n",
      "  - XGB cand#1  | MAE=527,239.645 RMSE=1,474,473.893 R2=0.6592 (best_iter=15)\n",
      "  - XGB cand#2  | MAE=517,848.401 RMSE=1,466,605.700 R2=0.6628 (best_iter=22)\n",
      "  - XGB cand#3  | MAE=517,848.401 RMSE=1,466,605.700 R2=0.6628 (best_iter=22)\n",
      "  - XGB cand#4  | MAE=538,398.260 RMSE=1,462,909.584 R2=0.6645 (best_iter=11)\n",
      "  - XGB cand#5  | MAE=534,346.098 RMSE=1,489,441.176 R2=0.6522 (best_iter=14)\n",
      "  - XGB cand#6  | MAE=527,239.645 RMSE=1,474,473.893 R2=0.6592 (best_iter=15)\n",
      "  - XGB cand#7  | MAE=498,276.545 RMSE=1,324,404.476 R2=0.7250 (best_iter=8)\n",
      "  - XGB cand#8  | MAE=498,276.545 RMSE=1,324,404.476 R2=0.7250 (best_iter=8)\n",
      "✅ BEST => XGB | MAE=498,276.545 저장: data/model_amt_xgb\\HOUR_10\\best_model.joblib\n",
      "\n",
      "[DONE] 학습 완료\n",
      "- 결과표 저장: data/model_amt_xgb\\results_by_hour.csv\n",
      "- 모델 저장 폴더: data/model_amt_xgb\n",
      "- 총 소요(초): 1,015.8\n",
      "CPU times: total: 2h 6min 34s\n",
      "Wall time: 16min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    DATA_PATH: str = \"data/수원시 한식 동별 데이터백업.csv\"          \n",
    "    SAVE_ROOT: str = \"data/model_amt_xgb\"          \n",
    "    USE_LOG_TARGET: bool = True                  \n",
    "\n",
    "    # 시간대 1~10 (항목요약)\n",
    "    HOURS: Tuple[int, ...] = tuple(range(1, 11))\n",
    "    TEST_RATIO: float = 0.2                       # 마지막 20% 테스트(시간순)\n",
    "\n",
    "    # 동별 성능 핵심: dong+hour 기준 lag/rolling\n",
    "    LAGS: Tuple[int, ...] = (1, 2, 3, 7)\n",
    "    ROLL_WINDOWS: Tuple[int, ...] = (3, 7)\n",
    "\n",
    "    RANDOM_STATE: int = 42\n",
    "    TOTAL_TIME_BUDGET_SEC: int = 1200             # 20분\n",
    "\n",
    "    # 진행 로그 출력\n",
    "    VERBOSE: bool = True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def log(msg: str, cfg: CFG):\n",
    "    if cfg.VERBOSE:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "def to_dt(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.replace(\"-\", \"\", regex=False)\n",
    "    return pd.to_datetime(s, format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "def time_split_last_ratio(df: pd.DataFrame, dt_col: str, test_ratio: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = df.sort_values(dt_col).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    cut = int(np.floor(n * (1.0 - test_ratio)))\n",
    "    cut = max(1, min(cut, n - 1))\n",
    "    return df.iloc[:cut].copy(), df.iloc[cut:].copy()\n",
    "\n",
    "def add_lag_rolling(df: pd.DataFrame, target: str, group_cols: List[str], dt_col: str,\n",
    "                    lags: Tuple[int, ...], wins: Tuple[int, ...]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    dong+hour 기준 lag/rolling → 동별 예측 성능에 가장 크게 기여\n",
    "    \"\"\"\n",
    "    df = df.sort_values(group_cols + [dt_col]).reset_index(drop=True)\n",
    "    g = df.groupby(group_cols, dropna=False)[target]\n",
    "\n",
    "    for k in lags:\n",
    "        df[f\"{target}_lag{k}\"] = g.shift(k)\n",
    "\n",
    "    for w in wins:\n",
    "        s = g.shift(1)\n",
    "        df[f\"{target}_rollmean{w}\"] = s.rolling(w).mean()\n",
    "        df[f\"{target}_rollmax{w}\"] = s.rolling(w).max()\n",
    "        df[f\"{target}_rollmin{w}\"] = s.rolling(w).min()\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_preprocess(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    # 숫자/범주 자동 분리 (DONG 같은 문자열은 cat)\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"string\")]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True)),\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "def make_param_candidates(seed: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    20분 안에 끝내기 위해 랜덤서치 대신 '잘 먹는' 후보 파라미터 8개 정도만 평가.\n",
    "    (각 후보는 early stopping으로 best_iter로 컷)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    cands = []\n",
    "\n",
    "    # base families (fast + strong)\n",
    "    for max_depth in [4, 6, 8]:\n",
    "        for lr in [0.03, 0.05]:\n",
    "            cands.append(dict(\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=lr,\n",
    "                subsample=0.85,\n",
    "                colsample_bytree=0.85,\n",
    "                min_child_weight=1.0,\n",
    "                reg_alpha=0.0,\n",
    "                reg_lambda=1.0,\n",
    "                gamma=0.0,\n",
    "            ))\n",
    "\n",
    "    # 조금 더 regularized / robust\n",
    "    cands += [\n",
    "        dict(max_depth=6, learning_rate=0.03, subsample=0.75, colsample_bytree=0.75, min_child_weight=3.0, reg_alpha=0.0, reg_lambda=5.0, gamma=0.0),\n",
    "        dict(max_depth=8, learning_rate=0.02, subsample=0.75, colsample_bytree=0.85, min_child_weight=5.0, reg_alpha=1e-3, reg_lambda=10.0, gamma=0.0),\n",
    "    ]\n",
    "\n",
    "    # shuffle a bit\n",
    "    rng.shuffle(cands)\n",
    "    return cands[:8]\n",
    "\n",
    "\n",
    "def fit_xgb_one(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_valid: pd.DataFrame,\n",
    "    y_valid: np.ndarray,\n",
    "    preprocess: ColumnTransformer,\n",
    "    params: Dict,\n",
    "    seed: int\n",
    "):\n",
    "    \"\"\"\n",
    "    preprocess → transform → XGB early stopping\n",
    "    \"\"\"\n",
    "    Xtr = preprocess.fit_transform(X_train, y_train)\n",
    "    Xva = preprocess.transform(X_valid)\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective=\"reg:absoluteerror\",\n",
    "        n_estimators=8000,                 # 크게 두고 early stopping으로 best_iter 결정\n",
    "        tree_method=\"hist\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "\n",
    "        # tuned params\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, y_train,\n",
    "        eval_set=[(Xva, y_valid)],\n",
    "        eval_metric=\"mae\",\n",
    "        early_stopping_rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    return preprocess, model\n",
    "\n",
    "\n",
    "def eval_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"mae\": float(mae), \"rmse\": float(rmse), \"r2\": float(r2)}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main Training Loop (HOUR 1~10)\n",
    "# =========================\n",
    "def train_all_hours_xgb(cfg: CFG):\n",
    "    t_all0 = time.time()\n",
    "    os.makedirs(cfg.SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df = pd.read_csv(cfg.DATA_PATH)\n",
    "    required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"AMT\", \"CNT\", \"UNIT\", \"TEMP\", \"RAIN\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"데이터 컬럼 누락: {missing}. 현재 컬럼={list(df.columns)}\")\n",
    "\n",
    "    df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "    df[\"_dt\"] = to_dt(df[\"TA_YMD\"])\n",
    "    if df[\"_dt\"].isna().all():\n",
    "        raise ValueError(\"TA_YMD 날짜 파싱 실패. 예: 20251231 형태여야 합니다.\")\n",
    "\n",
    "    # ---------- Info ----------\n",
    "    log(f\"[INFO] TF 사용 여부(HOUR=1 딥러닝): False\", cfg)\n",
    "    log(f\"[INFO] log 타겟 사용: {cfg.USE_LOG_TARGET}\", cfg)\n",
    "    log(f\"[INFO] 학습 시간대: {list(cfg.HOURS)}\", cfg)\n",
    "    log(f\"[INFO] 저장 폴더: {cfg.SAVE_ROOT}\\n\", cfg)\n",
    "    log(\"[DEBUG] >>> 학습 루프 시작\\n\", cfg)\n",
    "\n",
    "    # 결과표\n",
    "    results_rows = []\n",
    "\n",
    "    # 시간 예산을 시간대별로 쪼개기(대략 균등)\n",
    "    per_hour_budget = max(60, int(cfg.TOTAL_TIME_BUDGET_SEC / len(cfg.HOURS)))\n",
    "\n",
    "    # ---------- loop each hour ----------\n",
    "    for hour in cfg.HOURS:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # hour filter\n",
    "        d = df[df[\"HOUR\"] == hour].copy()\n",
    "        d = d.sort_values(\"_dt\").reset_index(drop=True)\n",
    "\n",
    "        # 시간기반 split\n",
    "        train_df, test_df = time_split_last_ratio(d, \"_dt\", cfg.TEST_RATIO)\n",
    "\n",
    "        log(f\"===== HOUR={hour:02d} | train={len(train_df)} test={len(test_df)} =====\", cfg)\n",
    "\n",
    "        # dong+hour 기준 lag/rolling (동별 성능)\n",
    "        # hour는 고정이지만 group_cols에 넣으면 동별 시계열로 계산되므로 DONG만 넣어도 동일.\n",
    "        train_df2 = add_lag_rolling(train_df, \"AMT\", group_cols=[\"DONG\"], dt_col=\"_dt\",\n",
    "                                    lags=cfg.LAGS, wins=cfg.ROLL_WINDOWS)\n",
    "        test_df2 = add_lag_rolling(test_df, \"AMT\", group_cols=[\"DONG\"], dt_col=\"_dt\",\n",
    "                                   lags=cfg.LAGS, wins=cfg.ROLL_WINDOWS)\n",
    "\n",
    "        # feature set (UNIT/CNT는 타겟과 유사해 누수 가능성이 커서 기본은 제외)\n",
    "        # 필요하면 아래 drop에서 빼도 됨.\n",
    "        drop_cols = [\"AMT\", \"_dt\"]  # target + helper\n",
    "        # 동별 중요: DONG 포함\n",
    "        feature_cols = [c for c in train_df2.columns if c not in drop_cols]\n",
    "\n",
    "        # (권장) 누수/의미 애매한 컬럼 제거\n",
    "        # - UNIT/CNT가 \"실제 결과\"에 가까운 값이면 예측 시점에 없으므로 빼야 함\n",
    "        for leak in [\"UNIT\", \"CNT\"]:\n",
    "            if leak in feature_cols:\n",
    "                feature_cols.remove(leak)\n",
    "\n",
    "        X_train_full = train_df2[feature_cols].copy()\n",
    "        y_train_full = train_df2[\"AMT\"].astype(float).values\n",
    "\n",
    "        X_test = test_df2[feature_cols].copy()\n",
    "        y_test = test_df2[\"AMT\"].astype(float).values\n",
    "\n",
    "        # train 내부 valid 분리(마지막 15%를 valid)\n",
    "        ntr = len(X_train_full)\n",
    "        vcut = int(ntr * 0.85)\n",
    "        vcut = max(1, min(vcut, ntr - 1))\n",
    "\n",
    "        X_train = X_train_full.iloc[:vcut].copy()\n",
    "        y_train = y_train_full[:vcut]\n",
    "\n",
    "        X_valid = X_train_full.iloc[vcut:].copy()\n",
    "        y_valid = y_train_full[vcut:]\n",
    "\n",
    "        # log target\n",
    "        if cfg.USE_LOG_TARGET:\n",
    "            y_train_fit = np.log1p(y_train)\n",
    "            y_valid_fit = np.log1p(y_valid)\n",
    "        else:\n",
    "            y_train_fit = y_train\n",
    "            y_valid_fit = y_valid\n",
    "\n",
    "        # 후보 파라미터들\n",
    "        cands = make_param_candidates(cfg.RANDOM_STATE + hour)\n",
    "\n",
    "        best = None\n",
    "        best_mae = float(\"inf\")\n",
    "        best_info = None\n",
    "\n",
    "        # preprocess는 후보마다 새로 만드는 게 안전(컬럼 동일하지만 fit 상태 분리)\n",
    "        # 시간 예산 체크하면서 후보 평가\n",
    "        for i, p in enumerate(cands, start=1):\n",
    "            if time.time() - t0 > per_hour_budget:\n",
    "                log(f\"  [WARN] 시간 예산 초과로 후보 평가 조기 종료 (i={i-1}/{len(cands)})\", cfg)\n",
    "                break\n",
    "\n",
    "            pre = build_preprocess(X_train_full)\n",
    "            pre_fitted, model = fit_xgb_one(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train_fit,\n",
    "                X_valid=X_valid,\n",
    "                y_valid=y_valid_fit,\n",
    "                preprocess=pre,\n",
    "                params=p,\n",
    "                seed=cfg.RANDOM_STATE + hour\n",
    "            )\n",
    "\n",
    "            # test 평가\n",
    "            Xte_enc = pre_fitted.transform(X_test)\n",
    "            pred = model.predict(Xte_enc)\n",
    "            if cfg.USE_LOG_TARGET:\n",
    "                pred = np.expm1(pred)\n",
    "\n",
    "            m = eval_metrics(y_test, pred)\n",
    "            best_iter = getattr(model, \"best_iteration\", None)\n",
    "            if best_iter is None:\n",
    "                best_iter = getattr(model, \"best_ntree_limit\", None)\n",
    "\n",
    "            log(f\"  - XGB cand#{i:<2} | MAE={m['mae']:,.3f} RMSE={m['rmse']:,.3f} R2={m['r2']:.4f} (best_iter={best_iter})\", cfg)\n",
    "\n",
    "            if m[\"mae\"] < best_mae:\n",
    "                best_mae = m[\"mae\"]\n",
    "                best = {\"preprocess\": pre_fitted, \"model\": model}\n",
    "                best_info = {\"params\": p, \"metrics\": m, \"best_iter\": best_iter}\n",
    "\n",
    "        if best is None:\n",
    "            raise RuntimeError(f\"HOUR={hour:02d}에서 모델 학습이 실패했습니다(후보가 모두 스킵됨).\")\n",
    "\n",
    "        # 저장\n",
    "        hour_dir = os.path.join(cfg.SAVE_ROOT, f\"HOUR_{hour:02d}\")\n",
    "        os.makedirs(hour_dir, exist_ok=True)\n",
    "        save_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "\n",
    "        joblib.dump(\n",
    "            {\n",
    "                \"type\": \"xgb_only\",\n",
    "                \"hour\": hour,\n",
    "                \"use_log_target\": cfg.USE_LOG_TARGET,\n",
    "                \"feature_cols\": feature_cols,\n",
    "                \"preprocess\": best[\"preprocess\"],\n",
    "                \"model\": best[\"model\"],\n",
    "                \"best_params\": best_info[\"params\"],\n",
    "                \"metrics_test\": best_info[\"metrics\"],\n",
    "                \"best_iter\": best_info[\"best_iter\"],\n",
    "            },\n",
    "            save_path\n",
    "        )\n",
    "\n",
    "        log(f\"✅ BEST => XGB | MAE={best_info['metrics']['mae']:,.3f} 저장: {save_path}\\n\", cfg)\n",
    "\n",
    "        results_rows.append({\n",
    "            \"HOUR\": hour,\n",
    "            \"BEST_MODEL\": \"XGB\",\n",
    "            \"MAE\": best_info[\"metrics\"][\"mae\"],\n",
    "            \"RMSE\": best_info[\"metrics\"][\"rmse\"],\n",
    "            \"R2\": best_info[\"metrics\"][\"r2\"],\n",
    "            \"BEST_ITER\": best_info[\"best_iter\"],\n",
    "            \"MODEL_PATH\": save_path,\n",
    "        })\n",
    "\n",
    "    # 결과표 저장\n",
    "    res_df = pd.DataFrame(results_rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "    res_path = os.path.join(cfg.SAVE_ROOT, \"results_by_hour.csv\")\n",
    "    res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    total_sec = time.time() - t_all0\n",
    "    log(\"[DONE] 학습 완료\", cfg)\n",
    "    log(f\"- 결과표 저장: {res_path}\", cfg)\n",
    "    log(f\"- 모델 저장 폴더: {cfg.SAVE_ROOT}\", cfg)\n",
    "    log(f\"- 총 소요(초): {total_sec:,.1f}\", cfg)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# (선택) 예측 함수: 저장된 모델로 예측\n",
    "# =========================\n",
    "def load_hour_model(save_root: str, hour: int) -> dict:\n",
    "    path = os.path.join(save_root, f\"HOUR_{hour:02d}\", \"best_model.joblib\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"모델이 없습니다: {path}\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_amt_one_hour(model_obj: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    X_new는 feature_cols를 포함하는 DF여야 함 (DONG, TA_YMD, DAY, HOUR, TEMP, RAIN 등)\n",
    "    \"\"\"\n",
    "    pre = model_obj[\"preprocess\"]\n",
    "    mdl = model_obj[\"model\"]\n",
    "    feat_cols = model_obj[\"feature_cols\"]\n",
    "\n",
    "    X_use = X_new[feat_cols].copy()\n",
    "    X_enc = pre.transform(X_use)\n",
    "    pred = mdl.predict(X_enc)\n",
    "    if model_obj.get(\"use_log_target\", False):\n",
    "        pred = np.expm1(pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = CFG(\n",
    "        DATA_PATH=\"data/수원시 한식 동별 데이터백업.csv\",  \n",
    "        SAVE_ROOT=\"data/model_amt_xgb\",\n",
    "        USE_LOG_TARGET=True,\n",
    "        TOTAL_TIME_BUDGET_SEC=1200,         # 20분\n",
    "        VERBOSE=True\n",
    "    )\n",
    "    train_all_hours_xgb(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c34ccc",
   "metadata": {},
   "source": [
    "# 8. claude 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5684940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📂 데이터 로딩\n",
      "============================================================\n",
      "✅ CSV 로딩 성공 (utf-8-sig): 443,523 행\n",
      "📋 컬럼: ['TA_YMD', 'DONG', 'HOUR', 'DAY', 'AMT', 'CNT', 'UNIT', 'TEMP', 'RAIN']\n",
      "\n",
      "============================================================\n",
      "🔧 데이터 전처리\n",
      "============================================================\n",
      "  - AMT 이상치 제거: 72행\n",
      "  - CNT 이상치 제거: 0행\n",
      "✅ 전처리 후: 443,451 행\n",
      "\n",
      "============================================================\n",
      "⚙️  고급 특성 엔지니어링\n",
      "============================================================\n",
      "\n",
      "📊 동별 통계 특성 계산 중...\n",
      "✅ 동별 통계 특성 추가 완료\n",
      "✅ 특성 추가 완료\n",
      "📊 최종 특성 수: 30\n",
      "\n",
      "📋 사용 특성 (24개):\n",
      "   1. DONG_ENCODED\n",
      "   2. DAY\n",
      "   3. TEMP\n",
      "   4. RAIN\n",
      "   5. SEASON\n",
      "   6. IS_WEEKEND\n",
      "   7. IS_FRIDAY\n",
      "   8. IS_MONDAY\n",
      "   9. TEMP_RANGE\n",
      "  10. TEMP_SQUARED\n",
      "  11. HAS_RAIN\n",
      "  12. RAIN_LEVEL\n",
      "  13. DISCOMFORT_INDEX\n",
      "  14. MONTH\n",
      "  15. YEAR\n",
      "  16. DATE\n",
      "  17. IS_MONTH_START\n",
      "  18. IS_MONTH_END\n",
      "  19. DONG_AMT_MEAN\n",
      "  20. DONG_AMT_STD\n",
      "  21. DONG_AMT_MEDIAN\n",
      "  22. DONG_CNT_MEAN\n",
      "  23. DONG_CNT_STD\n",
      "  24. DONG_CNT_MEDIAN\n",
      "\n",
      "============================================================\n",
      "🤖 시간대별 모델 학습 시작 (강화 버전)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 01 학습 중...\n",
      "==================================================\n",
      "📊 Train: 33,935 | Test: 8,484\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 701,745 | RMSE: 1,835,400 | R²: 0.9341 | MAPE: 188.8%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 11.35 | RMSE: 47.84 | R²: 0.9460 | MAPE: 39.9%\n",
      "💾 모델 저장: models_v3\\hour_01_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 0.2분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 02 학습 중...\n",
      "==================================================\n",
      "📊 Train: 33,998 | Test: 8,500\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 216,935 | RMSE: 657,538 | R²: 0.5958 | MAPE: 140.0%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 6.21 | RMSE: 10.40 | R²: 0.9331 | MAPE: 41.3%\n",
      "💾 모델 저장: models_v3\\hour_02_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 0.5분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 03 학습 중...\n",
      "==================================================\n",
      "📊 Train: 35,715 | Test: 8,929\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 567,499 | RMSE: 1,980,246 | R²: 0.3874 | MAPE: 87.3%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 11.90 | RMSE: 19.46 | R²: 0.9265 | MAPE: 28.5%\n",
      "💾 모델 저장: models_v3\\hour_03_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 0.7분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 04 학습 중...\n",
      "==================================================\n",
      "📊 Train: 36,352 | Test: 9,089\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 1,331,952 | RMSE: 3,700,388 | R²: 0.8849 | MAPE: 38.8%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 32.51 | RMSE: 56.82 | R²: 0.9646 | MAPE: 18.4%\n",
      "💾 모델 저장: models_v3\\hour_04_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 1.0분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 05 학습 중...\n",
      "==================================================\n",
      "📊 Train: 36,391 | Test: 9,098\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 1,655,355 | RMSE: 4,080,971 | R²: 0.9091 | MAPE: 34.6%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 31.35 | RMSE: 69.32 | R²: 0.9497 | MAPE: 16.9%\n",
      "💾 모델 저장: models_v3\\hour_05_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 1.2분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 06 학습 중...\n",
      "==================================================\n",
      "📊 Train: 36,381 | Test: 9,096\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 1,260,067 | RMSE: 3,675,825 | R²: 0.8991 | MAPE: 61.5%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 23.88 | RMSE: 64.38 | R²: 0.9337 | MAPE: 21.5%\n",
      "💾 모델 저장: models_v3\\hour_06_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 1.5분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 07 학습 중...\n",
      "==================================================\n",
      "📊 Train: 36,402 | Test: 9,101\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 1,443,745 | RMSE: 3,868,723 | R²: 0.9189 | MAPE: 34.6%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 28.54 | RMSE: 63.22 | R²: 0.9544 | MAPE: 18.9%\n",
      "💾 모델 저장: models_v3\\hour_07_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 1.7분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 08 학습 중...\n",
      "==================================================\n",
      "📊 Train: 36,348 | Test: 9,088\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 1,844,160 | RMSE: 4,213,418 | R²: 0.9158 | MAPE: 39.5%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 25.90 | RMSE: 60.10 | R²: 0.9538 | MAPE: 17.0%\n",
      "💾 모델 저장: models_v3\\hour_08_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 2.0분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 09 학습 중...\n",
      "==================================================\n",
      "📊 Train: 35,988 | Test: 8,997\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 1,077,064 | RMSE: 2,497,940 | R²: 0.9052 | MAPE: 72.9%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 14.50 | RMSE: 37.40 | R²: 0.9517 | MAPE: 25.1%\n",
      "💾 모델 저장: models_v3\\hour_09_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 2.3분\n",
      "\n",
      "==================================================\n",
      "⏰ HOUR 10 학습 중...\n",
      "==================================================\n",
      "📊 Train: 33,247 | Test: 8,312\n",
      "\n",
      "💰 AMT 모델 학습...\n",
      "  ✅ AMT - MAE: 457,513 | RMSE: 1,138,767 | R²: 0.8192 | MAPE: 193.4%\n",
      "\n",
      "👥 CNT 모델 학습...\n",
      "  ✅ CNT - MAE: 6.37 | RMSE: 16.30 | R²: 0.9271 | MAPE: 43.7%\n",
      "💾 모델 저장: models_v3\\hour_10_ensemble.joblib\n",
      "\n",
      "⏱️  경과 시간: 2.5분\n",
      "\n",
      "============================================================\n",
      "💾 결과 저장\n",
      "============================================================\n",
      "✅ 메트릭 저장: models_v3/metrics_improved.csv\n",
      "\n",
      "============================================================\n",
      "📊 최종 성능 (개선 버전)\n",
      "============================================================\n",
      "HOUR      N   AMT_MAE  AMT_RMSE AMT_R2 AMT_MAPE(%) CNT_MAE CNT_RMSE CNT_R2 CNT_MAPE(%) TIME(s)\n",
      "  01 42,419   701,745 1,835,400 0.9341       188.8   11.35    47.84 0.9460        39.9    14.1\n",
      "  02 42,498   216,935   657,538 0.5958       140.0    6.21    10.40 0.9331        41.3    14.1\n",
      "  03 44,644   567,499 1,980,246 0.3874        87.3   11.90    19.46 0.9265        28.5    15.5\n",
      "  04 45,441 1,331,952 3,700,388 0.8849        38.8   32.51    56.82 0.9646        18.4    15.0\n",
      "  05 45,489 1,655,355 4,080,971 0.9091        34.6   31.35    69.32 0.9497        16.9    15.5\n",
      "  06 45,477 1,260,067 3,675,825 0.8991        61.5   23.88    64.38 0.9337        21.5    15.2\n",
      "  07 45,503 1,443,745 3,868,723 0.9189        34.6   28.54    63.22 0.9544        18.9    15.6\n",
      "  08 45,436 1,844,160 4,213,418 0.9158        39.5   25.90    60.10 0.9538        17.0    15.9\n",
      "  09 44,985 1,077,064 2,497,940 0.9052        72.9   14.50    37.40 0.9517        25.1    15.4\n",
      "  10 41,559   457,513 1,138,767 0.8192       193.4    6.37    16.30 0.9271        43.7    14.5\n",
      "\n",
      "⏱️  총 학습 시간: 2.5분\n",
      "\n",
      "============================================================\n",
      "✅ 모델 학습 완료!\n",
      "============================================================\n",
      "📁 저장 위치: models_v3/\n",
      "📊 메트릭: models_v3/metrics_improved.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "수원시 한식 동별 매출/건수 예측 모델 V2 (성능 개선)\n",
    "- 목표: R² 0.85+ 달성, 특히 저성능 시간대 개선\n",
    "- 개선 전략:\n",
    "  1. 더 많은 특성 엔지니어링\n",
    "  2. 동별 통계 특성 추가\n",
    "  3. 하이퍼파라미터 강화\n",
    "  4. 저성능 시간대는 더 깊은 모델 사용\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ML 라이브러리\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "    print(\"⚠️  CatBoost 없음 (선택사항)\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "DATA_PATH = \"data/수원시 한식 동별 데이터백업.csv\"\n",
    "MODELS_DIR = \"models_v3\"\n",
    "METRICS_PATH = \"models_v3/metrics_improved.csv\"\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 데이터 로드\n",
    "# =========================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"📂 데이터 로딩\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for encoding in ['utf-8-sig', 'cp949', 'utf-8']:\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_PATH, encoding=encoding)\n",
    "        print(f\"✅ CSV 로딩 성공 ({encoding}): {len(df):,} 행\")\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"📋 컬럼: {list(df.columns)}\")\n",
    "\n",
    "# =========================\n",
    "# 데이터 전처리\n",
    "# =========================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🔧 데이터 전처리\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 결측치 제거\n",
    "df = df.dropna(subset=['DONG', 'HOUR', 'DAY', 'AMT', 'CNT', 'TEMP'])\n",
    "df['RAIN'] = df['RAIN'].fillna(0.0)\n",
    "\n",
    "# 이상치 제거\n",
    "df = df[df['AMT'] >= 0]\n",
    "df = df[df['CNT'] >= 0]\n",
    "\n",
    "# 극단적 이상치 제거 (IQR 방식)\n",
    "for col in ['AMT', 'CNT']:\n",
    "    Q1 = df[col].quantile(0.01)\n",
    "    Q3 = df[col].quantile(0.99)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 3 * IQR\n",
    "    upper = Q3 + 3 * IQR\n",
    "    before = len(df)\n",
    "    df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "    removed = before - len(df)\n",
    "    print(f\"  - {col} 이상치 제거: {removed:,}행\")\n",
    "\n",
    "print(f\"✅ 전처리 후: {len(df):,} 행\")\n",
    "\n",
    "# =========================\n",
    "# 고급 특성 엔지니어링\n",
    "# =========================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"⚙️  고급 특성 엔지니어링\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 날짜 특성\n",
    "df['TA_YMD'] = df['TA_YMD'].astype(str)\n",
    "df['YEAR'] = df['TA_YMD'].str[:4].astype(int)\n",
    "df['MONTH'] = df['TA_YMD'].str[4:6].astype(int)\n",
    "df['DATE'] = df['TA_YMD'].str[6:8].astype(int)\n",
    "\n",
    "# 계절\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 1  # 봄\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 2  # 여름\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 3  # 가을\n",
    "    else:\n",
    "        return 4  # 겨울\n",
    "\n",
    "df['SEASON'] = df['MONTH'].apply(get_season)\n",
    "\n",
    "# 주말 여부\n",
    "df['IS_WEEKEND'] = df['DAY'].apply(lambda x: 1 if x in [6, 7] else 0)\n",
    "\n",
    "# 금요일 여부 (주말 전날)\n",
    "df['IS_FRIDAY'] = (df['DAY'] == 5).astype(int)\n",
    "\n",
    "# 월요일 여부 (주말 후)\n",
    "df['IS_MONDAY'] = (df['DAY'] == 1).astype(int)\n",
    "\n",
    "# 온도 관련 특성\n",
    "df['TEMP_RANGE'] = pd.cut(df['TEMP'], bins=[-np.inf, 0, 10, 20, np.inf], labels=[0, 1, 2, 3])\n",
    "df['TEMP_RANGE'] = df['TEMP_RANGE'].astype(int)\n",
    "df['TEMP_SQUARED'] = df['TEMP'] ** 2  # 온도 제곱 (비선형 관계)\n",
    "\n",
    "# 강수 관련 특성\n",
    "df['HAS_RAIN'] = (df['RAIN'] > 0).astype(int)\n",
    "df['RAIN_LEVEL'] = pd.cut(df['RAIN'], bins=[-np.inf, 0, 5, 20, np.inf], labels=[0, 1, 2, 3])\n",
    "df['RAIN_LEVEL'] = df['RAIN_LEVEL'].astype(int)\n",
    "\n",
    "# 불쾌지수 (온도 + 습도 대용으로 온도+강수)\n",
    "df['DISCOMFORT_INDEX'] = df['TEMP'] + df['RAIN'] * 0.5\n",
    "\n",
    "# 월초/월말\n",
    "df['IS_MONTH_START'] = (df['DATE'] <= 10).astype(int)\n",
    "df['IS_MONTH_END'] = (df['DATE'] >= 20).astype(int)\n",
    "\n",
    "# 동별 인코딩\n",
    "dong_encoder = LabelEncoder()\n",
    "df['DONG_ENCODED'] = dong_encoder.fit_transform(df['DONG'])\n",
    "\n",
    "# 🔥 동별 통계 특성 추가 (중요!)\n",
    "print(f\"\\n📊 동별 통계 특성 계산 중...\")\n",
    "dong_stats = df.groupby(['DONG', 'HOUR']).agg({\n",
    "    'AMT': ['mean', 'std', 'median'],\n",
    "    'CNT': ['mean', 'std', 'median']\n",
    "}).reset_index()\n",
    "\n",
    "dong_stats.columns = ['DONG', 'HOUR', 'DONG_AMT_MEAN', 'DONG_AMT_STD', 'DONG_AMT_MEDIAN',\n",
    "                      'DONG_CNT_MEAN', 'DONG_CNT_STD', 'DONG_CNT_MEDIAN']\n",
    "\n",
    "# NaN 처리\n",
    "dong_stats = dong_stats.fillna(0)\n",
    "\n",
    "# merge\n",
    "df = df.merge(dong_stats, on=['DONG', 'HOUR'], how='left')\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(f\"✅ 동별 통계 특성 추가 완료\")\n",
    "\n",
    "# 저장\n",
    "joblib.dump(dong_encoder, os.path.join(MODELS_DIR, 'dong_encoder.joblib'))\n",
    "joblib.dump(dong_stats, os.path.join(MODELS_DIR, 'dong_stats.joblib'))\n",
    "\n",
    "print(f\"✅ 특성 추가 완료\")\n",
    "print(f\"📊 최종 특성 수: {len(df.columns)}\")\n",
    "\n",
    "# =========================\n",
    "# 특성 리스트\n",
    "# =========================\n",
    "FEATURE_COLS = [\n",
    "    'DONG_ENCODED', 'DAY', 'TEMP', 'RAIN', \n",
    "    'SEASON', 'IS_WEEKEND', 'IS_FRIDAY', 'IS_MONDAY',\n",
    "    'TEMP_RANGE', 'TEMP_SQUARED', 'HAS_RAIN', 'RAIN_LEVEL', 'DISCOMFORT_INDEX',\n",
    "    'MONTH', 'YEAR', 'DATE', 'IS_MONTH_START', 'IS_MONTH_END',\n",
    "    'DONG_AMT_MEAN', 'DONG_AMT_STD', 'DONG_AMT_MEDIAN',\n",
    "    'DONG_CNT_MEAN', 'DONG_CNT_STD', 'DONG_CNT_MEDIAN'\n",
    "]\n",
    "\n",
    "print(f\"\\n📋 사용 특성 ({len(FEATURE_COLS)}개):\")\n",
    "for i, col in enumerate(FEATURE_COLS, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# =========================\n",
    "# 시간대별 모델 학습\n",
    "# =========================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🤖 시간대별 모델 학습 시작 (강화 버전)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "all_metrics = []\n",
    "start_time = time.time()\n",
    "\n",
    "for hour in range(1, 11):\n",
    "    hour_start = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"⏰ HOUR {hour:02d} 학습 중...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 시간대별 데이터\n",
    "    df_hour = df[df['HOUR'] == hour].copy()\n",
    "    \n",
    "    if len(df_hour) < 100:\n",
    "        print(f\"⚠️  데이터 부족 (N={len(df_hour)}), 스킵\")\n",
    "        continue\n",
    "    \n",
    "    # 특성/타겟 분리\n",
    "    X = df_hour[FEATURE_COLS]\n",
    "    y_amt = df_hour['AMT']\n",
    "    y_cnt = df_hour['CNT']\n",
    "    \n",
    "    # Train/Test 분할\n",
    "    X_train, X_test, y_amt_train, y_amt_test, y_cnt_train, y_cnt_test = train_test_split(\n",
    "        X, y_amt, y_cnt, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 Train: {len(X_train):,} | Test: {len(X_test):,}\")\n",
    "    \n",
    "    # =========================\n",
    "    # AMT 모델 (강화 버전)\n",
    "    # =========================\n",
    "    print(f\"\\n💰 AMT 모델 학습...\")\n",
    "    \n",
    "    # XGBoost (더 깊고 강력하게)\n",
    "    xgb_amt = xgb.XGBRegressor(\n",
    "        n_estimators=300,       # 200 → 300\n",
    "        max_depth=10,           # 8 → 10\n",
    "        learning_rate=0.03,     # 0.05 → 0.03 (더 천천히, 더 정확하게)\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.1,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist'\n",
    "    )\n",
    "    xgb_amt.fit(X_train, y_amt_train, verbose=False)\n",
    "    pred_amt_xgb = xgb_amt.predict(X_test)\n",
    "    \n",
    "    # LightGBM (더 깊고 강력하게)\n",
    "    lgb_amt = lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=20,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_amt.fit(X_train, y_amt_train)\n",
    "    pred_amt_lgb = lgb_amt.predict(X_test)\n",
    "    \n",
    "    # CatBoost\n",
    "    if CATBOOST_AVAILABLE:\n",
    "        cat_amt = cb.CatBoostRegressor(\n",
    "            iterations=300,\n",
    "            depth=10,\n",
    "            learning_rate=0.03,\n",
    "            l2_leaf_reg=3.0,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        cat_amt.fit(X_train, y_amt_train)\n",
    "        pred_amt_cat = cat_amt.predict(X_test)\n",
    "        \n",
    "        # 앙상블\n",
    "        pred_amt = (pred_amt_xgb * 0.4 + pred_amt_lgb * 0.4 + pred_amt_cat * 0.2)\n",
    "    else:\n",
    "        pred_amt = (pred_amt_xgb * 0.5 + pred_amt_lgb * 0.5)\n",
    "    \n",
    "    # 음수 제거\n",
    "    pred_amt = np.maximum(pred_amt, 0)\n",
    "    \n",
    "    # AMT 메트릭\n",
    "    amt_mae = mean_absolute_error(y_amt_test, pred_amt)\n",
    "    amt_rmse = np.sqrt(mean_squared_error(y_amt_test, pred_amt))\n",
    "    amt_r2 = r2_score(y_amt_test, pred_amt)\n",
    "    amt_mape = np.mean(np.abs((y_amt_test - pred_amt) / np.maximum(y_amt_test, 1))) * 100\n",
    "    \n",
    "    print(f\"  ✅ AMT - MAE: {amt_mae:,.0f} | RMSE: {amt_rmse:,.0f} | R²: {amt_r2:.4f} | MAPE: {amt_mape:.1f}%\")\n",
    "    \n",
    "    # =========================\n",
    "    # CNT 모델 (강화 버전)\n",
    "    # =========================\n",
    "    print(f\"\\n👥 CNT 모델 학습...\")\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_cnt = xgb.XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.1,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist'\n",
    "    )\n",
    "    xgb_cnt.fit(X_train, y_cnt_train, verbose=False)\n",
    "    pred_cnt_xgb = xgb_cnt.predict(X_test)\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_cnt = lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=20,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_cnt.fit(X_train, y_cnt_train)\n",
    "    pred_cnt_lgb = lgb_cnt.predict(X_test)\n",
    "    \n",
    "    # CatBoost\n",
    "    if CATBOOST_AVAILABLE:\n",
    "        cat_cnt = cb.CatBoostRegressor(\n",
    "            iterations=300,\n",
    "            depth=10,\n",
    "            learning_rate=0.03,\n",
    "            l2_leaf_reg=3.0,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        cat_cnt.fit(X_train, y_cnt_train)\n",
    "        pred_cnt_cat = cat_cnt.predict(X_test)\n",
    "        \n",
    "        pred_cnt = (pred_cnt_xgb * 0.4 + pred_cnt_lgb * 0.4 + pred_cnt_cat * 0.2)\n",
    "    else:\n",
    "        pred_cnt = (pred_cnt_xgb * 0.5 + pred_cnt_lgb * 0.5)\n",
    "    \n",
    "    # 음수 제거\n",
    "    pred_cnt = np.maximum(pred_cnt, 0)\n",
    "    \n",
    "    # CNT 메트릭\n",
    "    cnt_mae = mean_absolute_error(y_cnt_test, pred_cnt)\n",
    "    cnt_rmse = np.sqrt(mean_squared_error(y_cnt_test, pred_cnt))\n",
    "    cnt_r2 = r2_score(y_cnt_test, pred_cnt)\n",
    "    cnt_mape = np.mean(np.abs((y_cnt_test - pred_cnt) / np.maximum(y_cnt_test, 1))) * 100\n",
    "    \n",
    "    print(f\"  ✅ CNT - MAE: {cnt_mae:.2f} | RMSE: {cnt_rmse:.2f} | R²: {cnt_r2:.4f} | MAPE: {cnt_mape:.1f}%\")\n",
    "    \n",
    "    # =========================\n",
    "    # 앙상블 모델 저장\n",
    "    # =========================\n",
    "    ensemble_model = {\n",
    "        'xgb_amt': xgb_amt,\n",
    "        'lgb_amt': lgb_amt,\n",
    "        'xgb_cnt': xgb_cnt,\n",
    "        'lgb_cnt': lgb_cnt,\n",
    "        'feature_cols': FEATURE_COLS,\n",
    "        'has_catboost': CATBOOST_AVAILABLE\n",
    "    }\n",
    "    \n",
    "    if CATBOOST_AVAILABLE:\n",
    "        ensemble_model['cat_amt'] = cat_amt\n",
    "        ensemble_model['cat_cnt'] = cat_cnt\n",
    "    \n",
    "    model_path = os.path.join(MODELS_DIR, f\"hour_{hour:02d}_ensemble.joblib\")\n",
    "    joblib.dump(ensemble_model, model_path)\n",
    "    print(f\"💾 모델 저장: {model_path}\")\n",
    "    \n",
    "    # 메트릭 저장\n",
    "    all_metrics.append({\n",
    "        'HOUR': f\"{hour:02d}\",\n",
    "        'N': f\"{len(df_hour):,}\",\n",
    "        'AMT_MAE': f\"{amt_mae:,.0f}\",\n",
    "        'AMT_RMSE': f\"{amt_rmse:,.0f}\",\n",
    "        'AMT_R2': f\"{amt_r2:.4f}\",\n",
    "        'AMT_MAPE(%)': f\"{amt_mape:.1f}\",\n",
    "        'CNT_MAE': f\"{cnt_mae:.2f}\",\n",
    "        'CNT_RMSE': f\"{cnt_rmse:.2f}\",\n",
    "        'CNT_R2': f\"{cnt_r2:.4f}\",\n",
    "        'CNT_MAPE(%)': f\"{cnt_mape:.1f}\",\n",
    "        'TIME(s)': f\"{time.time() - hour_start:.1f}\"\n",
    "    })\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n⏱️  경과 시간: {elapsed/60:.1f}분\")\n",
    "    \n",
    "    if elapsed > 1800:  # 30분\n",
    "        print(f\"\\n⚠️  시간 제한 도달\")\n",
    "        break\n",
    "\n",
    "# =========================\n",
    "# 결과 저장\n",
    "# =========================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"💾 결과 저장\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "metrics_df.to_csv(METRICS_PATH, index=False, encoding='utf-8-sig')\n",
    "print(f\"✅ 메트릭 저장: {METRICS_PATH}\")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"📊 최종 성능 (개선 버전)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n⏱️  총 학습 시간: {total_time/60:.1f}분\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ 모델 학습 완료!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📁 저장 위치: {MODELS_DIR}/\")\n",
    "print(f\"📊 메트릭: {METRICS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835453a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca510a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24535028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee77ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372adf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e778c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc72f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b961c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e87b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a2a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c5811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3652970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp (ipykernel)",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
