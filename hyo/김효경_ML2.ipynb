{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41a43fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.text_cell_render.rendered_html{font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.text_cell_render.rendered_html{font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09745b67",
   "metadata": {},
   "source": [
    "# 1. 한식만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691ca872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2711\n",
      "[LightGBM] [Info] Number of data points in the train set: 88705, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2080467.000000\n",
      "Fold 1 MAE: 126,532\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2753\n",
      "[LightGBM] [Info] Number of data points in the train set: 177406, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2119460.000000\n",
      "Fold 2 MAE: 318,519\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 266107, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2140878.000000\n",
      "Fold 3 MAE: 222,968\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 354808, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108242.500000\n",
      "Fold 4 MAE: 219,465\n",
      "\n",
      "====================\n",
      "CV 평균 MAE: 221,871\n",
      "====================\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2847\n",
      "[LightGBM] [Info] Number of data points in the train set: 443509, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2107993.000000\n",
      "최종 모델 학습 완료\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# ===== (1) 날짜 컬럼 찾기/통일 =====\n",
    "date_col = \"DATE\" if \"DATE\" in df.columns else (\"TA_YMD\" if \"TA_YMD\" in df.columns else None)\n",
    "if date_col is None:\n",
    "    raise ValueError(\"날짜 컬럼이 DATE 또는 TA_YMD로 존재해야 합니다.\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "TARGET = \"AMT\"\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"타겟 컬럼 {TARGET} 이(가) 없습니다. 컬럼명 확인 필요.\")\n",
    "\n",
    "# ===== (2) DONG 문자열 처리 =====\n",
    "cat_cols = []\n",
    "if \"DONG\" in df.columns:\n",
    "    df[\"DONG\"] = df[\"DONG\"].astype(\"category\")\n",
    "    cat_cols.append(\"DONG\")\n",
    "\n",
    "# ===== (3) 피처 엔지니어링 =====\n",
    "def make_features(data):\n",
    "    d = data.copy()\n",
    "\n",
    "    # 날짜 파생 (datetime 제거 목적)\n",
    "    d[\"year\"] = d[date_col].dt.year\n",
    "    d[\"month\"] = d[date_col].dt.month\n",
    "    d[\"day\"] = d[date_col].dt.day\n",
    "    d[\"dow\"] = d[date_col].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # Lag / Rolling (AMT 기반)\n",
    "    for lag in [1, 7, 14]:\n",
    "        d[f\"lag_{lag}\"] = d[TARGET].shift(lag)\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        d[f\"roll_mean_{win}\"] = d[TARGET].shift(1).rolling(win).mean()\n",
    "        d[f\"roll_std_{win}\"] = d[TARGET].shift(1).rolling(win).std()\n",
    "\n",
    "    return d\n",
    "\n",
    "df = make_features(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# ===== (4) 학습용 X/y 구성 =====\n",
    "drop_cols = [TARGET, date_col]\n",
    "FEATURES = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# LightGBM이 category dtype을 처리할 수 있도록 유지\n",
    "# (단, 혹시 object 남아있으면 category로 바꿔줌)\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == \"object\":\n",
    "        X[c] = X[c].astype(\"category\")\n",
    "        if c not in cat_cols:\n",
    "            cat_cols.append(c)\n",
    "\n",
    "# ===== (5) TimeSeries CV =====\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "mae_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"l1\",\n",
    "        categorical_feature=cat_cols if len(cat_cols) > 0 else \"auto\",\n",
    "        callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    mae_scores.append(mae)\n",
    "    print(f\"Fold {fold+1} MAE: {mae:,.0f}\")\n",
    "\n",
    "print(\"\\n====================\")\n",
    "print(f\"CV 평균 MAE: {np.mean(mae_scores):,.0f}\")\n",
    "print(\"====================\")\n",
    "\n",
    "# ===== (6) 최종 모델 =====\n",
    "final_model = lgb.LGBMRegressor(\n",
    "    objective=\"regression_l1\",\n",
    "    n_estimators=int(np.mean([model.best_iteration_ for _ in range(1)]) or 1500),\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=0.3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "final_model.fit(X, y, categorical_feature=cat_cols if len(cat_cols) > 0 else \"auto\")\n",
    "\n",
    "print(\"최종 모델 학습 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8646e41",
   "metadata": {},
   "source": [
    "# 2. 배달, 한식, 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d211dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [han] 학습 시작 (mode=temp) =====\n",
      "[han] 핵심 날씨 컬럼 후보: ['TEMP']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2711\n",
      "[LightGBM] [Info] Number of data points in the train set: 88583, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2083101.000000\n",
      "[han] Fold 1 MAE: 121,904  (best_iter=3499)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2753\n",
      "[LightGBM] [Info] Number of data points in the train set: 177164, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2120076.500000\n",
      "[han] Fold 2 MAE: 297,366  (best_iter=3500)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 265745, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2141769.000000\n",
      "[han] Fold 3 MAE: 199,292  (best_iter=3500)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 354326, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108547.500000\n",
      "[han] Fold 4 MAE: 197,593  (best_iter=3500)\n",
      "[han] CV 평균 MAE: 204,039 / 최종 n_estimators=3499\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2847\n",
      "[LightGBM] [Info] Number of data points in the train set: 442907, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108401.000000\n",
      "✅ [han] 최고 성능 모델 저장 완료\n",
      " - model: data/models\\han_best_model.pkl\n",
      " - meta : data/models\\han_best_meta.pkl\n",
      "\n",
      "===== [delivery] 학습 시작 (mode=rain) =====\n",
      "[delivery] 핵심 날씨 컬럼 후보: ['RAIN']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2439\n",
      "[LightGBM] [Info] Number of data points in the train set: 7485, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 65015.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 1 MAE: 7,871  (best_iter=2586)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2475\n",
      "[LightGBM] [Info] Number of data points in the train set: 14966, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 68111.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 2 MAE: 6,819  (best_iter=3497)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2514\n",
      "[LightGBM] [Info] Number of data points in the train set: 22447, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 61920.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 3 MAE: 7,027  (best_iter=1969)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2523\n",
      "[LightGBM] [Info] Number of data points in the train set: 29928, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 58824.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 4 MAE: 5,838  (best_iter=3500)\n",
      "[delivery] CV 평균 MAE: 6,889 / 최종 n_estimators=2888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2562\n",
      "[LightGBM] [Info] Number of data points in the train set: 37409, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 56966.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [delivery] 최고 성능 모델 저장 완료\n",
      " - model: data/models\\delivery_best_model.pkl\n",
      " - meta : data/models\\delivery_best_meta.pkl\n",
      "\n",
      "===== 전체 완료 =====\n",
      "한식 CV MAE: 204,039\n",
      "배달 CV MAE: 6,889\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# 경로/설정\n",
    "# =========================\n",
    "HAN_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DELIVERY_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "OUT_DIR = \"data/models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "# =========================\n",
    "# 유틸: 날씨 컬럼 자동 탐지 (temp/rain)\n",
    "# =========================\n",
    "def find_weather_cols(df, mode: str):\n",
    "    cols = list(df.columns)\n",
    "    low = {c: str(c).lower() for c in cols}\n",
    "\n",
    "    if mode == \"temp\":\n",
    "        keys = [\"temp\", \"tavg\", \"tmean\", \"기온\", \"평균기온\", \"최고기온\", \"최저기온\"]\n",
    "    elif mode == \"rain\":\n",
    "        keys = [\"rain\", \"precip\", \"prcp\", \"강수\", \"강수량\", \"강우\", \"mm\"]\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'temp' or 'rain'\")\n",
    "\n",
    "    picked = []\n",
    "    for c in cols:\n",
    "        s = low[c]\n",
    "        if any(k in s for k in keys):\n",
    "            picked.append(c)\n",
    "\n",
    "    # 숫자형만 유지\n",
    "    picked = [c for c in picked if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return picked\n",
    "\n",
    "# =========================\n",
    "# 피처 생성\n",
    "#  - 날짜 파생(연/월/일/요일 등)\n",
    "#  - 동(DONG) 있으면 category로 사용\n",
    "#  - lag/rolling (동별이면 groupby 적용)\n",
    "# =========================\n",
    "def make_features(df: pd.DataFrame, group_col: str | None = \"DONG\"):\n",
    "    d = df.copy()\n",
    "\n",
    "    if DATE_COL not in d.columns:\n",
    "        raise ValueError(f\"날짜 컬럼 {DATE_COL} 없음\")\n",
    "    if TARGET not in d.columns:\n",
    "        raise ValueError(f\"타겟 컬럼 {TARGET} 없음\")\n",
    "\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # 날짜 파생 (datetime dtype 제거 목적)\n",
    "    d[\"year\"] = d[DATE_COL].dt.year\n",
    "    d[\"month\"] = d[DATE_COL].dt.month\n",
    "    d[\"day\"] = d[DATE_COL].dt.day\n",
    "    d[\"dow\"] = d[DATE_COL].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    cat_cols = []\n",
    "    if group_col and group_col in d.columns:\n",
    "        d[group_col] = d[group_col].astype(\"category\")\n",
    "        cat_cols.append(group_col)\n",
    "    else:\n",
    "        group_col = None\n",
    "\n",
    "    # Lag/Rolling\n",
    "    def add_lag_roll(x):\n",
    "        for lag in [1, 7, 14]:\n",
    "            x[f\"lag_{lag}\"] = x[TARGET].shift(lag)\n",
    "        for win in [7, 14]:\n",
    "            x[f\"roll_mean_{win}\"] = x[TARGET].shift(1).rolling(win).mean()\n",
    "            x[f\"roll_std_{win}\"] = x[TARGET].shift(1).rolling(win).std()\n",
    "        return x\n",
    "\n",
    "    if group_col:\n",
    "        d = d.groupby(group_col, group_keys=False).apply(add_lag_roll)\n",
    "    else:\n",
    "        d = add_lag_roll(d)\n",
    "\n",
    "    # object 남아있으면 category로 (LightGBM dtype 에러 방지)\n",
    "    for c in d.columns:\n",
    "        if d[c].dtype == \"object\" and c != TARGET:\n",
    "            d[c] = d[c].astype(\"category\")\n",
    "            if c not in cat_cols:\n",
    "                cat_cols.append(c)\n",
    "\n",
    "    return d, cat_cols, group_col\n",
    "\n",
    "# =========================\n",
    "# 학습 + 최고 성능 모델 저장\n",
    "#  - TS CV 4 folds\n",
    "#  - 각 fold의 best_iteration_ 평균으로 최종 모델 재학습\n",
    "#  - CV 평균 MAE가 더 낮으면 파일 덮어쓰기\n",
    "# =========================\n",
    "def train_and_save_best(df: pd.DataFrame, name: str, mode: str):\n",
    "    \"\"\"\n",
    "    name: 저장 파일 prefix (예: 'han', 'delivery')\n",
    "    mode: 'temp' or 'rain' (컬럼 자동 탐지용)\n",
    "    \"\"\"\n",
    "    weather_cols = find_weather_cols(df, mode=mode)\n",
    "\n",
    "    d, cat_cols, group_col = make_features(df, group_col=\"DONG\")\n",
    "\n",
    "    # 학습 피처: 타겟/날짜 제외 전부 (날씨 컬럼이 포함되어 있으면 자동 포함)\n",
    "    drop_cols = [TARGET, DATE_COL]\n",
    "    feature_cols = [c for c in d.columns if c not in drop_cols]\n",
    "\n",
    "    # NA 제거\n",
    "    X = d[feature_cols].copy()\n",
    "    y = d[TARGET].copy()\n",
    "    mask = ~X.isna().any(axis=1) & ~y.isna()\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "\n",
    "    # TS CV\n",
    "    tscv = TimeSeriesSplit(n_splits=4)\n",
    "    maes, best_iters = [], []\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=3500,          # early stopping으로 자동 컷\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== [{name}] 학습 시작 (mode={mode}) =====\")\n",
    "    if len(weather_cols) == 0:\n",
    "        print(f\"⚠️ [{name}] {mode} 관련 날씨 컬럼 자동 탐지 실패(그래도 학습은 진행).\")\n",
    "    else:\n",
    "        print(f\"[{name}] 핵심 날씨 컬럼 후보: {weather_cols}\")\n",
    "\n",
    "    for fold, (tr, va) in enumerate(tscv.split(X), start=1):\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X.iloc[tr], y.iloc[tr],\n",
    "            eval_set=[(X.iloc[va], y.iloc[va])],\n",
    "            eval_metric=\"l1\",\n",
    "            categorical_feature=cat_cols if len(cat_cols) else \"auto\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "        pred = model.predict(X.iloc[va])\n",
    "        mae = mean_absolute_error(y.iloc[va], pred)\n",
    "        maes.append(mae)\n",
    "        best_iters.append(model.best_iteration_)\n",
    "        print(f\"[{name}] Fold {fold} MAE: {mae:,.0f}  (best_iter={model.best_iteration_})\")\n",
    "\n",
    "    cv_mae = float(np.mean(maes))\n",
    "    final_estimators = int(np.clip(np.mean(best_iters), 300, 3500))\n",
    "    print(f\"[{name}] CV 평균 MAE: {cv_mae:,.0f} / 최종 n_estimators={final_estimators}\")\n",
    "\n",
    "    # 최종 모델 재학습\n",
    "    final_model = lgb.LGBMRegressor(**{**params, \"n_estimators\": final_estimators})\n",
    "    final_model.fit(X, y, categorical_feature=cat_cols if len(cat_cols) else \"auto\")\n",
    "\n",
    "    # 저장 경로\n",
    "    model_path = os.path.join(OUT_DIR, f\"{name}_best_model.pkl\")\n",
    "    meta_path  = os.path.join(OUT_DIR, f\"{name}_best_meta.pkl\")\n",
    "\n",
    "    # 기존 저장 모델이 있으면 성능 비교 후 더 좋을 때만 덮어쓰기\n",
    "    should_save = True\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            old_meta = joblib.load(meta_path)\n",
    "            old_mae = float(old_meta.get(\"cv_mae\", np.inf))\n",
    "            if cv_mae >= old_mae:\n",
    "                should_save = False\n",
    "                print(f\"[{name}] 기존 모델이 더 좋거나 동일함: old_CV_MAE={old_mae:,.0f} <= new_CV_MAE={cv_mae:,.0f}\")\n",
    "        except Exception:\n",
    "            # 메타가 깨져있으면 새로 저장\n",
    "            should_save = True\n",
    "\n",
    "    if should_save:\n",
    "        joblib.dump(final_model, model_path)\n",
    "        meta = {\n",
    "            \"cv_mae\": cv_mae,\n",
    "            \"date_col\": DATE_COL,\n",
    "            \"target_col\": TARGET,\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"cat_cols\": cat_cols,\n",
    "            \"group_col\": group_col,\n",
    "            \"mode\": mode,\n",
    "            \"weather_cols_detected\": weather_cols,\n",
    "            \"n_estimators\": final_estimators,\n",
    "        }\n",
    "        joblib.dump(meta, meta_path)\n",
    "        print(f\"✅ [{name}] 최고 성능 모델 저장 완료\")\n",
    "        print(f\" - model: {model_path}\")\n",
    "        print(f\" - meta : {meta_path}\")\n",
    "\n",
    "    return final_model, cv_mae\n",
    "\n",
    "# =========================\n",
    "# 실행\n",
    "# =========================\n",
    "han_df = pd.read_csv(HAN_PATH)\n",
    "del_df = pd.read_csv(DELIVERY_PATH)\n",
    "\n",
    "# 한식 = 기온 중심\n",
    "han_model, han_cv_mae = train_and_save_best(han_df, name=\"han\", mode=\"temp\")\n",
    "\n",
    "# 배달 = 강수 중심\n",
    "del_model, del_cv_mae = train_and_save_best(del_df, name=\"delivery\", mode=\"rain\")\n",
    "\n",
    "print(\"\\n===== 전체 완료 =====\")\n",
    "print(f\"한식 CV MAE: {han_cv_mae:,.0f}\")\n",
    "print(f\"배달 CV MAE: {del_cv_mae:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944adc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 평가: data/models/han_best_model.pkl ====\n",
      "Samples: train=354,325 / test=88,582\n",
      "MAE  : 151,157\n",
      "RMSE : 1,679,193\n",
      "R2   : 0.9814\n",
      "WMAPE: 2.8072%\n",
      "\n",
      "[동별 MAE TOP 10 (좋은 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 권선구 세류1동  1444.0   5575.550229  0.071540\n",
      "수원시 권선구 세류3동  2065.0  13811.382568  0.010179\n",
      "수원시 영통구 매탄4동  2043.0  13914.801772  0.010554\n",
      "수원시 장안구 정자1동  2028.0  14649.504522  0.008968\n",
      "수원시 장안구 조원2동  1470.0  15033.683022  0.014717\n",
      "수원시 팔달구 화서1동  2055.0  17188.697282  0.009409\n",
      "수원시 권선구 입북동   1865.0  21462.478350  0.015297\n",
      "수원시 팔달구 매교동   2039.0  21764.044537  0.024374\n",
      "수원시 팔달구 고등동   2056.0  22144.977556  0.010232\n",
      "수원시 권선구 권선2동  2062.0  22380.597920  0.011487\n",
      "\n",
      "[동별 MAE WORST 10 (나쁜 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 영통구 매탄2동  2028.0  9.722877e+04  0.063856\n",
      "수원시 권선구 권선1동  2070.0  9.830294e+04  0.015604\n",
      "수원시 영통구 매탄3동  2060.0  1.079290e+05  0.015677\n",
      "수원시 영통구 광교1동  2059.0  1.771252e+05  0.016428\n",
      "수원시 영통구 원천동   2059.0  2.036054e+05  0.025479\n",
      "수원시 팔달구 행궁동   2060.0  2.375625e+05  0.020081\n",
      "수원시 영통구 영통2동  2060.0  2.384573e+05  0.029192\n",
      "수원시 영통구 영통3동  2060.0  2.391902e+05  0.019575\n",
      "수원시 팔달구 인계동   2060.0  6.738748e+05  0.021521\n",
      "수원시 팔달구 매산동   2060.0  3.267689e+06  0.072903\n",
      "\n",
      "==== 평가: data/models/delivery_best_model.pkl ====\n",
      "Samples: train=29,927 / test=7,482\n",
      "MAE  : 3,922\n",
      "RMSE : 57,061\n",
      "R2   : 0.9382\n",
      "WMAPE: 3.1086%\n",
      "\n",
      "[동별 MAE TOP 10 (좋은 순)]\n",
      "                  n         mae     wmape\n",
      "DONG                                     \n",
      "수원시 장안구 파장동    53.0   68.988621  0.004048\n",
      "수원시 장안구 조원2동   22.0   91.533046  0.002397\n",
      "수원시 권선구 곡선동   123.0  128.249513  0.004408\n",
      "수원시 영통구 매탄2동   32.0  145.901346  0.003445\n",
      "수원시 팔달구 매교동   145.0  163.913706  0.006012\n",
      "수원시 영통구 망포1동  889.0  219.821207  0.007710\n",
      "수원시 권선구 권선1동    4.0  226.300932  0.025989\n",
      "수원시 장안구 영화동   110.0  334.305410  0.006539\n",
      "수원시 권선구 권선2동   41.0  410.547076  0.009164\n",
      "수원시 장안구 정자1동  116.0  526.441465  0.012804\n",
      "\n",
      "[동별 MAE WORST 10 (나쁜 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 장안구 정자3동   201.0   1298.538403  0.008402\n",
      "수원시 팔달구 인계동   1171.0   1573.279183  0.016786\n",
      "수원시 권선구 세류3동   563.0   1678.198905  0.029074\n",
      "수원시 권선구 금곡동    325.0   2746.872868  0.012294\n",
      "수원시 권선구 호매실동    38.0   2850.075530  0.018004\n",
      "수원시 영통구 영통3동  1151.0   3565.511468  0.024360\n",
      "수원시 영통구 광교1동   384.0   5231.107485  0.017843\n",
      "수원시 영통구 광교2동   393.0   6446.216827  0.037606\n",
      "수원시 권선구 구운동    599.0   6690.896536  0.164116\n",
      "수원시 영통구 매탄3동  1049.0  11630.545347  0.045529\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# =====================\n",
    "# 경로\n",
    "# =====================\n",
    "HAN_MODEL_PATH = r\"data/models/han_best_model.pkl\"\n",
    "HAN_META_PATH  = r\"data/models/han_best_meta.pkl\"\n",
    "HAN_DATA_PATH  = r\"data/수원시 한식 데이터백업.csv\"   # 네 환경 경로로 맞춰\n",
    "\n",
    "DEL_MODEL_PATH = r\"data/models/delivery_best_model.pkl\"\n",
    "DEL_META_PATH  = r\"data/models/delivery_best_meta.pkl\"\n",
    "DEL_DATA_PATH  = r\"data/수원시 배달 데이터백업.csv\"  # 네 환경 경로로 맞춰\n",
    "\n",
    "# =====================\n",
    "# 공통: 피처 생성 (학습 코드와 동일해야 함)\n",
    "# =====================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "def make_features(df: pd.DataFrame, group_col: str | None = \"DONG\"):\n",
    "    d = df.copy()\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    d[\"year\"] = d[DATE_COL].dt.year\n",
    "    d[\"month\"] = d[DATE_COL].dt.month\n",
    "    d[\"day\"] = d[DATE_COL].dt.day\n",
    "    d[\"dow\"] = d[DATE_COL].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    cat_cols = []\n",
    "    if group_col and group_col in d.columns:\n",
    "        d[group_col] = d[group_col].astype(\"category\")\n",
    "        cat_cols.append(group_col)\n",
    "    else:\n",
    "        group_col = None\n",
    "\n",
    "    def add_lag_roll(x):\n",
    "        for lag in [1, 7, 14]:\n",
    "            x[f\"lag_{lag}\"] = x[TARGET].shift(lag)\n",
    "        for win in [7, 14]:\n",
    "            x[f\"roll_mean_{win}\"] = x[TARGET].shift(1).rolling(win).mean()\n",
    "            x[f\"roll_std_{win}\"] = x[TARGET].shift(1).rolling(win).std()\n",
    "        return x\n",
    "\n",
    "    if group_col:\n",
    "        d = d.groupby(group_col, group_keys=False).apply(add_lag_roll)\n",
    "    else:\n",
    "        d = add_lag_roll(d)\n",
    "\n",
    "    # object -> category (LightGBM dtype 방지)\n",
    "    for c in d.columns:\n",
    "        if d[c].dtype == \"object\" and c not in [TARGET]:\n",
    "            d[c] = d[c].astype(\"category\")\n",
    "            if c not in cat_cols:\n",
    "                cat_cols.append(c)\n",
    "\n",
    "    return d\n",
    "\n",
    "def wmape(y_true, y_pred, eps=1e-9):\n",
    "    denom = np.sum(np.abs(y_true)) + eps\n",
    "    return np.sum(np.abs(y_true - y_pred)) / denom\n",
    "\n",
    "# =====================\n",
    "# 평가 함수\n",
    "# =====================\n",
    "def evaluate_saved_model(model_path, meta_path, data_path, holdout_ratio=0.2, print_by_dong=True):\n",
    "    model = joblib.load(model_path)\n",
    "    meta = joblib.load(meta_path)\n",
    "\n",
    "    feature_cols = meta[\"feature_cols\"]\n",
    "    cat_cols = meta.get(\"cat_cols\", [])\n",
    "    group_col = meta.get(\"group_col\", None)\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    d = make_features(df, group_col=group_col)\n",
    "\n",
    "    # 학습 때처럼 NA 제거\n",
    "    X = d[feature_cols].copy()\n",
    "    y = d[TARGET].copy()\n",
    "    mask = ~X.isna().any(axis=1) & ~y.isna()\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "\n",
    "    # 시계열 holdout: 뒤쪽 20%를 test로\n",
    "    n = len(X)\n",
    "    split = int(n * (1 - holdout_ratio))\n",
    "    X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "    # 예측\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    # 지표\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    w = wmape(y_test.values, pred)\n",
    "\n",
    "    print(f\"\\n==== 평가: {model_path} ====\")\n",
    "    print(f\"Samples: train={len(X_train):,} / test={len(X_test):,}\")\n",
    "    print(f\"MAE  : {mae:,.0f}\")\n",
    "    print(f\"RMSE : {rmse:,.0f}\")\n",
    "    print(f\"R2   : {r2:.4f}\")\n",
    "    print(f\"WMAPE: {w:.4%}\")\n",
    "\n",
    "        # (선택) 동별 MAE\n",
    "    if print_by_dong and group_col and group_col in d.columns:\n",
    "        d_masked = d.loc[mask].reset_index(drop=True)\n",
    "        d_test = d_masked.iloc[split:].copy()\n",
    "        d_test[\"pred\"] = pred\n",
    "\n",
    "        # 샘플 수가 1 이상인 동만\n",
    "        grp = d_test.groupby(group_col, observed=True)\n",
    "        stats = grp.apply(lambda g: pd.Series({\n",
    "            \"n\": len(g),\n",
    "            \"mae\": mean_absolute_error(g[TARGET].values, g[\"pred\"].values) if len(g) > 0 else np.nan,\n",
    "            \"wmape\": (np.sum(np.abs(g[TARGET].values - g[\"pred\"].values)) / (np.sum(np.abs(g[TARGET].values)) + 1e-9)) if len(g) > 0 else np.nan\n",
    "        }))\n",
    "        stats = stats.dropna().sort_values(\"mae\")\n",
    "\n",
    "        if len(stats) == 0:\n",
    "            print(\"\\n[동별 MAE] 테스트 구간에 유효한 동 데이터가 없습니다. (표본 부족)\")\n",
    "        else:\n",
    "            print(\"\\n[동별 MAE TOP 10 (좋은 순)]\")\n",
    "            print(stats.head(10))\n",
    "            print(\"\\n[동별 MAE WORST 10 (나쁜 순)]\")\n",
    "            print(stats.tail(10))\n",
    "\n",
    "# =====================\n",
    "# 실행\n",
    "# =====================\n",
    "han_res = evaluate_saved_model(HAN_MODEL_PATH, HAN_META_PATH, HAN_DATA_PATH)\n",
    "del_res = evaluate_saved_model(DEL_MODEL_PATH, DEL_META_PATH, DEL_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ca233",
   "metadata": {},
   "source": [
    "# 3. 매출 데이터 바탕으로 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8057c",
   "metadata": {},
   "source": [
    "## 점유율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44cd0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =====================================================\n",
    "# 상수 / 경로\n",
    "# =====================================================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "MY_DATE_COL = \"ta_ymd\"\n",
    "MY_AMT_COL = \"store_amt\"\n",
    "\n",
    "GRID_XLSX_PATH = \"data/수원시 격자.xlsx\"\n",
    "\n",
    "HAN_DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DEL_DATA_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "HAN_MODEL_PATH = \"data/models/han_best_model.pkl\"\n",
    "DEL_MODEL_PATH = \"data/models/delivery_best_model.pkl\"\n",
    "\n",
    "HAN_META_PATH = \"data/models/han_best_meta.pkl\"\n",
    "DEL_META_PATH = \"data/models/delivery_best_meta.pkl\"\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 0) 수원시 격자 로드\n",
    "# =====================================================\n",
    "def load_suwon_grid_map(xlsx_path=GRID_XLSX_PATH):\n",
    "    df = pd.read_excel(xlsx_path, header=None)\n",
    "    df = df.dropna(subset=[0, 1, 2])\n",
    "\n",
    "    grid = {}\n",
    "    for _, r in df.iterrows():\n",
    "        try:\n",
    "            grid[str(r[0]).strip()] = (int(r[1]), int(r[2]))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not grid:\n",
    "        raise ValueError(\"격자 엑셀에서 nx, ny를 읽지 못했습니다.\")\n",
    "    return grid\n",
    "\n",
    "\n",
    "def resolve_nxny_from_input_dong(dong, grid_map):\n",
    "    if dong not in grid_map:\n",
    "        raise ValueError(f\"'{dong}'이(가) 격자 엑셀에 없습니다.\")\n",
    "    return grid_map[dong][0], grid_map[dong][1]\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 1) 기상청 단기예보 + fallback (과거 평균 날씨)\n",
    "# =====================================================\n",
    "def _parse_pcp(v):\n",
    "    if v in [None, \"강수없음\", \"-\", \"없음\"]:\n",
    "        return 0.0\n",
    "    s = str(v).replace(\"mm\", \"\").strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        nums = re.findall(r\"[\\d.]+\", s)\n",
    "        return float(nums[0]) if nums else 0.0\n",
    "\n",
    "\n",
    "def fetch_daily_weather_kma(\n",
    "    nx, ny, target_date, RAIN_ID, base_df, dong\n",
    "):\n",
    "    \"\"\"\n",
    "    1) 기상청 단기예보 가능 → 사용\n",
    "    2) NO_DATA / 과거·미래 → 과거 평균 날씨 대체\n",
    "    \"\"\"\n",
    "\n",
    "    d = datetime.strptime(target_date, \"%Y-%m-%d\").date()\n",
    "    base_date = (d - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": RAIN_ID,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"numOfRows\": 3000,\n",
    "        \"pageNo\": 1,\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": \"2300\",\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        js = r.json()\n",
    "        body = js.get(\"response\", {}).get(\"body\")\n",
    "\n",
    "        if body:\n",
    "            items = body.get(\"items\", {}).get(\"item\", [])\n",
    "            want = d.strftime(\"%Y%m%d\")\n",
    "\n",
    "            temps, rains = [], []\n",
    "            for it in items:\n",
    "                if it.get(\"fcstDate\") != want:\n",
    "                    continue\n",
    "                if it.get(\"category\") == \"TMP\":\n",
    "                    temps.append(float(it.get(\"fcstValue\")))\n",
    "                elif it.get(\"category\") == \"PCP\":\n",
    "                    rains.append(_parse_pcp(it.get(\"fcstValue\")))\n",
    "\n",
    "            if temps:\n",
    "                return {\n",
    "                    \"temp_mean\": float(np.mean(temps)),\n",
    "                    \"rain_mean\": float(np.mean(rains)) if rains else 0.0,\n",
    "                    \"rain_peak\": float(np.max(rains)) if rains else 0.0,\n",
    "                    \"source\": \"기상청 단기예보\",\n",
    "                }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # =============================\n",
    "    # fallback: 과거 평균 날씨\n",
    "    # =============================\n",
    "    hist = base_df.copy()\n",
    "    hist[DATE_COL] = pd.to_datetime(hist[DATE_COL])\n",
    "    hist = hist[hist[\"DONG\"] == dong]\n",
    "\n",
    "    temp_cols = [c for c in hist.columns if \"temp\" in c.lower()]\n",
    "    rain_cols = [c for c in hist.columns if \"rain\" in c.lower()]\n",
    "\n",
    "    return {\n",
    "        \"temp_mean\": float(hist[temp_cols[0]].mean()) if temp_cols else np.nan,\n",
    "        \"rain_mean\": float(hist[rain_cols[0]].mean()) if rain_cols else 0.0,\n",
    "        \"rain_peak\": float(hist[rain_cols[0]].max()) if rain_cols else 0.0,\n",
    "        \"source\": \"과거 평균 날씨(대체)\",\n",
    "    }\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2) 공휴일 (천문연구원)\n",
    "# =====================================================\n",
    "def fetch_holiday_flag_kasi(target_date, HOLIDAY_ID):\n",
    "    d = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    url = \"https://apis.data.go.kr/B090041/openapi/service/SpcdeInfoService/getRestDeInfo\"\n",
    "    params = {\n",
    "        \"serviceKey\": HOLIDAY_ID,\n",
    "        \"solYear\": d.year,\n",
    "        \"solMonth\": f\"{d.month:02d}\",\n",
    "        \"_type\": \"json\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        items = (\n",
    "            r.json()\n",
    "            .get(\"response\", {})\n",
    "            .get(\"body\", {})\n",
    "            .get(\"items\", {})\n",
    "            .get(\"item\", [])\n",
    "        )\n",
    "\n",
    "        if isinstance(items, dict):\n",
    "            items = [items]\n",
    "\n",
    "        for it in items:\n",
    "            if str(it.get(\"locdate\")) == d.strftime(\"%Y%m%d\"):\n",
    "                return {\"is_holiday\": 1}\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return {\"is_holiday\": 0}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3) 예측용 feature 생성 (❗ DONG 제거됨)\n",
    "# =====================================================\n",
    "def build_feature_row_for_prediction(\n",
    "    base_df, feature_cols, dong, target_date, weather, is_holiday\n",
    "):\n",
    "    d = base_df.copy()\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d[d[\"DONG\"] == dong].sort_values(DATE_COL)\n",
    "\n",
    "    td = pd.to_datetime(target_date)\n",
    "    hist = d[d[DATE_COL] < td]\n",
    "\n",
    "    row = {\n",
    "        \"year\": td.year,\n",
    "        \"month\": td.month,\n",
    "        \"day\": td.day,\n",
    "        \"dow\": td.weekday(),\n",
    "        \"weekend\": int(td.weekday() >= 5),\n",
    "    }\n",
    "\n",
    "    for lag in [1, 7, 14]:\n",
    "        row[f\"lag_{lag}\"] = hist[TARGET].iloc[-lag] if len(hist) >= lag else np.nan\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        row[f\"roll_mean_{win}\"] = hist[TARGET].tail(win).mean() if len(hist) >= win else np.nan\n",
    "        row[f\"roll_std_{win}\"] = hist[TARGET].tail(win).std() if len(hist) >= win else np.nan\n",
    "\n",
    "    row[\"temp_mean\"] = weather[\"temp_mean\"]\n",
    "    row[\"rain_mean\"] = weather[\"rain_mean\"]\n",
    "    row[\"rain_peak\"] = weather[\"rain_peak\"]\n",
    "    row[\"is_holiday\"] = is_holiday\n",
    "\n",
    "    return pd.DataFrame([{c: row.get(c, np.nan) for c in feature_cols}])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4) 점유율 + 보정계수\n",
    "# =====================================================\n",
    "def compute_store_share_and_adj(\n",
    "    my_store_df, base_df, dong, target_date, lookback_days=60\n",
    "):\n",
    "    d0 = datetime.strptime(target_date, \"%Y-%m-%d\").date()\n",
    "    start = d0 - timedelta(days=lookback_days)\n",
    "\n",
    "    ms = my_store_df.copy()\n",
    "    ms[MY_DATE_COL] = pd.to_datetime(ms[MY_DATE_COL])\n",
    "    store_amt_60 = ms[\n",
    "        (ms[MY_DATE_COL].dt.date >= start) &\n",
    "        (ms[MY_DATE_COL].dt.date < d0)\n",
    "    ][MY_AMT_COL].sum()\n",
    "\n",
    "    bd = base_df.copy()\n",
    "    bd[DATE_COL] = pd.to_datetime(bd[DATE_COL])\n",
    "    dong_amt_60 = bd[\n",
    "        (bd[\"DONG\"] == dong) &\n",
    "        (bd[DATE_COL].dt.date >= start) &\n",
    "        (bd[DATE_COL].dt.date < d0)\n",
    "    ][TARGET].sum()\n",
    "\n",
    "    share = (store_amt_60 / dong_amt_60 * 100) if dong_amt_60 > 0 else 0.0\n",
    "    return share, 1.0\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5) 거래건수 추정\n",
    "# =====================================================\n",
    "def estimate_txn_count(my_store_df, target_date, pred_store_amt):\n",
    "    pseudo_ticket = max(15000.0, pred_store_amt / 20.0)\n",
    "    return int(pred_store_amt / pseudo_ticket)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6) 메인 함수\n",
    "# =====================================================\n",
    "def predict_my_store_sales(\n",
    "    my_store_csv,\n",
    "    dong,\n",
    "    date_str,\n",
    "    service_type,\n",
    "    RAIN_ID,\n",
    "    HOLIDAY_ID,\n",
    "    grid_xlsx_path=GRID_XLSX_PATH,\n",
    "):\n",
    "    if service_type == \"delivery\":\n",
    "        model_path, meta_path, data_path = (\n",
    "            DEL_MODEL_PATH, DEL_META_PATH, DEL_DATA_PATH\n",
    "        )\n",
    "    else:\n",
    "        model_path, meta_path, data_path = (\n",
    "            HAN_MODEL_PATH, HAN_META_PATH, HAN_DATA_PATH\n",
    "        )\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    meta = joblib.load(meta_path)\n",
    "\n",
    "    # ❗ DONG 제거\n",
    "    feature_cols = [c for c in meta[\"feature_cols\"] if c != \"DONG\"]\n",
    "\n",
    "    base_df = pd.read_csv(data_path)\n",
    "    my_store_df = pd.read_csv(my_store_csv)\n",
    "\n",
    "    grid_map = load_suwon_grid_map(grid_xlsx_path)\n",
    "    nx, ny = resolve_nxny_from_input_dong(dong, grid_map)\n",
    "\n",
    "    weather = fetch_daily_weather_kma(\n",
    "        nx, ny, date_str, RAIN_ID, base_df, dong\n",
    "    )\n",
    "    holi = fetch_holiday_flag_kasi(date_str, HOLIDAY_ID)\n",
    "\n",
    "    X_pred = build_feature_row_for_prediction(\n",
    "        base_df, feature_cols, dong, date_str, weather, holi[\"is_holiday\"]\n",
    "    )\n",
    "\n",
    "    pred_dong_amt = float(model.predict(X_pred)[0])\n",
    "    share, adj = compute_store_share_and_adj(\n",
    "        my_store_df, base_df, dong, date_str\n",
    "    )\n",
    "\n",
    "    pred_store_amt = pred_dong_amt * (share / 100.0) * adj\n",
    "    pred_cnt = estimate_txn_count(my_store_df, date_str, pred_store_amt)\n",
    "\n",
    "    # =========================\n",
    "    # 출력\n",
    "    # =========================\n",
    "    print(f\"📍지역(동): {dong}\")\n",
    "    print(f\"📅날짜: {date_str}\")\n",
    "    print(f\"🌦️날씨(출처): {weather['source']}\")\n",
    "    print(f\"   - 평균기온: {weather['temp_mean']:.1f}°C\")\n",
    "    print(f\"   - 강수(평균/피크): {weather['rain_mean']:.2f} / {weather['rain_peak']:.2f} mm\")\n",
    "    print(f\"👥예상 매출건수: {pred_cnt}건\")\n",
    "    print(f\"💰예상 동 전체 일매출: {pred_dong_amt:,.0f} 원\")\n",
    "    print(f\"🏪가게 점유율(최근 60일): {share:.2f}%\")\n",
    "    print(f\"🏪예상 가게 일매출: {pred_store_amt:,.0f} 원\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e431b73",
   "metadata": {},
   "source": [
    "## 사용예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "687af6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍지역(동): 행궁동\n",
      "📅날짜: 2025-12-30\n",
      "🌦️날씨(출처): 과거 평균 날씨(대체)\n",
      "   - 평균기온: 13.4°C\n",
      "   - 강수(평균/피크): 0.16 / 37.30 mm\n",
      "👥예상 매출건수: 0건\n",
      "💰예상 동 전체 일매출: 84,208 원\n",
      "🏪가게 점유율(최근 60일): 0.00%\n",
      "🏪예상 가게 일매출: 0 원\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "holiday_key_env = os.getenv(\"HOLIDAY_ID\")\n",
    "rain_key_env = os.getenv(\"RAIN_ID\")\n",
    "\n",
    "predict_my_store_sales(\n",
    "    my_store_csv=\"data/my_store.csv\",\n",
    "    dong=\"행궁동\",\n",
    "    date_str=\"2025-12-30\",\n",
    "    service_type=\"han\",\n",
    "    RAIN_ID=rain_key_env,\n",
    "    HOLIDAY_ID=holiday_key_env,\n",
    "    grid_xlsx_path=\"data/수원시 격자.xlsx\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528355d",
   "metadata": {},
   "source": [
    "# 4. 모델 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b67ea58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAIN START: han =====\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2760\n",
      "[LightGBM] [Info] Number of data points in the train set: 355094, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 2108030.000000\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 210080\n",
      "✅ VALID MAE = 210,080.03\n",
      "💾 model saved: data/models/han_best_model.pkl\n",
      "💾 meta  saved: data/models/han_best_meta.pkl\n",
      "\n",
      "===== TRAIN START: delivery =====\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2500\n",
      "[LightGBM] [Info] Number of data points in the train set: 30234, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 58824.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4998]\tvalid_0's l1: 4223.44\n",
      "✅ VALID MAE = 4,223.44\n",
      "💾 model saved: data/models/delivery_best_model.pkl\n",
      "💾 meta  saved: data/models/delivery_best_meta.pkl\n",
      "\n",
      "===== DONE =====\n",
      "HAN MAE     : 210,080.03\n",
      "DELIVERY MAE: 4,223.44\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import timedelta\n",
    "\n",
    "# =====================================================\n",
    "# 설정\n",
    "# =====================================================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "HAN_DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DEL_DATA_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "OUT_DIR = \"data/models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# 공통 전처리\n",
    "# =====================================================\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "    df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # ---------- 날짜 파생 ----------\n",
    "    df[\"year\"] = df[DATE_COL].dt.year\n",
    "    df[\"month\"] = df[DATE_COL].dt.month\n",
    "    df[\"day\"] = df[DATE_COL].dt.day\n",
    "    df[\"dow\"] = df[DATE_COL].dt.weekday\n",
    "    df[\"weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # ---------- DONG 제거 ----------\n",
    "    if \"DONG\" in df.columns:\n",
    "        df = df.drop(columns=[\"DONG\"])\n",
    "\n",
    "    # ---------- lag / rolling ----------\n",
    "    for lag in [1, 7, 14]:\n",
    "        df[f\"lag_{lag}\"] = df[TARGET].shift(lag)\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        df[f\"roll_mean_{win}\"] = df[TARGET].rolling(win).mean()\n",
    "        df[f\"roll_std_{win}\"] = df[TARGET].rolling(win).std()\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 학습 함수\n",
    "# =====================================================\n",
    "def train_and_save(\n",
    "    data_path: str,\n",
    "    model_name: str,\n",
    "):\n",
    "    print(f\"\\n===== TRAIN START: {model_name} =====\")\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = preprocess(df)\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in [DATE_COL, TARGET]]\n",
    "\n",
    "    # ---------- time split ----------\n",
    "    split_date = df[DATE_COL].quantile(0.8)\n",
    "    train_df = df[df[DATE_COL] <= split_date]\n",
    "    valid_df = df[df[DATE_COL] > split_date]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[TARGET]\n",
    "    X_val = valid_df[feature_cols]\n",
    "    y_val = valid_df[TARGET]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=30,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"l1\",\n",
    "        callbacks=[lgb.early_stopping(300, verbose=True)],\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "\n",
    "    print(f\"✅ VALID MAE = {mae:,.2f}\")\n",
    "\n",
    "    # ---------- 저장 ----------\n",
    "    model_path = f\"{OUT_DIR}/{model_name}_best_model.pkl\"\n",
    "    meta_path = f\"{OUT_DIR}/{model_name}_best_meta.pkl\"\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(\n",
    "        {\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"valid_mae\": mae,\n",
    "            \"n_train\": len(train_df),\n",
    "            \"n_valid\": len(valid_df),\n",
    "        },\n",
    "        meta_path,\n",
    "    )\n",
    "\n",
    "    print(f\"💾 model saved: {model_path}\")\n",
    "    print(f\"💾 meta  saved: {meta_path}\")\n",
    "\n",
    "    return mae\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 실행\n",
    "# =====================================================\n",
    "if __name__ == \"__main__\":\n",
    "    han_mae = train_and_save(HAN_DATA_PATH, \"han\")\n",
    "    del_mae = train_and_save(DEL_DATA_PATH, \"delivery\")\n",
    "\n",
    "    print(\"\\n===== DONE =====\")\n",
    "    print(f\"HAN MAE     : {han_mae:,.2f}\")\n",
    "    print(f\"DELIVERY MAE: {del_mae:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d48c8",
   "metadata": {},
   "source": [
    "# 5. 정님 모델 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4e03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] XGB 사용 여부: True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model\n",
      "\n",
      "===== HOUR=01 | train=33935 test=8484 =====\n",
      "- XGB       | MAE=17,040.274 RMSE=36,555.344 R2=0.1986\n",
      "- EXTRATREE | MAE=18,416.771 RMSE=36,421.394 R2=0.2044\n",
      "✅ BEST => XGB | MAE=17,040.274 저장: data/model\\HOUR_01\\best_model.joblib\n",
      "\n",
      "===== HOUR=02 | train=33998 test=8500 =====\n",
      "- XGB       | MAE=7,673.858 RMSE=28,665.482 R2=0.0246\n",
      "- EXTRATREE | MAE=9,535.827 RMSE=28,756.635 R2=0.0184\n",
      "✅ BEST => XGB | MAE=7,673.858 저장: data/model\\HOUR_02\\best_model.joblib\n",
      "\n",
      "===== HOUR=03 | train=35715 test=8929 =====\n",
      "- XGB       | MAE=8,009.915 RMSE=30,807.032 R2=-0.0119\n",
      "- EXTRATREE | MAE=10,043.419 RMSE=30,236.351 R2=0.0253\n",
      "✅ BEST => XGB | MAE=8,009.915 저장: data/model\\HOUR_03\\best_model.joblib\n",
      "\n",
      "===== HOUR=04 | train=36360 test=9091 =====\n",
      "- XGB       | MAE=4,418.536 RMSE=9,327.978 R2=0.2978\n",
      "- EXTRATREE | MAE=4,300.277 RMSE=8,873.857 R2=0.3645\n",
      "✅ BEST => EXTRATREE | MAE=4,300.277 저장: data/model\\HOUR_04\\best_model.joblib\n",
      "\n",
      "===== HOUR=05 | train=36408 test=9102 =====\n",
      "- XGB       | MAE=5,682.599 RMSE=10,480.964 R2=0.3644\n",
      "- EXTRATREE | MAE=5,628.919 RMSE=9,812.485 R2=0.4429\n",
      "✅ BEST => EXTRATREE | MAE=5,628.919 저장: data/model\\HOUR_05\\best_model.joblib\n",
      "\n",
      "===== HOUR=06 | train=36400 test=9101 =====\n",
      "- XGB       | MAE=7,786.108 RMSE=20,178.530 R2=0.0980\n",
      "- EXTRATREE | MAE=8,141.943 RMSE=19,883.252 R2=0.1242\n",
      "✅ BEST => XGB | MAE=7,786.108 저장: data/model\\HOUR_06\\best_model.joblib\n",
      "\n",
      "===== HOUR=07 | train=36412 test=9104 =====\n",
      "- XGB       | MAE=6,397.255 RMSE=11,629.714 R2=0.3289\n",
      "- EXTRATREE | MAE=6,129.299 RMSE=11,121.370 R2=0.3863\n",
      "✅ BEST => EXTRATREE | MAE=6,129.299 저장: data/model\\HOUR_07\\best_model.joblib\n",
      "\n",
      "===== HOUR=08 | train=36351 test=9088 =====\n",
      "- XGB       | MAE=7,852.672 RMSE=15,186.483 R2=0.4069\n",
      "- EXTRATREE | MAE=7,460.548 RMSE=14,291.419 R2=0.4747\n",
      "✅ BEST => EXTRATREE | MAE=7,460.548 저장: data/model\\HOUR_08\\best_model.joblib\n",
      "\n",
      "===== HOUR=09 | train=35988 test=8998 =====\n",
      "- XGB       | MAE=12,886.179 RMSE=47,658.918 R2=0.0675\n",
      "- EXTRATREE | MAE=14,600.156 RMSE=48,140.980 R2=0.0485\n",
      "✅ BEST => XGB | MAE=12,886.179 저장: data/model\\HOUR_09\\best_model.joblib\n",
      "\n",
      "===== HOUR=10 | train=33247 test=8312 =====\n",
      "- XGB       | MAE=18,822.328 RMSE=51,328.993 R2=0.0369\n",
      "- EXTRATREE | MAE=22,140.061 RMSE=53,175.550 R2=-0.0336\n",
      "✅ BEST => XGB | MAE=18,822.328 저장: data/model\\HOUR_10\\best_model.joblib\n",
      "\n",
      "[DONE] 학습 완료\n",
      "- 결과표 저장: data/model\\results_by_hour.csv\n",
      "- 모델 저장 폴더: data/model\n",
      "- 총 소요(초): 22,954.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n사용 예:\\nbundle = load_best_bundle(5)\\nsample = X_all.iloc[[0]].copy()          # 컬럼 구조 동일해야 함\\nyhat = predict_with_bundle(bundle, sample)\\nprint(yhat)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# XGB 사용 가능 여부 체크\n",
    "# =============================\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "HANSIK_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DELIV_PATH  = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model\"           \n",
    "MAX_HOURS = list(range(1, 11))    # 1~10\n",
    "TEST_RATIO = 0.2                  # 마지막 20% 테스트\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 원핫 폭발 방지(가능한 sklearn 버전에서만 적용)\n",
    "OHE_MIN_FREQUENCY = 20\n",
    "OHE_MAX_CATEGORIES = 200\n",
    "\n",
    "# UNIT lag/rolling\n",
    "LAGS = [1, 7, 14]\n",
    "ROLL_WINDOWS = [7, 14]\n",
    "\n",
    "# XGB 파라미터(너무 느리면 max_depth/early stopping 줄이세요)\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=5000,          # 크게 주고 early stopping으로 컷\n",
    "    learning_rate=0.03,\n",
    "    max_depth=8,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    objective=\"reg:absoluteerror\",  # MAE 직접 최적화(지원 버전 필요)\n",
    "    tree_method=\"hist\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "ET_PARAMS = dict(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# 누수 위험(예측 시점에 모르는 값) 기본 제외\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate_reg(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return float(mae), float(rmse), float(r2)\n",
    "\n",
    "\n",
    "def safe_onehot_encoder():\n",
    "    \"\"\"\n",
    "    sklearn 버전에 따라 min_frequency / max_categories 미지원일 수 있어 예외 처리.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES\n",
    "        )\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "def build_preprocess(X: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    X의 dtype 기반으로 ColumnTransformer 생성\n",
    "    \"\"\"\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", safe_onehot_encoder())\n",
    "    ])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1) Load\n",
    "# =============================\n",
    "hansik = pd.read_csv(HANSIK_PATH)\n",
    "deliv  = pd.read_csv(DELIV_PATH)\n",
    "\n",
    "required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"UNIT\"}\n",
    "missing = required - set(hansik.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"[한식 데이터] 필수 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "missing2 = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\"} - set(deliv.columns)\n",
    "if missing2:\n",
    "    raise ValueError(f\"[배달 데이터] 필수 컬럼이 없습니다: {missing2}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 2) Merge DELIV as features\n",
    "# =============================\n",
    "merge_keys = [\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\"]\n",
    "\n",
    "deliv_feat = deliv.copy()\n",
    "for c in deliv_feat.columns:\n",
    "    if c not in merge_keys:\n",
    "        deliv_feat.rename(columns={c: f\"DELIV_{c}\"}, inplace=True)\n",
    "\n",
    "df = hansik.merge(deliv_feat, on=merge_keys, how=\"left\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3) Date features\n",
    "# =============================\n",
    "df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df = df[df[\"DATE\"].notna()].copy()\n",
    "\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"DAY_OF_MONTH\"] = df[\"DATE\"].dt.day\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 4) Lag/Rolling features for UNIT (no leakage if time-split)\n",
    "#    group key: (DONG, HOUR)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"]).reset_index(drop=True)\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"], sort=False)\n",
    "\n",
    "for lag in LAGS:\n",
    "    df[f\"UNIT_LAG_{lag}\"] = grp[\"UNIT\"].shift(lag)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"UNIT_ROLL_MEAN_{w}\"] = grp[\"UNIT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).mean()\n",
    "    df[f\"UNIT_ROLL_STD_{w}\"]  = grp[\"UNIT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).std()\n",
    "\n",
    "df[\"UNIT_LAG_1_MISSING\"] = df[\"UNIT_LAG_1\"].isna().astype(int)\n",
    "\n",
    "# 타겟\n",
    "y_all = df[\"UNIT\"].copy()\n",
    "\n",
    "# 피처\n",
    "X_all = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5) Train per HOUR\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "hours = sorted(pd.unique(df[\"HOUR\"].dropna()))\n",
    "hours = [int(h) for h in hours if int(h) in MAX_HOURS]\n",
    "\n",
    "print(f\"[INFO] XGB 사용 여부: {HAS_XGB}\")\n",
    "print(f\"[INFO] 학습 시간대: {hours}\")\n",
    "print(f\"[INFO] 저장 폴더: {SAVE_DIR}\")\n",
    "\n",
    "all_results = []\n",
    "t0 = time.time()\n",
    "\n",
    "for h in hours:\n",
    "    idx = (df[\"HOUR\"].astype(int) == h)\n",
    "    X_h = X_all.loc[idx].copy()\n",
    "    y_h = y_all.loc[idx].copy()\n",
    "    d_h = df.loc[idx, \"DATE\"].copy()\n",
    "\n",
    "    if len(X_h) < 800:\n",
    "        print(f\"\\n[SKIP] HOUR={h:02d}: 데이터가 너무 적음 (n={len(X_h)})\")\n",
    "        continue\n",
    "\n",
    "    # 날짜 정렬\n",
    "    order = np.argsort(d_h.values)\n",
    "    X_h = X_h.iloc[order].reset_index(drop=True)\n",
    "    y_h = y_h.iloc[order].reset_index(drop=True)\n",
    "\n",
    "    n = len(X_h)\n",
    "    split = int(n * (1 - TEST_RATIO))\n",
    "    X_train, X_test = X_h.iloc[:split], X_h.iloc[split:]\n",
    "    y_train, y_test = y_h.iloc[:split], y_h.iloc[split:]\n",
    "\n",
    "    # XGB early stopping용 valid (train의 마지막 10%)\n",
    "    split2 = int(len(X_train) * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2], X_train.iloc[split2:]\n",
    "    y_tr, y_val = y_train.iloc[:split2], y_train.iloc[split2:]\n",
    "\n",
    "    print(f\"\\n===== HOUR={h:02d} | train={len(X_train)} test={len(X_test)} =====\")\n",
    "\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{h:02d}\")\n",
    "    os.makedirs(hour_dir, exist_ok=True)\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # -------------------------\n",
    "    # A방식: preprocess를 먼저 fit/transform 후 모델 학습\n",
    "    # -------------------------\n",
    "    preprocess = build_preprocess(X_train)\n",
    "    pre = clone(preprocess)\n",
    "\n",
    "    X_tr_enc  = pre.fit_transform(X_tr)\n",
    "    X_val_enc = pre.transform(X_val)\n",
    "    X_test_enc = pre.transform(X_test)\n",
    "\n",
    "    # --- 1) XGB (있으면 가장 강력) ---\n",
    "    if HAS_XGB:\n",
    "        # objective가 버전에 따라 에러날 수 있어 fallback 처리\n",
    "        xgb_params = dict(XGB_PARAMS)\n",
    "        try:\n",
    "            model_xgb = XGBRegressor(**xgb_params)\n",
    "            model_xgb.fit(\n",
    "                X_tr_enc, y_tr,\n",
    "                eval_set=[(X_val_enc, y_val)],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # fallback: objective를 reg:squarederror로\n",
    "            xgb_params[\"objective\"] = \"reg:squarederror\"\n",
    "            model_xgb = XGBRegressor(**xgb_params)\n",
    "            model_xgb.fit(\n",
    "                X_tr_enc, y_tr,\n",
    "                eval_set=[(X_val_enc, y_val)],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200\n",
    "            )\n",
    "\n",
    "        pred = model_xgb.predict(X_test_enc)\n",
    "        mae, rmse, r2 = evaluate_reg(y_test, pred)\n",
    "        print(f\"- XGB       | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "\n",
    "        bundle = {\"preprocess\": pre, \"model\": model_xgb, \"model_name\": \"XGB\"}\n",
    "        candidates.append((\"XGB\", bundle, mae, rmse, r2))\n",
    "\n",
    "    # --- 2) ExtraTrees (빠른/안정 대안) ---\n",
    "    model_et = ExtraTreesRegressor(**ET_PARAMS)\n",
    "    model_et.fit(X_tr_enc, y_tr)  # valid는 사용 안 함(빠름)\n",
    "\n",
    "    pred = model_et.predict(X_test_enc)\n",
    "    mae, rmse, r2 = evaluate_reg(y_test, pred)\n",
    "    print(f\"- EXTRATREE | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "\n",
    "    bundle = {\"preprocess\": pre, \"model\": model_et, \"model_name\": \"EXTRATREE\"}\n",
    "    candidates.append((\"EXTRATREE\", bundle, mae, rmse, r2))\n",
    "\n",
    "    # --- pick best by MAE ---\n",
    "    candidates.sort(key=lambda x: x[2])\n",
    "    best_name, best_bundle, best_mae, best_rmse, best_r2 = candidates[0]\n",
    "\n",
    "    best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    joblib.dump(best_bundle, best_path)\n",
    "\n",
    "    meta = {\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": best_name,\n",
    "        \"MAE\": float(best_mae),\n",
    "        \"RMSE\": float(best_rmse),\n",
    "        \"R2\": float(best_r2),\n",
    "        \"TRAIN_SIZE\": int(len(X_train)),\n",
    "        \"TEST_SIZE\": int(len(X_test)),\n",
    "        \"FEATURE_COLS\": list(X_all.columns),\n",
    "        \"DROP_COLS\": DROP_COLS,\n",
    "        \"LAGS\": LAGS,\n",
    "        \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "        \"HAS_XGB\": bool(HAS_XGB),\n",
    "        \"XGB_PARAMS\": XGB_PARAMS if HAS_XGB else None,\n",
    "        \"ET_PARAMS\": ET_PARAMS,\n",
    "    }\n",
    "    with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ BEST => {best_name} | MAE={best_mae:,.3f} 저장: {best_path}\")\n",
    "\n",
    "    all_results.append({\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": best_name,\n",
    "        \"MAE\": float(best_mae),\n",
    "        \"RMSE\": float(best_rmse),\n",
    "        \"R2\": float(best_r2),\n",
    "        \"SAVE_PATH\": best_path\n",
    "    })\n",
    "\n",
    "# 결과 저장\n",
    "res_df = pd.DataFrame(all_results).sort_values([\"HOUR\"])\n",
    "res_path = os.path.join(SAVE_DIR, \"results_by_hour.csv\")\n",
    "res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[DONE] 학습 완료\")\n",
    "print(f\"- 결과표 저장: {res_path}\")\n",
    "print(f\"- 모델 저장 폴더: {SAVE_DIR}\")\n",
    "print(f\"- 총 소요(초): {time.time() - t0:,.1f}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 6) 로드/예측 헬퍼\n",
    "# =============================\n",
    "def load_best_bundle(hour: int):\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{hour:02d}\")\n",
    "    path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "\"\"\"\n",
    "사용 예:\n",
    "bundle = load_best_bundle(5)\n",
    "sample = X_all.iloc[[0]].copy()          # 컬럼 구조 동일해야 함\n",
    "yhat = predict_with_bundle(bundle, sample)\n",
    "print(yhat)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28211d39",
   "metadata": {},
   "source": [
    "# 6. 정님 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e09683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TA_YMD  DAY  HOUR BEST_MODEL  TEMP  RAIN  PRED_UNIT  KMA_BASE_USED\n",
      "0  20251229    1     1        XGB  0.25   0.0      56506  20251229-2000\n",
      "1  20251229    1     2        XGB  0.25   0.0      11251  20251229-2000\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "SERVICE_KEY = os.getenv(\"RAIN_ID\")\n",
    "\n",
    "# =========================\n",
    "# A방식 학습 저장 위치\n",
    "# =========================\n",
    "MODELS_ROOT = \"data/model\"\n",
    "MODEL_BUNDLE_TEMPLATE = os.path.join(MODELS_ROOT, \"HOUR_{hour:02d}\", \"best_model.joblib\")\n",
    "\n",
    "USE_LOG_TARGET = False\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "# =========================\n",
    "# 항목요약 시간대(01~10)\n",
    "# =========================\n",
    "HOUR_BINS = {\n",
    "    1: list(range(0, 7)),\n",
    "    2: [7, 8],\n",
    "    3: [9, 10],\n",
    "    4: [11, 12],\n",
    "    5: [13, 14],\n",
    "    6: [15, 16],\n",
    "    7: [17, 18],\n",
    "    8: [19, 20],\n",
    "    9: [21, 22],\n",
    "    10: [23],\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 날짜 파생 + DAY(01=월..07=일)\n",
    "# =========================\n",
    "def compute_day_code(date_yyyymmdd: str) -> int:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return int(dt.weekday() + 1)\n",
    "\n",
    "def make_date_features(date_yyyymmdd: str) -> dict:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return {\n",
    "        \"TA_YMD\": str(date_yyyymmdd),\n",
    "        \"YEAR\": int(dt.year),\n",
    "        \"MONTH\": int(dt.month),\n",
    "        \"DAY_OF_MONTH\": int(dt.day),\n",
    "        \"WEEKOFYEAR\": int(dt.isocalendar().week),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 기상청 단기예보 TMP/PCP\n",
    "# =========================\n",
    "def _parse_pcp_to_mm(x):\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s in (\"강수없음\", \"없음\", \"\", \"nan\", \"NaN\"):\n",
    "        return 0.0\n",
    "    if \"미만\" in s:\n",
    "        try:\n",
    "            num = float(s.replace(\"mm\", \"\").replace(\"미만\", \"\").strip())\n",
    "            return max(0.0, num * 0.5)\n",
    "        except:\n",
    "            return 0.0\n",
    "    if \"~\" in s:\n",
    "        try:\n",
    "            a, b = s.replace(\"mm\", \"\").split(\"~\")\n",
    "            return (float(a) + float(b)) / 2.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    try:\n",
    "        return float(s.replace(\"mm\", \"\"))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def fetch_vilage_fcst_json(service_key, base_date, base_time, nx, ny, num_rows=3000):\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_hourly_tmp_pcp_for_date(service_key, target_yyyymmdd, nx, ny):\n",
    "    base_times = [\"2300\",\"2000\",\"1700\",\"1400\",\"1100\",\"0800\",\"0500\",\"0200\"]\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    for bd in [today, yesterday]:\n",
    "        for bt in base_times:\n",
    "            try:\n",
    "                js = fetch_vilage_fcst_json(service_key, bd, bt, nx, ny)\n",
    "                items = js[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "                df = pd.DataFrame(items)\n",
    "\n",
    "                df = df[df[\"category\"].isin([\"TMP\", \"PCP\"])]\n",
    "                df = df[df[\"fcstDate\"].astype(str) == str(target_yyyymmdd)]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                df[\"fcstTime\"] = df[\"fcstTime\"].astype(str).str.zfill(4)\n",
    "                piv = df.pivot_table(index=\"fcstTime\", columns=\"category\", values=\"fcstValue\", aggfunc=\"first\").reset_index()\n",
    "\n",
    "                if \"TMP\" not in piv.columns:\n",
    "                    continue\n",
    "\n",
    "                piv[\"TMP\"] = piv[\"TMP\"].astype(float)\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].apply(_parse_pcp_to_mm) if \"PCP\" in piv.columns else 0.0\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].astype(float)\n",
    "                piv[\"base_used\"] = f\"{bd}-{bt}\"\n",
    "                return piv.sort_values(\"fcstTime\").reset_index(drop=True)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(\"해당 날짜의 기상청 예보 데이터를 찾지 못했습니다.\")\n",
    "\n",
    "def aggregate_weather_to_item_slots(hourly_df: pd.DataFrame) -> dict:\n",
    "    hourly_df[\"HOUR_OF_DAY\"] = hourly_df[\"fcstTime\"].str[:2].astype(int)\n",
    "    out = {}\n",
    "    for h, hours in HOUR_BINS.items():\n",
    "        sub = hourly_df[hourly_df[\"HOUR_OF_DAY\"].isin(hours)]\n",
    "        out[h] = {\n",
    "            \"TEMP\": float(sub[\"TMP\"].mean()) if not sub.empty else np.nan,\n",
    "            \"RAIN\": float(sub[\"PCP\"].sum()) if not sub.empty else 0.0,\n",
    "        }\n",
    "\n",
    "    temps = [v[\"TEMP\"] for v in out.values() if not pd.isna(v[\"TEMP\"])]\n",
    "    fill_temp = float(np.mean(temps)) if temps else 0.0\n",
    "    for h in out:\n",
    "        if pd.isna(out[h][\"TEMP\"]):\n",
    "            out[h][\"TEMP\"] = fill_temp\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# ✅ 핵심: 번들 로드 + 컬럼 align\n",
    "# =========================\n",
    "def load_best_bundle(hour: int) -> dict:\n",
    "    path = MODEL_BUNDLE_TEMPLATE.format(hour=hour)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"best_model.joblib이 없습니다: {path}\")\n",
    "    bundle = joblib.load(path)\n",
    "    if not isinstance(bundle, dict) or \"preprocess\" not in bundle or \"model\" not in bundle:\n",
    "        raise ValueError(f\"번들 포맷이 아닙니다(학습 A방식 저장물인지 확인): {path}\")\n",
    "    return bundle\n",
    "\n",
    "def get_required_input_columns(preprocess) -> list:\n",
    "    \"\"\"\n",
    "    ColumnTransformer가 fit될 때의 입력 컬럼명.\n",
    "    sklearn >= 1.0 이면 feature_names_in_ 존재.\n",
    "    없으면 transformers_에서 cols를 모아서 만든다.\n",
    "    \"\"\"\n",
    "    if hasattr(preprocess, \"feature_names_in_\"):\n",
    "        return [str(c) for c in preprocess.feature_names_in_]\n",
    "\n",
    "    # fallback\n",
    "    cols = []\n",
    "    for _, _, c in getattr(preprocess, \"transformers_\", []):\n",
    "        if c is None:\n",
    "            continue\n",
    "        if isinstance(c, (list, tuple, np.ndarray, pd.Index)):\n",
    "            cols.extend(list(c))\n",
    "    return sorted(set([str(x) for x in cols]))\n",
    "\n",
    "def align_X_to_required_columns(X: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
    "    X2 = X.copy()\n",
    "\n",
    "    # 없는 컬럼은 NaN으로 생성\n",
    "    for c in required_cols:\n",
    "        if c not in X2.columns:\n",
    "            X2[c] = np.nan\n",
    "\n",
    "    # 필요 없는 컬럼은 제거(안 해도 되지만 안전)\n",
    "    X2 = X2[required_cols]\n",
    "    return X2\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "\n",
    "    required_cols = get_required_input_columns(pre)\n",
    "    X_aligned = align_X_to_required_columns(X_new, required_cols)\n",
    "\n",
    "    X_enc = pre.transform(X_aligned)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "# =========================\n",
    "# 날짜 1개 → 1~10시간대 예측\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather(date_yyyymmdd: str, nx: int, ny: int, dong: str = None, service_key: str = SERVICE_KEY) -> pd.DataFrame:\n",
    "    if not service_key:\n",
    "        raise ValueError(\"RAIN_ID(기상청 서비스키)가 .env 또는 환경변수에 없습니다.\")\n",
    "\n",
    "    day_code = compute_day_code(date_yyyymmdd)\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for hour in range(1, 11):\n",
    "        try:\n",
    "            bundle = load_best_bundle(hour)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "        row = {}\n",
    "        row.update(make_date_features(date_yyyymmdd))\n",
    "        row.update({\n",
    "            \"HOUR\": hour,\n",
    "            \"DAY\": day_code,\n",
    "            \"TEMP\": weather_by_hour[hour][\"TEMP\"],\n",
    "            \"RAIN\": weather_by_hour[hour][\"RAIN\"],\n",
    "            \"DELIV_TEMP\": weather_by_hour[hour][\"TEMP\"],\n",
    "            \"DELIV_RAIN\": weather_by_hour[hour][\"RAIN\"],\n",
    "        })\n",
    "\n",
    "        # ✅ 학습 때 DONG이 있었다면, 예측에도 넣어주는 게 성능/안정에 중요\n",
    "        if dong is not None:\n",
    "            row[\"DONG\"] = str(dong)\n",
    "\n",
    "        X_row = pd.DataFrame([row])\n",
    "        X_row = X_row.drop(columns=[c for c in DROP_COLS if c in X_row.columns], errors=\"ignore\")\n",
    "\n",
    "        pred = float(predict_with_bundle(bundle, X_row)[0])\n",
    "        if USE_LOG_TARGET:\n",
    "            pred = float(np.expm1(pred))\n",
    "\n",
    "        rows.append({\n",
    "            \"TA_YMD\": str(date_yyyymmdd),\n",
    "            \"DAY\": day_code,\n",
    "            \"HOUR\": hour,\n",
    "            \"BEST_MODEL\": bundle.get(\"model_name\", \"UNKNOWN\"),\n",
    "            \"TEMP\": float(row[\"TEMP\"]),\n",
    "            \"RAIN\": float(row[\"RAIN\"]),\n",
    "            \"PRED_UNIT\": int(round(pred)),\n",
    "            \"KMA_BASE_USED\": str(hourly_weather.loc[0, \"base_used\"]),\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        raise FileNotFoundError(f\"{MODELS_ROOT}/HOUR_XX/best_model.joblib 파일을 찾지 못했습니다. 저장 경로를 확인하세요.\")\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# 사용 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df_pred = predict_date_all_hours_with_weather(\n",
    "        date_yyyymmdd=\"20251229\",\n",
    "        nx=61,\n",
    "        ny=121,\n",
    "        dong=\"행궁동\"   # ✅ 가능하면 꼭 넣으세요 (학습에 DONG이 있었기 때문)\n",
    "    )\n",
    "    print(df_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098a4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a4ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02173ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp (ipykernel)",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
