{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41a43fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.text_cell_render.rendered_html{font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.text_cell_render.rendered_html{font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09745b67",
   "metadata": {},
   "source": [
    "# 1. 한식만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691ca872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2711\n",
      "[LightGBM] [Info] Number of data points in the train set: 88705, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2080467.000000\n",
      "Fold 1 MAE: 126,532\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2753\n",
      "[LightGBM] [Info] Number of data points in the train set: 177406, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2119460.000000\n",
      "Fold 2 MAE: 318,519\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 266107, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2140878.000000\n",
      "Fold 3 MAE: 222,968\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 354808, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108242.500000\n",
      "Fold 4 MAE: 219,465\n",
      "\n",
      "====================\n",
      "CV 평균 MAE: 221,871\n",
      "====================\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2847\n",
      "[LightGBM] [Info] Number of data points in the train set: 443509, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2107993.000000\n",
      "최종 모델 학습 완료\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# ===== (1) 날짜 컬럼 찾기/통일 =====\n",
    "date_col = \"DATE\" if \"DATE\" in df.columns else (\"TA_YMD\" if \"TA_YMD\" in df.columns else None)\n",
    "if date_col is None:\n",
    "    raise ValueError(\"날짜 컬럼이 DATE 또는 TA_YMD로 존재해야 합니다.\")\n",
    "\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "TARGET = \"AMT\"\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"타겟 컬럼 {TARGET} 이(가) 없습니다. 컬럼명 확인 필요.\")\n",
    "\n",
    "# ===== (2) DONG 문자열 처리 =====\n",
    "cat_cols = []\n",
    "if \"DONG\" in df.columns:\n",
    "    df[\"DONG\"] = df[\"DONG\"].astype(\"category\")\n",
    "    cat_cols.append(\"DONG\")\n",
    "\n",
    "# ===== (3) 피처 엔지니어링 =====\n",
    "def make_features(data):\n",
    "    d = data.copy()\n",
    "\n",
    "    # 날짜 파생 (datetime 제거 목적)\n",
    "    d[\"year\"] = d[date_col].dt.year\n",
    "    d[\"month\"] = d[date_col].dt.month\n",
    "    d[\"day\"] = d[date_col].dt.day\n",
    "    d[\"dow\"] = d[date_col].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # Lag / Rolling (AMT 기반)\n",
    "    for lag in [1, 7, 14]:\n",
    "        d[f\"lag_{lag}\"] = d[TARGET].shift(lag)\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        d[f\"roll_mean_{win}\"] = d[TARGET].shift(1).rolling(win).mean()\n",
    "        d[f\"roll_std_{win}\"] = d[TARGET].shift(1).rolling(win).std()\n",
    "\n",
    "    return d\n",
    "\n",
    "df = make_features(df)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# ===== (4) 학습용 X/y 구성 =====\n",
    "drop_cols = [TARGET, date_col]\n",
    "FEATURES = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# LightGBM이 category dtype을 처리할 수 있도록 유지\n",
    "# (단, 혹시 object 남아있으면 category로 바꿔줌)\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == \"object\":\n",
    "        X[c] = X[c].astype(\"category\")\n",
    "        if c not in cat_cols:\n",
    "            cat_cols.append(c)\n",
    "\n",
    "# ===== (5) TimeSeries CV =====\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "mae_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"l1\",\n",
    "        categorical_feature=cat_cols if len(cat_cols) > 0 else \"auto\",\n",
    "        callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    mae_scores.append(mae)\n",
    "    print(f\"Fold {fold+1} MAE: {mae:,.0f}\")\n",
    "\n",
    "print(\"\\n====================\")\n",
    "print(f\"CV 평균 MAE: {np.mean(mae_scores):,.0f}\")\n",
    "print(\"====================\")\n",
    "\n",
    "# ===== (6) 최종 모델 =====\n",
    "final_model = lgb.LGBMRegressor(\n",
    "    objective=\"regression_l1\",\n",
    "    n_estimators=int(np.mean([model.best_iteration_ for _ in range(1)]) or 1500),\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=0.3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "final_model.fit(X, y, categorical_feature=cat_cols if len(cat_cols) > 0 else \"auto\")\n",
    "\n",
    "print(\"최종 모델 학습 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8646e41",
   "metadata": {},
   "source": [
    "# 2. 배달, 한식, 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d211dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [han] 학습 시작 (mode=temp) =====\n",
      "[han] 핵심 날씨 컬럼 후보: ['TEMP']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2711\n",
      "[LightGBM] [Info] Number of data points in the train set: 88583, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2083101.000000\n",
      "[han] Fold 1 MAE: 121,904  (best_iter=3499)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2753\n",
      "[LightGBM] [Info] Number of data points in the train set: 177164, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2120076.500000\n",
      "[han] Fold 2 MAE: 297,366  (best_iter=3500)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 265745, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2141769.000000\n",
      "[han] Fold 3 MAE: 199,292  (best_iter=3500)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 354326, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108547.500000\n",
      "[han] Fold 4 MAE: 197,593  (best_iter=3500)\n",
      "[han] CV 평균 MAE: 204,039 / 최종 n_estimators=3499\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2847\n",
      "[LightGBM] [Info] Number of data points in the train set: 442907, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 2108401.000000\n",
      "✅ [han] 최고 성능 모델 저장 완료\n",
      " - model: data/models\\han_best_model.pkl\n",
      " - meta : data/models\\han_best_meta.pkl\n",
      "\n",
      "===== [delivery] 학습 시작 (mode=rain) =====\n",
      "[delivery] 핵심 날씨 컬럼 후보: ['RAIN']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2439\n",
      "[LightGBM] [Info] Number of data points in the train set: 7485, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 65015.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 1 MAE: 7,871  (best_iter=2586)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2475\n",
      "[LightGBM] [Info] Number of data points in the train set: 14966, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 68111.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 2 MAE: 6,819  (best_iter=3497)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2514\n",
      "[LightGBM] [Info] Number of data points in the train set: 22447, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 61920.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 3 MAE: 7,027  (best_iter=1969)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2523\n",
      "[LightGBM] [Info] Number of data points in the train set: 29928, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 58824.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[delivery] Fold 4 MAE: 5,838  (best_iter=3500)\n",
      "[delivery] CV 평균 MAE: 6,889 / 최종 n_estimators=2888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2562\n",
      "[LightGBM] [Info] Number of data points in the train set: 37409, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 56966.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [delivery] 최고 성능 모델 저장 완료\n",
      " - model: data/models\\delivery_best_model.pkl\n",
      " - meta : data/models\\delivery_best_meta.pkl\n",
      "\n",
      "===== 전체 완료 =====\n",
      "한식 CV MAE: 204,039\n",
      "배달 CV MAE: 6,889\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# 경로/설정\n",
    "# =========================\n",
    "HAN_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DELIVERY_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "OUT_DIR = \"data/models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "# =========================\n",
    "# 유틸: 날씨 컬럼 자동 탐지 (temp/rain)\n",
    "# =========================\n",
    "def find_weather_cols(df, mode: str):\n",
    "    cols = list(df.columns)\n",
    "    low = {c: str(c).lower() for c in cols}\n",
    "\n",
    "    if mode == \"temp\":\n",
    "        keys = [\"temp\", \"tavg\", \"tmean\", \"기온\", \"평균기온\", \"최고기온\", \"최저기온\"]\n",
    "    elif mode == \"rain\":\n",
    "        keys = [\"rain\", \"precip\", \"prcp\", \"강수\", \"강수량\", \"강우\", \"mm\"]\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'temp' or 'rain'\")\n",
    "\n",
    "    picked = []\n",
    "    for c in cols:\n",
    "        s = low[c]\n",
    "        if any(k in s for k in keys):\n",
    "            picked.append(c)\n",
    "\n",
    "    # 숫자형만 유지\n",
    "    picked = [c for c in picked if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return picked\n",
    "\n",
    "# =========================\n",
    "# 피처 생성\n",
    "#  - 날짜 파생(연/월/일/요일 등)\n",
    "#  - 동(DONG) 있으면 category로 사용\n",
    "#  - lag/rolling (동별이면 groupby 적용)\n",
    "# =========================\n",
    "def make_features(df: pd.DataFrame, group_col: str | None = \"DONG\"):\n",
    "    d = df.copy()\n",
    "\n",
    "    if DATE_COL not in d.columns:\n",
    "        raise ValueError(f\"날짜 컬럼 {DATE_COL} 없음\")\n",
    "    if TARGET not in d.columns:\n",
    "        raise ValueError(f\"타겟 컬럼 {TARGET} 없음\")\n",
    "\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # 날짜 파생 (datetime dtype 제거 목적)\n",
    "    d[\"year\"] = d[DATE_COL].dt.year\n",
    "    d[\"month\"] = d[DATE_COL].dt.month\n",
    "    d[\"day\"] = d[DATE_COL].dt.day\n",
    "    d[\"dow\"] = d[DATE_COL].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    cat_cols = []\n",
    "    if group_col and group_col in d.columns:\n",
    "        d[group_col] = d[group_col].astype(\"category\")\n",
    "        cat_cols.append(group_col)\n",
    "    else:\n",
    "        group_col = None\n",
    "\n",
    "    # Lag/Rolling\n",
    "    def add_lag_roll(x):\n",
    "        for lag in [1, 7, 14]:\n",
    "            x[f\"lag_{lag}\"] = x[TARGET].shift(lag)\n",
    "        for win in [7, 14]:\n",
    "            x[f\"roll_mean_{win}\"] = x[TARGET].shift(1).rolling(win).mean()\n",
    "            x[f\"roll_std_{win}\"] = x[TARGET].shift(1).rolling(win).std()\n",
    "        return x\n",
    "\n",
    "    if group_col:\n",
    "        d = d.groupby(group_col, group_keys=False).apply(add_lag_roll)\n",
    "    else:\n",
    "        d = add_lag_roll(d)\n",
    "\n",
    "    # object 남아있으면 category로 (LightGBM dtype 에러 방지)\n",
    "    for c in d.columns:\n",
    "        if d[c].dtype == \"object\" and c != TARGET:\n",
    "            d[c] = d[c].astype(\"category\")\n",
    "            if c not in cat_cols:\n",
    "                cat_cols.append(c)\n",
    "\n",
    "    return d, cat_cols, group_col\n",
    "\n",
    "# =========================\n",
    "# 학습 + 최고 성능 모델 저장\n",
    "#  - TS CV 4 folds\n",
    "#  - 각 fold의 best_iteration_ 평균으로 최종 모델 재학습\n",
    "#  - CV 평균 MAE가 더 낮으면 파일 덮어쓰기\n",
    "# =========================\n",
    "def train_and_save_best(df: pd.DataFrame, name: str, mode: str):\n",
    "    \"\"\"\n",
    "    name: 저장 파일 prefix (예: 'han', 'delivery')\n",
    "    mode: 'temp' or 'rain' (컬럼 자동 탐지용)\n",
    "    \"\"\"\n",
    "    weather_cols = find_weather_cols(df, mode=mode)\n",
    "\n",
    "    d, cat_cols, group_col = make_features(df, group_col=\"DONG\")\n",
    "\n",
    "    # 학습 피처: 타겟/날짜 제외 전부 (날씨 컬럼이 포함되어 있으면 자동 포함)\n",
    "    drop_cols = [TARGET, DATE_COL]\n",
    "    feature_cols = [c for c in d.columns if c not in drop_cols]\n",
    "\n",
    "    # NA 제거\n",
    "    X = d[feature_cols].copy()\n",
    "    y = d[TARGET].copy()\n",
    "    mask = ~X.isna().any(axis=1) & ~y.isna()\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "\n",
    "    # TS CV\n",
    "    tscv = TimeSeriesSplit(n_splits=4)\n",
    "    maes, best_iters = [], []\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=3500,          # early stopping으로 자동 컷\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== [{name}] 학습 시작 (mode={mode}) =====\")\n",
    "    if len(weather_cols) == 0:\n",
    "        print(f\"⚠️ [{name}] {mode} 관련 날씨 컬럼 자동 탐지 실패(그래도 학습은 진행).\")\n",
    "    else:\n",
    "        print(f\"[{name}] 핵심 날씨 컬럼 후보: {weather_cols}\")\n",
    "\n",
    "    for fold, (tr, va) in enumerate(tscv.split(X), start=1):\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X.iloc[tr], y.iloc[tr],\n",
    "            eval_set=[(X.iloc[va], y.iloc[va])],\n",
    "            eval_metric=\"l1\",\n",
    "            categorical_feature=cat_cols if len(cat_cols) else \"auto\",\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "        pred = model.predict(X.iloc[va])\n",
    "        mae = mean_absolute_error(y.iloc[va], pred)\n",
    "        maes.append(mae)\n",
    "        best_iters.append(model.best_iteration_)\n",
    "        print(f\"[{name}] Fold {fold} MAE: {mae:,.0f}  (best_iter={model.best_iteration_})\")\n",
    "\n",
    "    cv_mae = float(np.mean(maes))\n",
    "    final_estimators = int(np.clip(np.mean(best_iters), 300, 3500))\n",
    "    print(f\"[{name}] CV 평균 MAE: {cv_mae:,.0f} / 최종 n_estimators={final_estimators}\")\n",
    "\n",
    "    # 최종 모델 재학습\n",
    "    final_model = lgb.LGBMRegressor(**{**params, \"n_estimators\": final_estimators})\n",
    "    final_model.fit(X, y, categorical_feature=cat_cols if len(cat_cols) else \"auto\")\n",
    "\n",
    "    # 저장 경로\n",
    "    model_path = os.path.join(OUT_DIR, f\"{name}_best_model.pkl\")\n",
    "    meta_path  = os.path.join(OUT_DIR, f\"{name}_best_meta.pkl\")\n",
    "\n",
    "    # 기존 저장 모델이 있으면 성능 비교 후 더 좋을 때만 덮어쓰기\n",
    "    should_save = True\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            old_meta = joblib.load(meta_path)\n",
    "            old_mae = float(old_meta.get(\"cv_mae\", np.inf))\n",
    "            if cv_mae >= old_mae:\n",
    "                should_save = False\n",
    "                print(f\"[{name}] 기존 모델이 더 좋거나 동일함: old_CV_MAE={old_mae:,.0f} <= new_CV_MAE={cv_mae:,.0f}\")\n",
    "        except Exception:\n",
    "            # 메타가 깨져있으면 새로 저장\n",
    "            should_save = True\n",
    "\n",
    "    if should_save:\n",
    "        joblib.dump(final_model, model_path)\n",
    "        meta = {\n",
    "            \"cv_mae\": cv_mae,\n",
    "            \"date_col\": DATE_COL,\n",
    "            \"target_col\": TARGET,\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"cat_cols\": cat_cols,\n",
    "            \"group_col\": group_col,\n",
    "            \"mode\": mode,\n",
    "            \"weather_cols_detected\": weather_cols,\n",
    "            \"n_estimators\": final_estimators,\n",
    "        }\n",
    "        joblib.dump(meta, meta_path)\n",
    "        print(f\"✅ [{name}] 최고 성능 모델 저장 완료\")\n",
    "        print(f\" - model: {model_path}\")\n",
    "        print(f\" - meta : {meta_path}\")\n",
    "\n",
    "    return final_model, cv_mae\n",
    "\n",
    "# =========================\n",
    "# 실행\n",
    "# =========================\n",
    "han_df = pd.read_csv(HAN_PATH)\n",
    "del_df = pd.read_csv(DELIVERY_PATH)\n",
    "\n",
    "# 한식 = 기온 중심\n",
    "han_model, han_cv_mae = train_and_save_best(han_df, name=\"han\", mode=\"temp\")\n",
    "\n",
    "# 배달 = 강수 중심\n",
    "del_model, del_cv_mae = train_and_save_best(del_df, name=\"delivery\", mode=\"rain\")\n",
    "\n",
    "print(\"\\n===== 전체 완료 =====\")\n",
    "print(f\"한식 CV MAE: {han_cv_mae:,.0f}\")\n",
    "print(f\"배달 CV MAE: {del_cv_mae:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944adc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 평가: data/models/han_best_model.pkl ====\n",
      "Samples: train=354,325 / test=88,582\n",
      "MAE  : 151,157\n",
      "RMSE : 1,679,193\n",
      "R2   : 0.9814\n",
      "WMAPE: 2.8072%\n",
      "\n",
      "[동별 MAE TOP 10 (좋은 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 권선구 세류1동  1444.0   5575.550229  0.071540\n",
      "수원시 권선구 세류3동  2065.0  13811.382568  0.010179\n",
      "수원시 영통구 매탄4동  2043.0  13914.801772  0.010554\n",
      "수원시 장안구 정자1동  2028.0  14649.504522  0.008968\n",
      "수원시 장안구 조원2동  1470.0  15033.683022  0.014717\n",
      "수원시 팔달구 화서1동  2055.0  17188.697282  0.009409\n",
      "수원시 권선구 입북동   1865.0  21462.478350  0.015297\n",
      "수원시 팔달구 매교동   2039.0  21764.044537  0.024374\n",
      "수원시 팔달구 고등동   2056.0  22144.977556  0.010232\n",
      "수원시 권선구 권선2동  2062.0  22380.597920  0.011487\n",
      "\n",
      "[동별 MAE WORST 10 (나쁜 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 영통구 매탄2동  2028.0  9.722877e+04  0.063856\n",
      "수원시 권선구 권선1동  2070.0  9.830294e+04  0.015604\n",
      "수원시 영통구 매탄3동  2060.0  1.079290e+05  0.015677\n",
      "수원시 영통구 광교1동  2059.0  1.771252e+05  0.016428\n",
      "수원시 영통구 원천동   2059.0  2.036054e+05  0.025479\n",
      "수원시 팔달구 행궁동   2060.0  2.375625e+05  0.020081\n",
      "수원시 영통구 영통2동  2060.0  2.384573e+05  0.029192\n",
      "수원시 영통구 영통3동  2060.0  2.391902e+05  0.019575\n",
      "수원시 팔달구 인계동   2060.0  6.738748e+05  0.021521\n",
      "수원시 팔달구 매산동   2060.0  3.267689e+06  0.072903\n",
      "\n",
      "==== 평가: data/models/delivery_best_model.pkl ====\n",
      "Samples: train=29,927 / test=7,482\n",
      "MAE  : 3,922\n",
      "RMSE : 57,061\n",
      "R2   : 0.9382\n",
      "WMAPE: 3.1086%\n",
      "\n",
      "[동별 MAE TOP 10 (좋은 순)]\n",
      "                  n         mae     wmape\n",
      "DONG                                     \n",
      "수원시 장안구 파장동    53.0   68.988621  0.004048\n",
      "수원시 장안구 조원2동   22.0   91.533046  0.002397\n",
      "수원시 권선구 곡선동   123.0  128.249513  0.004408\n",
      "수원시 영통구 매탄2동   32.0  145.901346  0.003445\n",
      "수원시 팔달구 매교동   145.0  163.913706  0.006012\n",
      "수원시 영통구 망포1동  889.0  219.821207  0.007710\n",
      "수원시 권선구 권선1동    4.0  226.300932  0.025989\n",
      "수원시 장안구 영화동   110.0  334.305410  0.006539\n",
      "수원시 권선구 권선2동   41.0  410.547076  0.009164\n",
      "수원시 장안구 정자1동  116.0  526.441465  0.012804\n",
      "\n",
      "[동별 MAE WORST 10 (나쁜 순)]\n",
      "                   n           mae     wmape\n",
      "DONG                                        \n",
      "수원시 장안구 정자3동   201.0   1298.538403  0.008402\n",
      "수원시 팔달구 인계동   1171.0   1573.279183  0.016786\n",
      "수원시 권선구 세류3동   563.0   1678.198905  0.029074\n",
      "수원시 권선구 금곡동    325.0   2746.872868  0.012294\n",
      "수원시 권선구 호매실동    38.0   2850.075530  0.018004\n",
      "수원시 영통구 영통3동  1151.0   3565.511468  0.024360\n",
      "수원시 영통구 광교1동   384.0   5231.107485  0.017843\n",
      "수원시 영통구 광교2동   393.0   6446.216827  0.037606\n",
      "수원시 권선구 구운동    599.0   6690.896536  0.164116\n",
      "수원시 영통구 매탄3동  1049.0  11630.545347  0.045529\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# =====================\n",
    "# 경로\n",
    "# =====================\n",
    "HAN_MODEL_PATH = r\"data/models/han_best_model.pkl\"\n",
    "HAN_META_PATH  = r\"data/models/han_best_meta.pkl\"\n",
    "HAN_DATA_PATH  = r\"data/수원시 한식 데이터백업.csv\"   # 네 환경 경로로 맞춰\n",
    "\n",
    "DEL_MODEL_PATH = r\"data/models/delivery_best_model.pkl\"\n",
    "DEL_META_PATH  = r\"data/models/delivery_best_meta.pkl\"\n",
    "DEL_DATA_PATH  = r\"data/수원시 배달 데이터백업.csv\"  # 네 환경 경로로 맞춰\n",
    "\n",
    "# =====================\n",
    "# 공통: 피처 생성 (학습 코드와 동일해야 함)\n",
    "# =====================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "def make_features(df: pd.DataFrame, group_col: str | None = \"DONG\"):\n",
    "    d = df.copy()\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    d[\"year\"] = d[DATE_COL].dt.year\n",
    "    d[\"month\"] = d[DATE_COL].dt.month\n",
    "    d[\"day\"] = d[DATE_COL].dt.day\n",
    "    d[\"dow\"] = d[DATE_COL].dt.weekday\n",
    "    d[\"weekend\"] = (d[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    cat_cols = []\n",
    "    if group_col and group_col in d.columns:\n",
    "        d[group_col] = d[group_col].astype(\"category\")\n",
    "        cat_cols.append(group_col)\n",
    "    else:\n",
    "        group_col = None\n",
    "\n",
    "    def add_lag_roll(x):\n",
    "        for lag in [1, 7, 14]:\n",
    "            x[f\"lag_{lag}\"] = x[TARGET].shift(lag)\n",
    "        for win in [7, 14]:\n",
    "            x[f\"roll_mean_{win}\"] = x[TARGET].shift(1).rolling(win).mean()\n",
    "            x[f\"roll_std_{win}\"] = x[TARGET].shift(1).rolling(win).std()\n",
    "        return x\n",
    "\n",
    "    if group_col:\n",
    "        d = d.groupby(group_col, group_keys=False).apply(add_lag_roll)\n",
    "    else:\n",
    "        d = add_lag_roll(d)\n",
    "\n",
    "    # object -> category (LightGBM dtype 방지)\n",
    "    for c in d.columns:\n",
    "        if d[c].dtype == \"object\" and c not in [TARGET]:\n",
    "            d[c] = d[c].astype(\"category\")\n",
    "            if c not in cat_cols:\n",
    "                cat_cols.append(c)\n",
    "\n",
    "    return d\n",
    "\n",
    "def wmape(y_true, y_pred, eps=1e-9):\n",
    "    denom = np.sum(np.abs(y_true)) + eps\n",
    "    return np.sum(np.abs(y_true - y_pred)) / denom\n",
    "\n",
    "# =====================\n",
    "# 평가 함수\n",
    "# =====================\n",
    "def evaluate_saved_model(model_path, meta_path, data_path, holdout_ratio=0.2, print_by_dong=True):\n",
    "    model = joblib.load(model_path)\n",
    "    meta = joblib.load(meta_path)\n",
    "\n",
    "    feature_cols = meta[\"feature_cols\"]\n",
    "    cat_cols = meta.get(\"cat_cols\", [])\n",
    "    group_col = meta.get(\"group_col\", None)\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    d = make_features(df, group_col=group_col)\n",
    "\n",
    "    # 학습 때처럼 NA 제거\n",
    "    X = d[feature_cols].copy()\n",
    "    y = d[TARGET].copy()\n",
    "    mask = ~X.isna().any(axis=1) & ~y.isna()\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "\n",
    "    # 시계열 holdout: 뒤쪽 20%를 test로\n",
    "    n = len(X)\n",
    "    split = int(n * (1 - holdout_ratio))\n",
    "    X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "    # 예측\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    # 지표\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    w = wmape(y_test.values, pred)\n",
    "\n",
    "    print(f\"\\n==== 평가: {model_path} ====\")\n",
    "    print(f\"Samples: train={len(X_train):,} / test={len(X_test):,}\")\n",
    "    print(f\"MAE  : {mae:,.0f}\")\n",
    "    print(f\"RMSE : {rmse:,.0f}\")\n",
    "    print(f\"R2   : {r2:.4f}\")\n",
    "    print(f\"WMAPE: {w:.4%}\")\n",
    "\n",
    "        # (선택) 동별 MAE\n",
    "    if print_by_dong and group_col and group_col in d.columns:\n",
    "        d_masked = d.loc[mask].reset_index(drop=True)\n",
    "        d_test = d_masked.iloc[split:].copy()\n",
    "        d_test[\"pred\"] = pred\n",
    "\n",
    "        # 샘플 수가 1 이상인 동만\n",
    "        grp = d_test.groupby(group_col, observed=True)\n",
    "        stats = grp.apply(lambda g: pd.Series({\n",
    "            \"n\": len(g),\n",
    "            \"mae\": mean_absolute_error(g[TARGET].values, g[\"pred\"].values) if len(g) > 0 else np.nan,\n",
    "            \"wmape\": (np.sum(np.abs(g[TARGET].values - g[\"pred\"].values)) / (np.sum(np.abs(g[TARGET].values)) + 1e-9)) if len(g) > 0 else np.nan\n",
    "        }))\n",
    "        stats = stats.dropna().sort_values(\"mae\")\n",
    "\n",
    "        if len(stats) == 0:\n",
    "            print(\"\\n[동별 MAE] 테스트 구간에 유효한 동 데이터가 없습니다. (표본 부족)\")\n",
    "        else:\n",
    "            print(\"\\n[동별 MAE TOP 10 (좋은 순)]\")\n",
    "            print(stats.head(10))\n",
    "            print(\"\\n[동별 MAE WORST 10 (나쁜 순)]\")\n",
    "            print(stats.tail(10))\n",
    "\n",
    "# =====================\n",
    "# 실행\n",
    "# =====================\n",
    "han_res = evaluate_saved_model(HAN_MODEL_PATH, HAN_META_PATH, HAN_DATA_PATH)\n",
    "del_res = evaluate_saved_model(DEL_MODEL_PATH, DEL_META_PATH, DEL_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ca233",
   "metadata": {},
   "source": [
    "# 3. 매출 데이터 바탕으로 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8057c",
   "metadata": {},
   "source": [
    "## 점유율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44cd0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =====================================================\n",
    "# 상수 / 경로\n",
    "# =====================================================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "MY_DATE_COL = \"ta_ymd\"\n",
    "MY_AMT_COL = \"store_amt\"\n",
    "\n",
    "GRID_XLSX_PATH = \"data/수원시 격자.xlsx\"\n",
    "\n",
    "HAN_DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DEL_DATA_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "HAN_MODEL_PATH = \"data/models/han_best_model.pkl\"\n",
    "DEL_MODEL_PATH = \"data/models/delivery_best_model.pkl\"\n",
    "\n",
    "HAN_META_PATH = \"data/models/han_best_meta.pkl\"\n",
    "DEL_META_PATH = \"data/models/delivery_best_meta.pkl\"\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 0) 수원시 격자 로드\n",
    "# =====================================================\n",
    "def load_suwon_grid_map(xlsx_path=GRID_XLSX_PATH):\n",
    "    df = pd.read_excel(xlsx_path, header=None)\n",
    "    df = df.dropna(subset=[0, 1, 2])\n",
    "\n",
    "    grid = {}\n",
    "    for _, r in df.iterrows():\n",
    "        try:\n",
    "            grid[str(r[0]).strip()] = (int(r[1]), int(r[2]))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not grid:\n",
    "        raise ValueError(\"격자 엑셀에서 nx, ny를 읽지 못했습니다.\")\n",
    "    return grid\n",
    "\n",
    "\n",
    "def resolve_nxny_from_input_dong(dong, grid_map):\n",
    "    if dong not in grid_map:\n",
    "        raise ValueError(f\"'{dong}'이(가) 격자 엑셀에 없습니다.\")\n",
    "    return grid_map[dong][0], grid_map[dong][1]\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 1) 기상청 단기예보 + fallback (과거 평균 날씨)\n",
    "# =====================================================\n",
    "def _parse_pcp(v):\n",
    "    if v in [None, \"강수없음\", \"-\", \"없음\"]:\n",
    "        return 0.0\n",
    "    s = str(v).replace(\"mm\", \"\").strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        nums = re.findall(r\"[\\d.]+\", s)\n",
    "        return float(nums[0]) if nums else 0.0\n",
    "\n",
    "\n",
    "def fetch_daily_weather_kma(\n",
    "    nx, ny, target_date, RAIN_ID, base_df, dong\n",
    "):\n",
    "    \"\"\"\n",
    "    1) 기상청 단기예보 가능 → 사용\n",
    "    2) NO_DATA / 과거·미래 → 과거 평균 날씨 대체\n",
    "    \"\"\"\n",
    "\n",
    "    d = datetime.strptime(target_date, \"%Y-%m-%d\").date()\n",
    "    base_date = (d - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": RAIN_ID,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"numOfRows\": 3000,\n",
    "        \"pageNo\": 1,\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": \"2300\",\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        js = r.json()\n",
    "        body = js.get(\"response\", {}).get(\"body\")\n",
    "\n",
    "        if body:\n",
    "            items = body.get(\"items\", {}).get(\"item\", [])\n",
    "            want = d.strftime(\"%Y%m%d\")\n",
    "\n",
    "            temps, rains = [], []\n",
    "            for it in items:\n",
    "                if it.get(\"fcstDate\") != want:\n",
    "                    continue\n",
    "                if it.get(\"category\") == \"TMP\":\n",
    "                    temps.append(float(it.get(\"fcstValue\")))\n",
    "                elif it.get(\"category\") == \"PCP\":\n",
    "                    rains.append(_parse_pcp(it.get(\"fcstValue\")))\n",
    "\n",
    "            if temps:\n",
    "                return {\n",
    "                    \"temp_mean\": float(np.mean(temps)),\n",
    "                    \"rain_mean\": float(np.mean(rains)) if rains else 0.0,\n",
    "                    \"rain_peak\": float(np.max(rains)) if rains else 0.0,\n",
    "                    \"source\": \"기상청 단기예보\",\n",
    "                }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # =============================\n",
    "    # fallback: 과거 평균 날씨\n",
    "    # =============================\n",
    "    hist = base_df.copy()\n",
    "    hist[DATE_COL] = pd.to_datetime(hist[DATE_COL])\n",
    "    hist = hist[hist[\"DONG\"] == dong]\n",
    "\n",
    "    temp_cols = [c for c in hist.columns if \"temp\" in c.lower()]\n",
    "    rain_cols = [c for c in hist.columns if \"rain\" in c.lower()]\n",
    "\n",
    "    return {\n",
    "        \"temp_mean\": float(hist[temp_cols[0]].mean()) if temp_cols else np.nan,\n",
    "        \"rain_mean\": float(hist[rain_cols[0]].mean()) if rain_cols else 0.0,\n",
    "        \"rain_peak\": float(hist[rain_cols[0]].max()) if rain_cols else 0.0,\n",
    "        \"source\": \"과거 평균 날씨(대체)\",\n",
    "    }\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2) 공휴일 (천문연구원)\n",
    "# =====================================================\n",
    "def fetch_holiday_flag_kasi(target_date, HOLIDAY_ID):\n",
    "    d = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    url = \"https://apis.data.go.kr/B090041/openapi/service/SpcdeInfoService/getRestDeInfo\"\n",
    "    params = {\n",
    "        \"serviceKey\": HOLIDAY_ID,\n",
    "        \"solYear\": d.year,\n",
    "        \"solMonth\": f\"{d.month:02d}\",\n",
    "        \"_type\": \"json\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        items = (\n",
    "            r.json()\n",
    "            .get(\"response\", {})\n",
    "            .get(\"body\", {})\n",
    "            .get(\"items\", {})\n",
    "            .get(\"item\", [])\n",
    "        )\n",
    "\n",
    "        if isinstance(items, dict):\n",
    "            items = [items]\n",
    "\n",
    "        for it in items:\n",
    "            if str(it.get(\"locdate\")) == d.strftime(\"%Y%m%d\"):\n",
    "                return {\"is_holiday\": 1}\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return {\"is_holiday\": 0}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3) 예측용 feature 생성 (❗ DONG 제거됨)\n",
    "# =====================================================\n",
    "def build_feature_row_for_prediction(\n",
    "    base_df, feature_cols, dong, target_date, weather, is_holiday\n",
    "):\n",
    "    d = base_df.copy()\n",
    "    d[DATE_COL] = pd.to_datetime(d[DATE_COL])\n",
    "    d = d[d[\"DONG\"] == dong].sort_values(DATE_COL)\n",
    "\n",
    "    td = pd.to_datetime(target_date)\n",
    "    hist = d[d[DATE_COL] < td]\n",
    "\n",
    "    row = {\n",
    "        \"year\": td.year,\n",
    "        \"month\": td.month,\n",
    "        \"day\": td.day,\n",
    "        \"dow\": td.weekday(),\n",
    "        \"weekend\": int(td.weekday() >= 5),\n",
    "    }\n",
    "\n",
    "    for lag in [1, 7, 14]:\n",
    "        row[f\"lag_{lag}\"] = hist[TARGET].iloc[-lag] if len(hist) >= lag else np.nan\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        row[f\"roll_mean_{win}\"] = hist[TARGET].tail(win).mean() if len(hist) >= win else np.nan\n",
    "        row[f\"roll_std_{win}\"] = hist[TARGET].tail(win).std() if len(hist) >= win else np.nan\n",
    "\n",
    "    row[\"temp_mean\"] = weather[\"temp_mean\"]\n",
    "    row[\"rain_mean\"] = weather[\"rain_mean\"]\n",
    "    row[\"rain_peak\"] = weather[\"rain_peak\"]\n",
    "    row[\"is_holiday\"] = is_holiday\n",
    "\n",
    "    return pd.DataFrame([{c: row.get(c, np.nan) for c in feature_cols}])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4) 점유율 + 보정계수\n",
    "# =====================================================\n",
    "def compute_store_share_and_adj(\n",
    "    my_store_df, base_df, dong, target_date, lookback_days=60\n",
    "):\n",
    "    d0 = datetime.strptime(target_date, \"%Y-%m-%d\").date()\n",
    "    start = d0 - timedelta(days=lookback_days)\n",
    "\n",
    "    ms = my_store_df.copy()\n",
    "    ms[MY_DATE_COL] = pd.to_datetime(ms[MY_DATE_COL])\n",
    "    store_amt_60 = ms[\n",
    "        (ms[MY_DATE_COL].dt.date >= start) &\n",
    "        (ms[MY_DATE_COL].dt.date < d0)\n",
    "    ][MY_AMT_COL].sum()\n",
    "\n",
    "    bd = base_df.copy()\n",
    "    bd[DATE_COL] = pd.to_datetime(bd[DATE_COL])\n",
    "    dong_amt_60 = bd[\n",
    "        (bd[\"DONG\"] == dong) &\n",
    "        (bd[DATE_COL].dt.date >= start) &\n",
    "        (bd[DATE_COL].dt.date < d0)\n",
    "    ][TARGET].sum()\n",
    "\n",
    "    share = (store_amt_60 / dong_amt_60 * 100) if dong_amt_60 > 0 else 0.0\n",
    "    return share, 1.0\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5) 거래건수 추정\n",
    "# =====================================================\n",
    "def estimate_txn_count(my_store_df, target_date, pred_store_amt):\n",
    "    pseudo_ticket = max(15000.0, pred_store_amt / 20.0)\n",
    "    return int(pred_store_amt / pseudo_ticket)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6) 메인 함수\n",
    "# =====================================================\n",
    "def predict_my_store_sales(\n",
    "    my_store_csv,\n",
    "    dong,\n",
    "    date_str,\n",
    "    service_type,\n",
    "    RAIN_ID,\n",
    "    HOLIDAY_ID,\n",
    "    grid_xlsx_path=GRID_XLSX_PATH,\n",
    "):\n",
    "    if service_type == \"delivery\":\n",
    "        model_path, meta_path, data_path = (\n",
    "            DEL_MODEL_PATH, DEL_META_PATH, DEL_DATA_PATH\n",
    "        )\n",
    "    else:\n",
    "        model_path, meta_path, data_path = (\n",
    "            HAN_MODEL_PATH, HAN_META_PATH, HAN_DATA_PATH\n",
    "        )\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    meta = joblib.load(meta_path)\n",
    "\n",
    "    # ❗ DONG 제거\n",
    "    feature_cols = [c for c in meta[\"feature_cols\"] if c != \"DONG\"]\n",
    "\n",
    "    base_df = pd.read_csv(data_path)\n",
    "    my_store_df = pd.read_csv(my_store_csv)\n",
    "\n",
    "    grid_map = load_suwon_grid_map(grid_xlsx_path)\n",
    "    nx, ny = resolve_nxny_from_input_dong(dong, grid_map)\n",
    "\n",
    "    weather = fetch_daily_weather_kma(\n",
    "        nx, ny, date_str, RAIN_ID, base_df, dong\n",
    "    )\n",
    "    holi = fetch_holiday_flag_kasi(date_str, HOLIDAY_ID)\n",
    "\n",
    "    X_pred = build_feature_row_for_prediction(\n",
    "        base_df, feature_cols, dong, date_str, weather, holi[\"is_holiday\"]\n",
    "    )\n",
    "\n",
    "    pred_dong_amt = float(model.predict(X_pred)[0])\n",
    "    share, adj = compute_store_share_and_adj(\n",
    "        my_store_df, base_df, dong, date_str\n",
    "    )\n",
    "\n",
    "    pred_store_amt = pred_dong_amt * (share / 100.0) * adj\n",
    "    pred_cnt = estimate_txn_count(my_store_df, date_str, pred_store_amt)\n",
    "\n",
    "    # =========================\n",
    "    # 출력\n",
    "    # =========================\n",
    "    print(f\"📍지역(동): {dong}\")\n",
    "    print(f\"📅날짜: {date_str}\")\n",
    "    print(f\"🌦️날씨(출처): {weather['source']}\")\n",
    "    print(f\"   - 평균기온: {weather['temp_mean']:.1f}°C\")\n",
    "    print(f\"   - 강수(평균/피크): {weather['rain_mean']:.2f} / {weather['rain_peak']:.2f} mm\")\n",
    "    print(f\"👥예상 매출건수: {pred_cnt}건\")\n",
    "    print(f\"💰예상 동 전체 일매출: {pred_dong_amt:,.0f} 원\")\n",
    "    print(f\"🏪가게 점유율(최근 60일): {share:.2f}%\")\n",
    "    print(f\"🏪예상 가게 일매출: {pred_store_amt:,.0f} 원\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e431b73",
   "metadata": {},
   "source": [
    "## 사용예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "687af6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍지역(동): 행궁동\n",
      "📅날짜: 2025-12-30\n",
      "🌦️날씨(출처): 과거 평균 날씨(대체)\n",
      "   - 평균기온: 13.4°C\n",
      "   - 강수(평균/피크): 0.16 / 37.30 mm\n",
      "👥예상 매출건수: 0건\n",
      "💰예상 동 전체 일매출: 84,208 원\n",
      "🏪가게 점유율(최근 60일): 0.00%\n",
      "🏪예상 가게 일매출: 0 원\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "holiday_key_env = os.getenv(\"HOLIDAY_ID\")\n",
    "rain_key_env = os.getenv(\"RAIN_ID\")\n",
    "\n",
    "predict_my_store_sales(\n",
    "    my_store_csv=\"data/my_store.csv\",\n",
    "    dong=\"행궁동\",\n",
    "    date_str=\"2025-12-30\",\n",
    "    service_type=\"han\",\n",
    "    RAIN_ID=rain_key_env,\n",
    "    HOLIDAY_ID=holiday_key_env,\n",
    "    grid_xlsx_path=\"data/수원시 격자.xlsx\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528355d",
   "metadata": {},
   "source": [
    "# 4. 모델 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b67ea58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAIN START: han =====\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2760\n",
      "[LightGBM] [Info] Number of data points in the train set: 355094, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 2108030.000000\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l1: 210080\n",
      "✅ VALID MAE = 210,080.03\n",
      "💾 model saved: data/models/han_best_model.pkl\n",
      "💾 meta  saved: data/models/han_best_meta.pkl\n",
      "\n",
      "===== TRAIN START: delivery =====\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2500\n",
      "[LightGBM] [Info] Number of data points in the train set: 30234, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score 58824.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4998]\tvalid_0's l1: 4223.44\n",
      "✅ VALID MAE = 4,223.44\n",
      "💾 model saved: data/models/delivery_best_model.pkl\n",
      "💾 meta  saved: data/models/delivery_best_meta.pkl\n",
      "\n",
      "===== DONE =====\n",
      "HAN MAE     : 210,080.03\n",
      "DELIVERY MAE: 4,223.44\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import timedelta\n",
    "\n",
    "# =====================================================\n",
    "# 설정\n",
    "# =====================================================\n",
    "DATE_COL = \"TA_YMD\"\n",
    "TARGET = \"AMT\"\n",
    "\n",
    "HAN_DATA_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DEL_DATA_PATH = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "OUT_DIR = \"data/models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# 공통 전처리\n",
    "# =====================================================\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "    df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "    # ---------- 날짜 파생 ----------\n",
    "    df[\"year\"] = df[DATE_COL].dt.year\n",
    "    df[\"month\"] = df[DATE_COL].dt.month\n",
    "    df[\"day\"] = df[DATE_COL].dt.day\n",
    "    df[\"dow\"] = df[DATE_COL].dt.weekday\n",
    "    df[\"weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    # ---------- DONG 제거 ----------\n",
    "    if \"DONG\" in df.columns:\n",
    "        df = df.drop(columns=[\"DONG\"])\n",
    "\n",
    "    # ---------- lag / rolling ----------\n",
    "    for lag in [1, 7, 14]:\n",
    "        df[f\"lag_{lag}\"] = df[TARGET].shift(lag)\n",
    "\n",
    "    for win in [7, 14]:\n",
    "        df[f\"roll_mean_{win}\"] = df[TARGET].rolling(win).mean()\n",
    "        df[f\"roll_std_{win}\"] = df[TARGET].rolling(win).std()\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 학습 함수\n",
    "# =====================================================\n",
    "def train_and_save(\n",
    "    data_path: str,\n",
    "    model_name: str,\n",
    "):\n",
    "    print(f\"\\n===== TRAIN START: {model_name} =====\")\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = preprocess(df)\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in [DATE_COL, TARGET]]\n",
    "\n",
    "    # ---------- time split ----------\n",
    "    split_date = df[DATE_COL].quantile(0.8)\n",
    "    train_df = df[df[DATE_COL] <= split_date]\n",
    "    valid_df = df[df[DATE_COL] > split_date]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[TARGET]\n",
    "    X_val = valid_df[feature_cols]\n",
    "    y_val = valid_df[TARGET]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression_l1\",\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=30,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"l1\",\n",
    "        callbacks=[lgb.early_stopping(300, verbose=True)],\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "\n",
    "    print(f\"✅ VALID MAE = {mae:,.2f}\")\n",
    "\n",
    "    # ---------- 저장 ----------\n",
    "    model_path = f\"{OUT_DIR}/{model_name}_best_model.pkl\"\n",
    "    meta_path = f\"{OUT_DIR}/{model_name}_best_meta.pkl\"\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(\n",
    "        {\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"valid_mae\": mae,\n",
    "            \"n_train\": len(train_df),\n",
    "            \"n_valid\": len(valid_df),\n",
    "        },\n",
    "        meta_path,\n",
    "    )\n",
    "\n",
    "    print(f\"💾 model saved: {model_path}\")\n",
    "    print(f\"💾 meta  saved: {meta_path}\")\n",
    "\n",
    "    return mae\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 실행\n",
    "# =====================================================\n",
    "if __name__ == \"__main__\":\n",
    "    han_mae = train_and_save(HAN_DATA_PATH, \"han\")\n",
    "    del_mae = train_and_save(DEL_DATA_PATH, \"delivery\")\n",
    "\n",
    "    print(\"\\n===== DONE =====\")\n",
    "    print(f\"HAN MAE     : {han_mae:,.2f}\")\n",
    "    print(f\"DELIVERY MAE: {del_mae:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d48c8",
   "metadata": {},
   "source": [
    "# 5. 정님 모델 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c4e03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] XGB 사용 여부: True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model\n",
      "\n",
      "[DEBUG] >>> 학습 루프 시작 직전\n",
      "\n",
      "[DEBUG] >>> HOUR=01 루프 진입\n",
      "[DEBUG] HOUR=01 rows=42419\n",
      "[DEBUG] HOUR=01 enc shapes: tr=(30541, 267), val=(3394, 267), test=(8484, 267)\n",
      "[DEBUG] HOUR=01 DONE | MAE=16813.879\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_01\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=02 루프 진입\n",
      "[DEBUG] HOUR=02 rows=42498\n",
      "[DEBUG] HOUR=02 enc shapes: tr=(30598, 267), val=(3400, 267), test=(8500, 267)\n",
      "[DEBUG] HOUR=02 DONE | MAE=8036.923\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_02\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=03 루프 진입\n",
      "[DEBUG] HOUR=03 rows=44644\n",
      "[DEBUG] HOUR=03 enc shapes: tr=(32143, 267), val=(3572, 267), test=(8929, 267)\n",
      "[DEBUG] HOUR=03 DONE | MAE=8344.425\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_03\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=04 루프 진입\n",
      "[DEBUG] HOUR=04 rows=45451\n",
      "[DEBUG] HOUR=04 enc shapes: tr=(32724, 267), val=(3636, 267), test=(9091, 267)\n",
      "[DEBUG] HOUR=04 DONE | MAE=4265.749\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_04\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=05 루프 진입\n",
      "[DEBUG] HOUR=05 rows=45510\n",
      "[DEBUG] HOUR=05 enc shapes: tr=(32767, 267), val=(3641, 267), test=(9102, 267)\n",
      "[DEBUG] HOUR=05 DONE | MAE=5370.516\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_05\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=06 루프 진입\n",
      "[DEBUG] HOUR=06 rows=45501\n",
      "[DEBUG] HOUR=06 enc shapes: tr=(32760, 267), val=(3640, 267), test=(9101, 267)\n",
      "[DEBUG] HOUR=06 DONE | MAE=7684.002\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_06\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=07 루프 진입\n",
      "[DEBUG] HOUR=07 rows=45516\n",
      "[DEBUG] HOUR=07 enc shapes: tr=(32770, 267), val=(3642, 267), test=(9104, 267)\n",
      "[DEBUG] HOUR=07 DONE | MAE=6052.627\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_07\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=08 루프 진입\n",
      "[DEBUG] HOUR=08 rows=45439\n",
      "[DEBUG] HOUR=08 enc shapes: tr=(32715, 267), val=(3636, 267), test=(9088, 267)\n",
      "[DEBUG] HOUR=08 DONE | MAE=7437.397\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_08\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=09 루프 진입\n",
      "[DEBUG] HOUR=09 rows=44986\n",
      "[DEBUG] HOUR=09 enc shapes: tr=(32389, 267), val=(3599, 267), test=(8998, 267)\n",
      "[DEBUG] HOUR=09 DONE | MAE=12911.622\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_09\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] >>> HOUR=10 루프 진입\n",
      "[DEBUG] HOUR=10 rows=41559\n",
      "[DEBUG] HOUR=10 enc shapes: tr=(29922, 267), val=(3325, 267), test=(8312, 267)\n",
      "[DEBUG] HOUR=10 DONE | MAE=20040.728\n",
      "[DEBUG] ✅ SAVED => data/model\\HOUR_10\\best_model.joblib (exists=True)\n",
      "\n",
      "[DEBUG] 루프 종료: trained=10, skipped=0\n",
      "[DEBUG] SAVE_DIR 최종 파일 리스트 일부: ['HOUR_01', 'HOUR_02', 'HOUR_03', 'HOUR_04', 'HOUR_05', 'HOUR_06', 'HOUR_07', 'HOUR_08', 'HOUR_09', 'HOUR_10', 'results_by_hour.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n사용 예:\\nbundle = load_best_bundle(5)\\nsample = X_all.iloc[[0]].copy()          # 컬럼 구조 동일해야 함\\nyhat = predict_with_bundle(bundle, sample)\\nprint(yhat)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# XGB 사용 가능 여부 체크\n",
    "# =============================\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "HANSIK_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "DELIV_PATH  = \"data/수원시 배달 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model\"           \n",
    "MAX_HOURS = list(range(1, 11))    # 1~10\n",
    "TEST_RATIO = 0.2                  # 마지막 20% 테스트\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 원핫 폭발 방지(가능한 sklearn 버전에서만 적용)\n",
    "OHE_MIN_FREQUENCY = 20\n",
    "OHE_MAX_CATEGORIES = 200\n",
    "\n",
    "# UNIT lag/rolling\n",
    "LAGS = [1, 7, 14]\n",
    "ROLL_WINDOWS = [7, 14]\n",
    "\n",
    "# XGB 파라미터(너무 느리면 max_depth/early stopping 줄이세요)\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=5000,          # 크게 주고 early stopping으로 컷\n",
    "    learning_rate=0.03,\n",
    "    max_depth=8,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    objective=\"reg:absoluteerror\",  # MAE 직접 최적화(지원 버전 필요)\n",
    "    tree_method=\"hist\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "ET_PARAMS = dict(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# 누수 위험(예측 시점에 모르는 값) 기본 제외\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate_reg(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return float(mae), float(rmse), float(r2)\n",
    "\n",
    "\n",
    "def safe_onehot_encoder():\n",
    "    \"\"\"\n",
    "    sklearn 버전에 따라 min_frequency / max_categories 미지원일 수 있어 예외 처리.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES\n",
    "        )\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "def build_preprocess(X: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    X의 dtype 기반으로 ColumnTransformer 생성\n",
    "    \"\"\"\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", safe_onehot_encoder())\n",
    "    ])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return preprocess\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1) Load\n",
    "# =============================\n",
    "hansik = pd.read_csv(HANSIK_PATH)\n",
    "deliv  = pd.read_csv(DELIV_PATH)\n",
    "\n",
    "required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"UNIT\"}\n",
    "missing = required - set(hansik.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"[한식 데이터] 필수 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "missing2 = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\"} - set(deliv.columns)\n",
    "if missing2:\n",
    "    raise ValueError(f\"[배달 데이터] 필수 컬럼이 없습니다: {missing2}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 2) Merge DELIV as features\n",
    "# =============================\n",
    "merge_keys = [\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\"]\n",
    "\n",
    "deliv_feat = deliv.copy()\n",
    "for c in deliv_feat.columns:\n",
    "    if c not in merge_keys:\n",
    "        deliv_feat.rename(columns={c: f\"DELIV_{c}\"}, inplace=True)\n",
    "\n",
    "df = hansik.merge(deliv_feat, on=merge_keys, how=\"left\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3) Date features\n",
    "# =============================\n",
    "df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df = df[df[\"DATE\"].notna()].copy()\n",
    "\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"DAY_OF_MONTH\"] = df[\"DATE\"].dt.day\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 4) Lag/Rolling features for UNIT (no leakage if time-split)\n",
    "#    group key: (DONG, HOUR)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"]).reset_index(drop=True)\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"], sort=False)\n",
    "\n",
    "for lag in LAGS:\n",
    "    df[f\"UNIT_LAG_{lag}\"] = grp[\"UNIT\"].shift(lag)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"UNIT_ROLL_MEAN_{w}\"] = grp[\"UNIT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).mean()\n",
    "    df[f\"UNIT_ROLL_STD_{w}\"]  = grp[\"UNIT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).std()\n",
    "\n",
    "df[\"UNIT_LAG_1_MISSING\"] = df[\"UNIT_LAG_1\"].isna().astype(int)\n",
    "\n",
    "# 타겟\n",
    "y_all = df[\"UNIT\"].copy()\n",
    "\n",
    "# 피처\n",
    "X_all = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5) Train per HOUR\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "hours = sorted(pd.unique(df[\"HOUR\"].dropna()))\n",
    "hours = [int(h) for h in hours if int(h) in MAX_HOURS]\n",
    "\n",
    "print(f\"[INFO] XGB 사용 여부: {HAS_XGB}\")\n",
    "print(f\"[INFO] 학습 시간대: {hours}\")\n",
    "print(f\"[INFO] 저장 폴더: {SAVE_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "all_results = []\n",
    "t0 = time.time()\n",
    "\n",
    "print(\"\\n[DEBUG] >>> 학습 루프 시작 직전\")\n",
    "for h in hours:\n",
    "    idx = (df[\"HOUR\"].astype(int) == h)\n",
    "    X_h = X_all.loc[idx].copy()\n",
    "    y_h = y_all.loc[idx].copy()\n",
    "    d_h = df.loc[idx, \"DATE\"].copy()\n",
    "\n",
    "    if len(X_h) < 800:\n",
    "        print(f\"\\n[SKIP] HOUR={h:02d}: 데이터가 너무 적음 (n={len(X_h)})\")\n",
    "        continue\n",
    "\n",
    "    # 날짜 정렬\n",
    "    order = np.argsort(d_h.values)\n",
    "    X_h = X_h.iloc[order].reset_index(drop=True)\n",
    "    y_h = y_h.iloc[order].reset_index(drop=True)\n",
    "\n",
    "    n = len(X_h)\n",
    "    split = int(n * (1 - TEST_RATIO))\n",
    "    X_train, X_test = X_h.iloc[:split], X_h.iloc[split:]\n",
    "    y_train, y_test = y_h.iloc[:split], y_h.iloc[split:]\n",
    "\n",
    "    # XGB early stopping용 valid (train의 마지막 10%)\n",
    "    split2 = int(len(X_train) * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2], X_train.iloc[split2:]\n",
    "    y_tr, y_val = y_train.iloc[:split2], y_train.iloc[split2:]\n",
    "\n",
    "    print(f\"\\n===== HOUR={h:02d} | train={len(X_train)} test={len(X_test)} =====\")\n",
    "\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{h:02d}\")\n",
    "    os.makedirs(hour_dir, exist_ok=True)\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # -------------------------\n",
    "    # A방식: preprocess를 먼저 fit/transform 후 모델 학습\n",
    "    # -------------------------\n",
    "    preprocess = build_preprocess(X_train)\n",
    "    pre = clone(preprocess)\n",
    "\n",
    "    X_tr_enc  = pre.fit_transform(X_tr)\n",
    "    X_val_enc = pre.transform(X_val)\n",
    "    X_test_enc = pre.transform(X_test)\n",
    "\n",
    "    # --- 1) XGB (있으면 가장 강력) ---\n",
    "    if HAS_XGB:\n",
    "        # objective가 버전에 따라 에러날 수 있어 fallback 처리\n",
    "        xgb_params = dict(XGB_PARAMS)\n",
    "        try:\n",
    "            model_xgb = XGBRegressor(**xgb_params)\n",
    "            model_xgb.fit(\n",
    "                X_tr_enc, y_tr,\n",
    "                eval_set=[(X_val_enc, y_val)],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # fallback: objective를 reg:squarederror로\n",
    "            xgb_params[\"objective\"] = \"reg:squarederror\"\n",
    "            model_xgb = XGBRegressor(**xgb_params)\n",
    "            model_xgb.fit(\n",
    "                X_tr_enc, y_tr,\n",
    "                eval_set=[(X_val_enc, y_val)],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=200\n",
    "            )\n",
    "\n",
    "        pred = model_xgb.predict(X_test_enc)\n",
    "        mae, rmse, r2 = evaluate_reg(y_test, pred)\n",
    "        print(f\"- XGB       | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "\n",
    "        bundle = {\"preprocess\": pre, \"model\": model_xgb, \"model_name\": \"XGB\"}\n",
    "        candidates.append((\"XGB\", bundle, mae, rmse, r2))\n",
    "\n",
    "    # --- 2) ExtraTrees (빠른/안정 대안) ---\n",
    "    model_et = ExtraTreesRegressor(**ET_PARAMS)\n",
    "    model_et.fit(X_tr_enc, y_tr)  # valid는 사용 안 함(빠름)\n",
    "\n",
    "    pred = model_et.predict(X_test_enc)\n",
    "    mae, rmse, r2 = evaluate_reg(y_test, pred)\n",
    "    print(f\"- EXTRATREE | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "\n",
    "    bundle = {\"preprocess\": pre, \"model\": model_et, \"model_name\": \"EXTRATREE\"}\n",
    "    candidates.append((\"EXTRATREE\", bundle, mae, rmse, r2))\n",
    "\n",
    "    # --- pick best by MAE ---\n",
    "    candidates.sort(key=lambda x: x[2])\n",
    "    best_name, best_bundle, best_mae, best_rmse, best_r2 = candidates[0]\n",
    "\n",
    "    best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    joblib.dump(best_bundle, best_path)\n",
    "\n",
    "    meta = {\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": best_name,\n",
    "        \"MAE\": float(best_mae),\n",
    "        \"RMSE\": float(best_rmse),\n",
    "        \"R2\": float(best_r2),\n",
    "        \"TRAIN_SIZE\": int(len(X_train)),\n",
    "        \"TEST_SIZE\": int(len(X_test)),\n",
    "        \"FEATURE_COLS\": list(X_all.columns),\n",
    "        \"DROP_COLS\": DROP_COLS,\n",
    "        \"LAGS\": LAGS,\n",
    "        \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "        \"HAS_XGB\": bool(HAS_XGB),\n",
    "        \"XGB_PARAMS\": XGB_PARAMS if HAS_XGB else None,\n",
    "        \"ET_PARAMS\": ET_PARAMS,\n",
    "    }\n",
    "    with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ BEST => {best_name} | MAE={best_mae:,.3f} 저장: {best_path}\")\n",
    "\n",
    "    all_results.append({\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": best_name,\n",
    "        \"MAE\": float(best_mae),\n",
    "        \"RMSE\": float(best_rmse),\n",
    "        \"R2\": float(best_r2),\n",
    "        \"SAVE_PATH\": best_path\n",
    "    })\n",
    "\n",
    "# 결과 저장\n",
    "res_df = pd.DataFrame(all_results).sort_values([\"HOUR\"])\n",
    "res_path = os.path.join(SAVE_DIR, \"results_by_hour.csv\")\n",
    "res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[DONE] 학습 완료\")\n",
    "print(f\"- 결과표 저장: {res_path}\")\n",
    "print(f\"- 모델 저장 폴더: {SAVE_DIR}\")\n",
    "print(f\"- 총 소요(초): {time.time() - t0:,.1f}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 6) 로드/예측 헬퍼\n",
    "# =============================\n",
    "def load_best_bundle(hour: int):\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{hour:02d}\")\n",
    "    path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "\"\"\"\n",
    "사용 예:\n",
    "bundle = load_best_bundle(5)\n",
    "sample = X_all.iloc[[0]].copy()          # 컬럼 구조 동일해야 함\n",
    "yhat = predict_with_bundle(bundle, sample)\n",
    "print(yhat)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bdea1",
   "metadata": {},
   "source": [
    "## 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41dc0469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TA_YMD  DAY  HOUR BEST_MODEL  TEMP  RAIN  PRED_UNIT  KMA_BASE_USED\n",
      "0  20251230    2     1        XGB -0.86   0.0      38545  20251230-1100\n",
      "1  20251230    2     2        XGB -0.86   0.0      11827  20251230-1100\n",
      "2  20251230    2     3        XGB -0.86   0.0      16934  20251230-1100\n",
      "3  20251230    2     4  EXTRATREE  1.00   0.0      25555  20251230-1100\n",
      "4  20251230    2     5  EXTRATREE  1.50   0.0      24160  20251230-1100\n",
      "5  20251230    2     6        XGB  1.00   0.0      23881  20251230-1100\n",
      "6  20251230    2     7  EXTRATREE -0.50   0.0      33109  20251230-1100\n",
      "7  20251230    2     8  EXTRATREE -2.00   0.0      38352  20251230-1100\n",
      "8  20251230    2     9        XGB -3.00   0.0      51828  20251230-1100\n",
      "9  20251230    2    10        XGB -4.00   0.0      51965  20251230-1100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "SERVICE_KEY = os.getenv(\"RAIN_ID\")  # 기상청 단기예보 서비스키\n",
    "\n",
    "MODELS_ROOT = \"data/model\"\n",
    "BEST_MAP_PATH = os.path.join(MODELS_ROOT, \"results_by_hour.csv\")  \n",
    "BUNDLE_PATH_TEMPLATE = os.path.join(MODELS_ROOT, \"HOUR_{hour:02d}\", \"best_model.joblib\")  \n",
    "\n",
    "# 학습 때 로그타겟이면 True\n",
    "USE_LOG_TARGET = False\n",
    "\n",
    "# (학습 때와 동일해야 함) 안전장치로 drop\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "# =========================\n",
    "# 항목요약 시간대(01~10)\n",
    "# =========================\n",
    "HOUR_BINS = {\n",
    "    1: list(range(0, 7)),     \n",
    "    2: [7, 8],\n",
    "    3: [9, 10],\n",
    "    4: [11, 12],\n",
    "    5: [13, 14],\n",
    "    6: [15, 16],\n",
    "    7: [17, 18],\n",
    "    8: [19, 20],\n",
    "    9: [21, 22],\n",
    "    10: [23],\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 날짜 파생 + DAY(01=월..07=일)\n",
    "# =========================\n",
    "def compute_day_code(date_yyyymmdd: str) -> int:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return int(dt.weekday() + 1)\n",
    "\n",
    "def make_date_features(date_yyyymmdd: str) -> dict:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return {\n",
    "        \"TA_YMD\": str(date_yyyymmdd),\n",
    "        \"YEAR\": int(dt.year),\n",
    "        \"MONTH\": int(dt.month),\n",
    "        \"DAY_OF_MONTH\": int(dt.day),\n",
    "        \"WEEKOFYEAR\": int(dt.isocalendar().week),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 기상청 단기예보 TMP/PCP\n",
    "# =========================\n",
    "def _parse_pcp_to_mm(x):\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s in (\"강수없음\", \"없음\", \"\", \"nan\", \"NaN\"):\n",
    "        return 0.0\n",
    "    if \"미만\" in s:\n",
    "        try:\n",
    "            num = float(s.replace(\"mm\", \"\").replace(\"미만\", \"\").strip())\n",
    "            return max(0.0, num * 0.5)\n",
    "        except:\n",
    "            return 0.0\n",
    "    if \"~\" in s:\n",
    "        try:\n",
    "            a, b = s.replace(\"mm\", \"\").split(\"~\")\n",
    "            return (float(a) + float(b)) / 2.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    try:\n",
    "        return float(s.replace(\"mm\", \"\"))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def fetch_vilage_fcst_json(service_key, base_date, base_time, nx, ny, num_rows=3000):\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_hourly_tmp_pcp_for_date(service_key, target_yyyymmdd, nx, ny):\n",
    "    base_times = [\"2300\",\"2000\",\"1700\",\"1400\",\"1100\",\"0800\",\"0500\",\"0200\"]\n",
    "    now = datetime.now()\n",
    "    today = now.strftime(\"%Y%m%d\")\n",
    "    yesterday = (now - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    last_err = None\n",
    "    for bd in [today, yesterday]:\n",
    "        for bt in base_times:\n",
    "            try:\n",
    "                js = fetch_vilage_fcst_json(service_key, bd, bt, nx, ny)\n",
    "                items = js[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "                df = pd.DataFrame(items)\n",
    "\n",
    "                df = df[df[\"category\"].isin([\"TMP\", \"PCP\"])]\n",
    "                df = df[df[\"fcstDate\"].astype(str) == str(target_yyyymmdd)]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                df[\"fcstTime\"] = df[\"fcstTime\"].astype(str).str.zfill(4)\n",
    "                piv = df.pivot_table(\n",
    "                    index=\"fcstTime\",\n",
    "                    columns=\"category\",\n",
    "                    values=\"fcstValue\",\n",
    "                    aggfunc=\"first\"\n",
    "                ).reset_index()\n",
    "\n",
    "                if \"TMP\" not in piv.columns:\n",
    "                    continue\n",
    "\n",
    "                piv[\"TMP\"] = piv[\"TMP\"].astype(float)\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].apply(_parse_pcp_to_mm) if \"PCP\" in piv.columns else 0.0\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].astype(float)\n",
    "                piv[\"base_used\"] = f\"{bd}-{bt}\"\n",
    "                return piv.sort_values(\"fcstTime\").reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(f\"해당 날짜의 기상청 예보 데이터를 찾지 못했습니다. last_err={last_err}\")\n",
    "\n",
    "def aggregate_weather_to_item_slots(hourly_df: pd.DataFrame) -> dict:\n",
    "    hourly_df = hourly_df.copy()\n",
    "    hourly_df[\"HOUR_OF_DAY\"] = hourly_df[\"fcstTime\"].str[:2].astype(int)\n",
    "\n",
    "    out = {}\n",
    "    for h, hours in HOUR_BINS.items():\n",
    "        sub = hourly_df[hourly_df[\"HOUR_OF_DAY\"].isin(hours)]\n",
    "        out[h] = {\n",
    "            \"TEMP\": float(sub[\"TMP\"].mean()) if not sub.empty else np.nan,\n",
    "            \"RAIN\": float(sub[\"PCP\"].sum()) if not sub.empty else 0.0,\n",
    "        }\n",
    "\n",
    "    temps = [v[\"TEMP\"] for v in out.values() if not pd.isna(v[\"TEMP\"])]\n",
    "    fill_temp = float(np.mean(temps)) if temps else 0.0\n",
    "    for h in out:\n",
    "        if pd.isna(out[h][\"TEMP\"]):\n",
    "            out[h][\"TEMP\"] = fill_temp\n",
    "\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# 모델 로드 + \"모델이 기대하는 컬럼\" 자동 맞춤\n",
    "# (학습 저장물: best_model.joblib = {\"preprocess\": ColumnTransformer, \"model\": estimator, \"model_name\": str})\n",
    "# =========================\n",
    "def load_best_model_map():\n",
    "    if not os.path.exists(BEST_MAP_PATH):\n",
    "        raise FileNotFoundError(f\"결과 파일이 없습니다: {BEST_MAP_PATH}\")\n",
    "    df = pd.read_csv(BEST_MAP_PATH)\n",
    "    required_cols = {\"HOUR\", \"BEST_MODEL\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"{BEST_MAP_PATH}에 필요한 컬럼이 없습니다. 필요: {required_cols}, 현재: {set(df.columns)}\")\n",
    "    return df.set_index(\"HOUR\")[\"BEST_MODEL\"].to_dict()\n",
    "\n",
    "def load_best_bundle(hour: int):\n",
    "    path = BUNDLE_PATH_TEMPLATE.format(hour=hour)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"번들 파일이 없습니다: {path}\")\n",
    "    bundle = joblib.load(path)\n",
    "    # 최소 검증\n",
    "    if not isinstance(bundle, dict) or \"preprocess\" not in bundle or \"model\" not in bundle:\n",
    "        raise ValueError(f\"번들 포맷이 예상과 다릅니다: {path}\")\n",
    "    return bundle\n",
    "\n",
    "def get_required_feature_names_from_preprocess(preprocess) -> list:\n",
    "    \"\"\"\n",
    "    ColumnTransformer가 기대하는 원본 feature명 추출\n",
    "    (fit 이후 preprocess.transformers_에 cols가 들어있음)\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    for name, trans, cols in getattr(preprocess, \"transformers_\", []):\n",
    "        if cols is None:\n",
    "            continue\n",
    "        if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "            names.extend(list(cols))\n",
    "    return sorted(set([str(x) for x in names]))\n",
    "\n",
    "def align_row_to_required_columns(X_row: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    required_cols에 없는 컬럼은 제거, 없는 컬럼은 생성(NaN).\n",
    "    imputer가 있으니 NaN 생성이 안전.\n",
    "    \"\"\"\n",
    "    X = X_row.copy()\n",
    "\n",
    "    if required_cols:\n",
    "        # 제거\n",
    "        drop_cols = [c for c in X.columns if c not in required_cols]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        # 생성\n",
    "        for c in required_cols:\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "\n",
    "        # 순서 고정\n",
    "        X = X[required_cols]\n",
    "\n",
    "    return X\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "# =========================\n",
    "# 핵심: 날짜 1개 → 1~10시간대 예측\n",
    "# - 기상청 TEMP/RAIN을 항목요약 시간대로 집계해서 넣음\n",
    "# - 시간대별 \"best_model.joblib\" 자동 로드\n",
    "# - preprocess가 기대하는 컬럼 자동 보정\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather(\n",
    "    date_yyyymmdd: str,\n",
    "    nx: int,\n",
    "    ny: int,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)가 비어있습니다. .env 또는 환경변수를 확인하세요.\")\n",
    "\n",
    "    day_code = compute_day_code(date_yyyymmdd)\n",
    "\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    best_map = load_best_model_map()\n",
    "    rows = []\n",
    "\n",
    "    for hour in range(1, 11):\n",
    "        if hour not in best_map:\n",
    "            continue\n",
    "\n",
    "        # 학습 코드에서는 best_model.joblib 한 개만 저장 (BEST_MODEL 명은 meta용)\n",
    "        bundle = load_best_bundle(hour)\n",
    "        best_name = str(best_map[hour])\n",
    "\n",
    "        # 기본 row: 날짜/시간대/요일 + 날씨\n",
    "        row = {}\n",
    "        row.update(make_date_features(date_yyyymmdd))\n",
    "        row.update({\n",
    "            \"HOUR\": int(hour),\n",
    "            \"DAY\": int(day_code),\n",
    "\n",
    "            # (학습 데이터에 존재한다면 자동 사용될 수 있게)\n",
    "            \"TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "\n",
    "            # merge된 배달 피처명을 대비해서도 넣어둠(학습에서 DELIV_*가 있었음)\n",
    "            \"DELIV_TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"DELIV_RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "        })\n",
    "\n",
    "        X_row = pd.DataFrame([row])\n",
    "        X_row = X_row.drop(columns=[c for c in DROP_COLS if c in X_row.columns], errors=\"ignore\")\n",
    "\n",
    "        # preprocess가 기대하는 원본 컬럼으로 맞춤\n",
    "        required = get_required_feature_names_from_preprocess(bundle[\"preprocess\"])\n",
    "        X_row_aligned = align_row_to_required_columns(X_row, required)\n",
    "\n",
    "        pred = float(predict_with_bundle(bundle, X_row_aligned)[0])\n",
    "        if USE_LOG_TARGET:\n",
    "            pred = float(np.expm1(pred))\n",
    "\n",
    "        rows.append({\n",
    "            \"TA_YMD\": str(date_yyyymmdd),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"HOUR\": int(hour),\n",
    "            \"BEST_MODEL\": best_name,             \n",
    "            \"TEMP\": round(float(row[\"TEMP\"]), 2),\n",
    "            \"RAIN\": float(row[\"RAIN\"]),\n",
    "            \"PRED_UNIT\": int(round(pred)),\n",
    "            \"KMA_BASE_USED\": str(hourly_weather.loc[0, \"base_used\"]),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# 사용 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df_pred = predict_date_all_hours_with_weather(\n",
    "        date_yyyymmdd=\"20251230\",\n",
    "        nx=61,\n",
    "        ny=121\n",
    "    )\n",
    "    print(df_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816a827",
   "metadata": {},
   "source": [
    "## 동입력후 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dcacc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMP(slot_hour=5): -4.0 °C\n",
      "     TA_YMD  DONG  DAY  HOUR BEST_MODEL  TEMP  RAIN  PRED_UNIT  KMA_BASE_USED\n",
      "0  20251231  정자2동    3     1        XGB -5.43   0.0      38545  20251230-1100\n",
      "1  20251231  정자2동    3     2        XGB -8.00   0.0      11827  20251230-1100\n",
      "2  20251231  정자2동    3     3        XGB -7.00   0.0      16934  20251230-1100\n",
      "3  20251231  정자2동    3     4  EXTRATREE -5.50   0.0      25851  20251230-1100\n",
      "4  20251231  정자2동    3     5  EXTRATREE -4.00   0.0      24799  20251230-1100\n",
      "5  20251231  정자2동    3     6        XGB -4.00   0.0      20502  20251230-1100\n",
      "6  20251231  정자2동    3     7  EXTRATREE -5.50   0.0      32290  20251230-1100\n",
      "7  20251231  정자2동    3     8  EXTRATREE -6.50   0.0      38535  20251230-1100\n",
      "8  20251231  정자2동    3     9        XGB -8.00   0.0      34702  20251230-1100\n",
      "9  20251231  정자2동    3    10        XGB -9.00   0.0      51965  20251230-1100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "SERVICE_KEY = os.getenv(\"RAIN_ID\")  # 기상청 단기예보 서비스키\n",
    "\n",
    "MODELS_ROOT = \"data/model\"\n",
    "BEST_MAP_PATH = os.path.join(MODELS_ROOT, \"results_by_hour.csv\")\n",
    "BUNDLE_PATH_TEMPLATE = os.path.join(MODELS_ROOT, \"HOUR_{hour:02d}\", \"best_model.joblib\")\n",
    "\n",
    "# 학습 때 로그타겟이면 True\n",
    "USE_LOG_TARGET = False\n",
    "\n",
    "# (학습 때와 동일해야 함) 안전장치로 drop\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "# 격자 엑셀\n",
    "GRID_XLSX_PATH = \"data/수원시 격자.xlsx\"\n",
    "_GRID_CACHE = None  # 반복 로딩 방지 캐시\n",
    "\n",
    "# =========================\n",
    "# 항목요약 시간대(01~10)\n",
    "# =========================\n",
    "HOUR_BINS = {\n",
    "    1: list(range(0, 7)),     # 00~06\n",
    "    2: [7, 8],\n",
    "    3: [9, 10],\n",
    "    4: [11, 12],\n",
    "    5: [13, 14],\n",
    "    6: [15, 16],\n",
    "    7: [17, 18],\n",
    "    8: [19, 20],\n",
    "    9: [21, 22],\n",
    "    10: [23],\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 날짜 파생 + DAY(01=월..07=일)\n",
    "# =========================\n",
    "def compute_day_code(date_yyyymmdd: str) -> int:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return int(dt.weekday() + 1)\n",
    "\n",
    "def make_date_features(date_yyyymmdd: str) -> dict:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return {\n",
    "        \"TA_YMD\": str(date_yyyymmdd),\n",
    "        \"YEAR\": int(dt.year),\n",
    "        \"MONTH\": int(dt.month),\n",
    "        \"DAY_OF_MONTH\": int(dt.day),\n",
    "        \"WEEKOFYEAR\": int(dt.isocalendar().week),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 격자: DONG -> (nx, ny)\n",
    "# =========================\n",
    "def load_grid_table(path: str = GRID_XLSX_PATH) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    data/수원시 격자.xlsx\n",
    "    - A열: 동이름, B열: nx, C열: ny (사용자 설명 기준)\n",
    "    - header가 없다고 가정하고 A/B/C를 고정으로 읽음\n",
    "      (만약 실제 파일에 헤더가 있으면 header=None -> header=0 으로 변경)\n",
    "    \"\"\"\n",
    "    global _GRID_CACHE\n",
    "    if _GRID_CACHE is not None:\n",
    "        return _GRID_CACHE\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"격자 파일이 없습니다: {path}\")\n",
    "\n",
    "    df = pd.read_excel(path, header=None)\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(f\"격자 파일 컬럼 수가 부족합니다(A,B,C 필요): {path}\")\n",
    "\n",
    "    df = df.iloc[:, :3].copy()\n",
    "    df.columns = [\"DONG_NAME\", \"nx\", \"ny\"]\n",
    "\n",
    "    df[\"DONG_NAME\"] = df[\"DONG_NAME\"].astype(str).str.strip()\n",
    "    df[\"nx\"] = pd.to_numeric(df[\"nx\"], errors=\"coerce\")\n",
    "    df[\"ny\"] = pd.to_numeric(df[\"ny\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"nx\", \"ny\"]).copy()\n",
    "    df[\"nx\"] = df[\"nx\"].astype(int)\n",
    "    df[\"ny\"] = df[\"ny\"].astype(int)\n",
    "\n",
    "    _GRID_CACHE = df\n",
    "    return df\n",
    "\n",
    "def find_grid_by_dong(dong: str, path: str = GRID_XLSX_PATH) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    사용자가 입력한 dong(예: '행궁동', '수원시 팔달구 행궁동')으로\n",
    "    엑셀 A열(DONG_NAME)에서 격자(nx,ny) 찾기.\n",
    "    - 완전일치 우선, 없으면 포함검색(양방향)\n",
    "    - 후보가 여러 개면 DONG_NAME 길이가 짧은 것을 우선 선택\n",
    "    \"\"\"\n",
    "    dong = str(dong).strip()\n",
    "    if not dong:\n",
    "        raise ValueError(\"dong(동) 입력이 비었습니다.\")\n",
    "\n",
    "    grid = load_grid_table(path)\n",
    "\n",
    "    # 1) 완전일치\n",
    "    exact = grid[grid[\"DONG_NAME\"] == dong]\n",
    "    if not exact.empty:\n",
    "        r = exact.iloc[0]\n",
    "        return int(r[\"nx\"]), int(r[\"ny\"])\n",
    "\n",
    "    # 2) 포함검색(양방향)\n",
    "    a = grid[\"DONG_NAME\"].str.contains(dong, na=False)\n",
    "    b = grid[\"DONG_NAME\"].apply(lambda x: str(dong).find(str(x)) >= 0)\n",
    "    cand = grid[a | b].copy()\n",
    "\n",
    "    # fallback: 마지막 토큰(보통 'OO동')로 재시도\n",
    "    if cand.empty:\n",
    "        tok = dong.split()[-1]\n",
    "        cand = grid[grid[\"DONG_NAME\"].str.contains(tok, na=False)].copy()\n",
    "\n",
    "    if cand.empty:\n",
    "        raise ValueError(f\"'{dong}'에 해당하는 격자(nx,ny)를 찾지 못했습니다. 엑셀 A열 동이름을 확인하세요.\")\n",
    "\n",
    "    cand[\"name_len\"] = cand[\"DONG_NAME\"].str.len()\n",
    "    cand = cand.sort_values([\"name_len\"]).reset_index(drop=True)\n",
    "    r = cand.iloc[0]\n",
    "    return int(r[\"nx\"]), int(r[\"ny\"])\n",
    "\n",
    "# =========================\n",
    "# 기상청 단기예보 TMP/PCP\n",
    "# =========================\n",
    "def _parse_pcp_to_mm(x):\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s in (\"강수없음\", \"없음\", \"\", \"nan\", \"NaN\"):\n",
    "        return 0.0\n",
    "    if \"미만\" in s:\n",
    "        try:\n",
    "            num = float(s.replace(\"mm\", \"\").replace(\"미만\", \"\").strip())\n",
    "            return max(0.0, num * 0.5)\n",
    "        except:\n",
    "            return 0.0\n",
    "    if \"~\" in s:\n",
    "        try:\n",
    "            a, b = s.replace(\"mm\", \"\").split(\"~\")\n",
    "            return (float(a) + float(b)) / 2.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    try:\n",
    "        return float(s.replace(\"mm\", \"\"))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def fetch_vilage_fcst_json(service_key, base_date, base_time, nx, ny, num_rows=3000):\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_hourly_tmp_pcp_for_date(service_key, target_yyyymmdd, nx, ny):\n",
    "    base_times = [\"2300\", \"2000\", \"1700\", \"1400\", \"1100\", \"0800\", \"0500\", \"0200\"]\n",
    "    now = datetime.now()\n",
    "    today = now.strftime(\"%Y%m%d\")\n",
    "    yesterday = (now - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    last_err = None\n",
    "    for bd in [today, yesterday]:\n",
    "        for bt in base_times:\n",
    "            try:\n",
    "                js = fetch_vilage_fcst_json(service_key, bd, bt, nx, ny)\n",
    "                items = js[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "                df = pd.DataFrame(items)\n",
    "\n",
    "                df = df[df[\"category\"].isin([\"TMP\", \"PCP\"])]\n",
    "                df = df[df[\"fcstDate\"].astype(str) == str(target_yyyymmdd)]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                df[\"fcstTime\"] = df[\"fcstTime\"].astype(str).str.zfill(4)\n",
    "                piv = df.pivot_table(\n",
    "                    index=\"fcstTime\",\n",
    "                    columns=\"category\",\n",
    "                    values=\"fcstValue\",\n",
    "                    aggfunc=\"first\"\n",
    "                ).reset_index()\n",
    "\n",
    "                if \"TMP\" not in piv.columns:\n",
    "                    continue\n",
    "\n",
    "                piv[\"TMP\"] = piv[\"TMP\"].astype(float)\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].apply(_parse_pcp_to_mm) if \"PCP\" in piv.columns else 0.0\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].astype(float)\n",
    "                piv[\"base_used\"] = f\"{bd}-{bt}\"\n",
    "                return piv.sort_values(\"fcstTime\").reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(f\"해당 날짜의 기상청 예보 데이터를 찾지 못했습니다. last_err={last_err}\")\n",
    "\n",
    "def aggregate_weather_to_item_slots(hourly_df: pd.DataFrame) -> dict:\n",
    "    hourly_df = hourly_df.copy()\n",
    "    hourly_df[\"HOUR_OF_DAY\"] = hourly_df[\"fcstTime\"].str[:2].astype(int)\n",
    "\n",
    "    out = {}\n",
    "    for h, hours in HOUR_BINS.items():\n",
    "        sub = hourly_df[hourly_df[\"HOUR_OF_DAY\"].isin(hours)]\n",
    "        out[h] = {\n",
    "            \"TEMP\": float(sub[\"TMP\"].mean()) if not sub.empty else np.nan,\n",
    "            \"RAIN\": float(sub[\"PCP\"].sum()) if not sub.empty else 0.0,\n",
    "        }\n",
    "\n",
    "    temps = [v[\"TEMP\"] for v in out.values() if not pd.isna(v[\"TEMP\"])]\n",
    "    fill_temp = float(np.mean(temps)) if temps else 0.0\n",
    "    for h in out:\n",
    "        if pd.isna(out[h][\"TEMP\"]):\n",
    "            out[h][\"TEMP\"] = fill_temp\n",
    "\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# ✅ 동 입력 -> 그 동의 \"기온\" 반환(원하는 함수)\n",
    "# =========================\n",
    "def get_temperature_by_dong(\n",
    "    date_yyyymmdd: str,\n",
    "    dong: str,\n",
    "    slot_hour: int = 5,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    dong 입력 -> 격자(nx,ny) 조회 -> 기상청 예보 -> slot_hour(1~10)의 TEMP 반환\n",
    "    \"\"\"\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)가 비어있습니다. .env 또는 환경변수를 확인하세요.\")\n",
    "    if not (1 <= int(slot_hour) <= 10):\n",
    "        raise ValueError(\"slot_hour는 1~10 이어야 합니다.\")\n",
    "\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    temp = float(weather_by_hour[int(slot_hour)][\"TEMP\"])\n",
    "    return round(temp, 1)  # ✅ 소수점 1자리 반환\n",
    "\n",
    "# =========================\n",
    "# 모델 로드 + \"모델이 기대하는 컬럼\" 자동 맞춤\n",
    "# (학습 저장물: best_model.joblib = {\"preprocess\": ColumnTransformer, \"model\": estimator, \"model_name\": str})\n",
    "# =========================\n",
    "def load_best_model_map():\n",
    "    if not os.path.exists(BEST_MAP_PATH):\n",
    "        raise FileNotFoundError(f\"결과 파일이 없습니다: {BEST_MAP_PATH}\")\n",
    "    df = pd.read_csv(BEST_MAP_PATH)\n",
    "    required_cols = {\"HOUR\", \"BEST_MODEL\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"{BEST_MAP_PATH}에 필요한 컬럼이 없습니다. 필요: {required_cols}, 현재: {set(df.columns)}\")\n",
    "    return df.set_index(\"HOUR\")[\"BEST_MODEL\"].to_dict()\n",
    "\n",
    "def load_best_bundle(hour: int):\n",
    "    path = BUNDLE_PATH_TEMPLATE.format(hour=hour)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"번들 파일이 없습니다: {path}\")\n",
    "    bundle = joblib.load(path)\n",
    "    if not isinstance(bundle, dict) or \"preprocess\" not in bundle or \"model\" not in bundle:\n",
    "        raise ValueError(f\"번들 포맷이 예상과 다릅니다: {path}\")\n",
    "    return bundle\n",
    "\n",
    "def get_required_feature_names_from_preprocess(preprocess) -> list:\n",
    "    names = []\n",
    "    for name, trans, cols in getattr(preprocess, \"transformers_\", []):\n",
    "        if cols is None:\n",
    "            continue\n",
    "        if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "            names.extend(list(cols))\n",
    "    return sorted(set([str(x) for x in names]))\n",
    "\n",
    "def align_row_to_required_columns(X_row: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
    "    X = X_row.copy()\n",
    "\n",
    "    if required_cols:\n",
    "        drop_cols = [c for c in X.columns if c not in required_cols]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        for c in required_cols:\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "\n",
    "        X = X[required_cols]\n",
    "\n",
    "    return X\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "# =========================\n",
    "# 핵심: 날짜 1개 → 1~10시간대 예측\n",
    "# - (nx, ny 직접 입력 버전)\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather(\n",
    "    date_yyyymmdd: str,\n",
    "    nx: int,\n",
    "    ny: int,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)가 비어있습니다. .env 또는 환경변수를 확인하세요.\")\n",
    "\n",
    "    day_code = compute_day_code(date_yyyymmdd)\n",
    "\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    best_map = load_best_model_map()\n",
    "    rows = []\n",
    "\n",
    "    for hour in range(1, 11):\n",
    "        if hour not in best_map:\n",
    "            continue\n",
    "\n",
    "        bundle = load_best_bundle(hour)\n",
    "        best_name = str(best_map[hour])\n",
    "\n",
    "        row = {}\n",
    "        row.update(make_date_features(date_yyyymmdd))\n",
    "        row.update({\n",
    "            \"HOUR\": int(hour),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "            \"DELIV_TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"DELIV_RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "        })\n",
    "\n",
    "        X_row = pd.DataFrame([row])\n",
    "        X_row = X_row.drop(columns=[c for c in DROP_COLS if c in X_row.columns], errors=\"ignore\")\n",
    "\n",
    "        required = get_required_feature_names_from_preprocess(bundle[\"preprocess\"])\n",
    "        X_row_aligned = align_row_to_required_columns(X_row, required)\n",
    "\n",
    "        pred = float(predict_with_bundle(bundle, X_row_aligned)[0])\n",
    "        if USE_LOG_TARGET:\n",
    "            pred = float(np.expm1(pred))\n",
    "\n",
    "        rows.append({\n",
    "            \"TA_YMD\": str(date_yyyymmdd),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"HOUR\": int(hour),\n",
    "            \"BEST_MODEL\": best_name,\n",
    "            \"TEMP\": round(float(row[\"TEMP\"]), 2), \n",
    "            \"RAIN\": float(row[\"RAIN\"]),\n",
    "            \"PRED_UNIT\": int(round(pred)),\n",
    "            \"KMA_BASE_USED\": str(hourly_weather.loc[0, \"base_used\"]),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# ✅ 핵심: 사용자 DONG 입력 버전 (요청한 흐름)\n",
    "# - DONG으로 nx,ny 자동 추출\n",
    "# - 그 nx,ny로 예측 수행\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather_by_dong(\n",
    "    date_yyyymmdd: str,\n",
    "    dong: str,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "    df_pred = predict_date_all_hours_with_weather(\n",
    "        date_yyyymmdd=date_yyyymmdd,\n",
    "        nx=nx,\n",
    "        ny=ny,\n",
    "        service_key=service_key\n",
    "    )\n",
    "    df_pred.insert(1, \"DONG\", str(dong))\n",
    "#     df_pred.insert(2, \"nx\", int(nx))\n",
    "#     df_pred.insert(3, \"ny\", int(ny))\n",
    "    return df_pred\n",
    "\n",
    "# =========================\n",
    "# 사용 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 동의 특정 시간대(항목요약) 기온만 뽑기\n",
    "#     temp = get_temperature_by_dong(\"20251231\", dong=\"정자2동\", slot_hour=5)\n",
    "#     print(\"TEMP(slot_hour=5):\", temp, \"°C\")\n",
    "\n",
    "    # 2) 동 입력으로 1~10 시간대 매출(UNIT) 예측\n",
    "    df_pred = predict_date_all_hours_with_weather_by_dong(\n",
    "        date_yyyymmdd=\"20251231\",\n",
    "        dong=\"정자2동\"\n",
    "    )\n",
    "    print(df_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38cf7af",
   "metadata": {},
   "source": [
    "## 매출 데이터 입력 후 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55d23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28211d39",
   "metadata": {},
   "source": [
    "# 6. 휴게소, 빵집 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# XGB 사용 가능 여부\n",
    "# =============================\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "BASE_PATH   = \"data/수원시 한식 데이터백업.csv\"     \n",
    "DELIV_PATH  = \"data/수원시 배달 데이터백업.csv\"\n",
    "BAKERY_PATH = \"data/수원시 빵집 데이터백업.csv\"\n",
    "REST_PATH   = \"data/수원시 휴게소 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model_amt_fast\"\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "TARGET_COL = \"AMT\"\n",
    "\n",
    "# Lag / Rolling (AMT 기준)\n",
    "LAGS = [1, 7, 14]\n",
    "ROLL_WINDOWS = [7, 14]\n",
    "\n",
    "# OneHot 제한\n",
    "OHE_MIN_FREQUENCY = 20\n",
    "OHE_MAX_CATEGORIES = 200\n",
    "\n",
    "# XGB (속도 최적)\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=1800,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"mae\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "ET_PARAMS = dict(\n",
    "    n_estimators=600,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"AMT\", \"DATE\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate(y, p):\n",
    "    return (\n",
    "        mean_absolute_error(y, p),\n",
    "        mean_squared_error(y, p, squared=False),\n",
    "        r2_score(y, p),\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES,\n",
    "            sparse_output=True\n",
    "        )\n",
    "    except:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "def build_preprocess(X):\n",
    "    cat = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num = [c for c in X.columns if c not in cat]\n",
    "\n",
    "    return ColumnTransformer([\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", safe_ohe())\n",
    "        ]), cat)\n",
    "    ])\n",
    "\n",
    "\n",
    "def prefix(df, name, keys):\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if c not in keys:\n",
    "            out.rename(columns={c: f\"{name}_{c}\"}, inplace=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Load & Merge\n",
    "# =============================\n",
    "merge_keys = [\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\"]\n",
    "\n",
    "base   = pd.read_csv(BASE_PATH)\n",
    "deliv  = prefix(pd.read_csv(DELIV_PATH),  \"DELIV\",  merge_keys)\n",
    "bakery = prefix(pd.read_csv(BAKERY_PATH), \"BAKERY\", merge_keys)\n",
    "rest   = prefix(pd.read_csv(REST_PATH),   \"REST\",   merge_keys)\n",
    "\n",
    "df = (\n",
    "    base\n",
    "    .merge(deliv, on=merge_keys, how=\"left\")\n",
    "    .merge(bakery, on=merge_keys, how=\"left\")\n",
    "    .merge(rest, on=merge_keys, how=\"left\")\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# Date features\n",
    "# =============================\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"].astype(str), format=\"%Y%m%d\")\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# =============================\n",
    "# Lag / Rolling (AMT)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"])\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"])\n",
    "\n",
    "for l in LAGS:\n",
    "    df[f\"AMT_LAG_{l}\"] = grp[TARGET_COL].shift(l)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"AMT_ROLL_MEAN_{w}\"] = grp[TARGET_COL].shift(1).rolling(w, min_periods=3).mean()\n",
    "    df[f\"AMT_ROLL_STD_{w}\"]  = grp[TARGET_COL].shift(1).rolling(w, min_periods=3).std()\n",
    "\n",
    "# =============================\n",
    "# Target / Feature split\n",
    "# =============================\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "\n",
    "# 시간 순 분할\n",
    "order = np.argsort(df[\"DATE\"].values)\n",
    "X, y = X.iloc[order], y.iloc[order]\n",
    "\n",
    "split = int(len(X) * (1 - TEST_RATIO))\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# =============================\n",
    "# Encode\n",
    "# =============================\n",
    "pre = build_preprocess(X_train)\n",
    "X_tr = pre.fit_transform(X_train)\n",
    "X_te = pre.transform(X_test)\n",
    "\n",
    "# =============================\n",
    "# Train\n",
    "# =============================\n",
    "candidates = []\n",
    "\n",
    "if HAS_XGB:\n",
    "    xgb = XGBRegressor(**XGB_PARAMS)\n",
    "    xgb.fit(\n",
    "        X_tr, y_train,\n",
    "        eval_set=[(X_te, y_test)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    p = xgb.predict(X_te)\n",
    "    candidates.append((\"XGB\", xgb, *evaluate(y_test, p)))\n",
    "\n",
    "et = ExtraTreesRegressor(**ET_PARAMS)\n",
    "et.fit(X_tr, y_train)\n",
    "p = et.predict(X_te)\n",
    "candidates.append((\"ET\", et, *evaluate(y_test, p)))\n",
    "\n",
    "best = sorted(candidates, key=lambda x: x[2])[0]\n",
    "\n",
    "# =============================\n",
    "# Save\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\"model\": best[1], \"preprocess\": pre, \"name\": best[0]},\n",
    "    os.path.join(SAVE_DIR, \"best_model_amt.joblib\")\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "[DONE]\n",
    "MODEL = {best[0]}\n",
    "MAE   = {best[2]:,.2f}\n",
    "RMSE  = {best[3]:,.2f}\n",
    "R2    = {best[4]:.4f}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de26376",
   "metadata": {},
   "source": [
    "# 7. 하루매출, 동별 매출을 중요시한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "649a4ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF 사용 여부(HOUR=1 딥러닝): True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model_amt\n",
      "\n",
      "[DEBUG] >>> 학습 루프 시작\n",
      "\n",
      "===== HOUR=01 | train=33935 test=8484 =====\n",
      "- DL(H1)    | MAE=1,422,978.290 RMSE=5,048,622.482 R2=0.3067\n",
      "✅ BEST => DL_MLP_DONG_EMB 저장: data/model_amt\\HOUR_01\\best_model.joblib\n",
      "\n",
      "===== HOUR=02 | train=33998 test=8500 =====\n",
      "- XGB       | MAE=248,367.541 RMSE=953,382.150 R2=0.4135\n",
      "✅ BEST => XGB | MAE=248,367.541 저장: data/model_amt\\HOUR_02\\best_model.joblib\n",
      "\n",
      "===== HOUR=03 | train=35715 test=8929 =====\n",
      "- XGB       | MAE=645,458.140 RMSE=2,432,873.733 R2=0.3735\n",
      "✅ BEST => XGB | MAE=645,458.140 저장: data/model_amt\\HOUR_03\\best_model.joblib\n",
      "\n",
      "===== HOUR=04 | train=36360 test=9091 =====\n",
      "- XGB       | MAE=1,821,287.913 RMSE=5,831,440.797 R2=0.8165\n",
      "✅ BEST => XGB | MAE=1,821,287.913 저장: data/model_amt\\HOUR_04\\best_model.joblib\n",
      "\n",
      "===== HOUR=05 | train=36408 test=9102 =====\n",
      "- XGB       | MAE=2,171,725.638 RMSE=6,673,013.365 R2=0.8493\n",
      "✅ BEST => XGB | MAE=2,171,725.638 저장: data/model_amt\\HOUR_05\\best_model.joblib\n",
      "\n",
      "===== HOUR=06 | train=36400 test=9101 =====\n",
      "- XGB       | MAE=1,698,310.130 RMSE=5,559,996.672 R2=0.8802\n",
      "✅ BEST => XGB | MAE=1,698,310.130 저장: data/model_amt\\HOUR_06\\best_model.joblib\n",
      "\n",
      "===== HOUR=07 | train=36412 test=9104 =====\n",
      "- XGB       | MAE=1,987,626.198 RMSE=7,092,356.954 R2=0.8113\n",
      "✅ BEST => XGB | MAE=1,987,626.198 저장: data/model_amt\\HOUR_07\\best_model.joblib\n",
      "\n",
      "===== HOUR=08 | train=36351 test=9088 =====\n",
      "- XGB       | MAE=2,663,182.395 RMSE=7,076,861.935 R2=0.7882\n",
      "✅ BEST => XGB | MAE=2,663,182.395 저장: data/model_amt\\HOUR_08\\best_model.joblib\n",
      "\n",
      "===== HOUR=09 | train=35988 test=8998 =====\n",
      "- XGB       | MAE=1,461,944.132 RMSE=3,571,435.462 R2=0.7866\n",
      "✅ BEST => XGB | MAE=1,461,944.132 저장: data/model_amt\\HOUR_09\\best_model.joblib\n",
      "\n",
      "===== HOUR=10 | train=33247 test=8312 =====\n",
      "- XGB       | MAE=572,524.849 RMSE=1,386,120.188 R2=0.6984\n",
      "✅ BEST => XGB | MAE=572,524.849 저장: data/model_amt\\HOUR_10\\best_model.joblib\n",
      "\n",
      "[DONE] 학습 완료\n",
      "- 결과표 저장: data/model_amt\\results_by_hour.csv\n",
      "- 모델 저장 폴더: data/model_amt\n",
      "- 총 소요(초): 42.7\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# TensorFlow 사용 가능 여부 체크 (HOUR=1 딥러닝용)\n",
    "# =============================\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    HAS_TF = True\n",
    "except Exception:\n",
    "    HAS_TF = False\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "HANSIK_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model_amt\"\n",
    "MAX_HOURS = list(range(1, 11))    # 1~10\n",
    "TEST_RATIO = 0.2                  # 마지막 20% 테스트(시간순)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "OHE_MIN_FREQUENCY = 20\n",
    "OHE_MAX_CATEGORIES = 200\n",
    "\n",
    "LAGS = [1, 7, 14]\n",
    "ROLL_WINDOWS = [7, 14]\n",
    "\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=4000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=7,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    objective=\"reg:absoluteerror\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "DL_CFG = dict(\n",
    "    epochs=60,\n",
    "    batch_size=1024,\n",
    "    patience=8,\n",
    "    lr=1e-3,\n",
    "    hidden_units=[128, 64],\n",
    "    dropout=0.10\n",
    ")\n",
    "\n",
    "DROP_COLS = [\"DATE\", \"AMT\", \"CNT\", \"UNIT\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate_reg(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return float(mae), float(rmse), float(r2)\n",
    "\n",
    "def safe_onehot_encoder():\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES\n",
    "        )\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "def build_preprocess_xgb(X: pd.DataFrame):\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", safe_onehot_encoder())\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "def build_preprocess_dl_numeric(X: pd.DataFrame):\n",
    "    num_cols = [c for c in X.columns if X[c].dtype != \"object\"]\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    return pipe, num_cols\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if HAS_TF:\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "# =============================\n",
    "# 1) Load\n",
    "# =============================\n",
    "df = pd.read_csv(HANSIK_PATH)\n",
    "\n",
    "required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"AMT\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"[한식 데이터] 필수 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "# =============================\n",
    "# 2) Date features\n",
    "# =============================\n",
    "df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df = df[df[\"DATE\"].notna()].copy()\n",
    "\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"DAY_OF_MONTH\"] = df[\"DATE\"].dt.day\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# =============================\n",
    "# 3) Lag/Rolling features for AMT (no leakage if time-split)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"]).reset_index(drop=True)\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"], sort=False)\n",
    "\n",
    "for lag in LAGS:\n",
    "    df[f\"AMT_LAG_{lag}\"] = grp[\"AMT\"].shift(lag)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"AMT_ROLL_MEAN_{w}\"] = grp[\"AMT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).mean()\n",
    "    df[f\"AMT_ROLL_STD_{w}\"]  = grp[\"AMT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).std()\n",
    "\n",
    "df[\"AMT_LAG_1_MISSING\"] = df[\"AMT_LAG_1\"].isna().astype(int)\n",
    "\n",
    "# =============================\n",
    "# 3-1) DONG 중요도 강화 (에러 수정 버전)\n",
    "# - transform 기반: df 인덱스에 1:1로 안전하게 붙음\n",
    "# - 누수 방지: shift(1) 이후 expanding\n",
    "# =============================\n",
    "def expanding_mean_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).mean()\n",
    "\n",
    "def expanding_median_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).median()\n",
    "\n",
    "df[\"DONGHOUR_AMT_EXP_MEAN\"] = grp[\"AMT\"].transform(lambda s: expanding_mean_shift1(s, min_periods=5))\n",
    "df[\"DONGHOUR_AMT_EXP_MED\"]  = grp[\"AMT\"].transform(lambda s: expanding_median_shift1(s, min_periods=5))\n",
    "\n",
    "grp_d = df.groupby(\"DONG\", sort=False)\n",
    "df[\"DONG_AMT_EXP_MEAN\"] = grp_d[\"AMT\"].transform(lambda s: expanding_mean_shift1(s, min_periods=10))\n",
    "\n",
    "# =============================\n",
    "# Target / Features\n",
    "# =============================\n",
    "y_all = df[\"AMT\"].copy()\n",
    "X_all = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# 4) Train per HOUR\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "hours = sorted(pd.unique(df[\"HOUR\"].dropna()))\n",
    "hours = [int(h) for h in hours if int(h) in MAX_HOURS]\n",
    "\n",
    "print(f\"[INFO] TF 사용 여부(HOUR=1 딥러닝): {HAS_TF}\")\n",
    "print(f\"[INFO] 학습 시간대: {hours}\")\n",
    "print(f\"[INFO] 저장 폴더: {SAVE_DIR}\")\n",
    "\n",
    "all_results = []\n",
    "t0 = time.time()\n",
    "\n",
    "def train_hour1_deep_learning(X_train, y_train, X_test, y_test, hour_dir):\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow가 없어 딥러닝 학습 불가\")\n",
    "\n",
    "    # train/val split\n",
    "    ntr = len(X_train)\n",
    "    split2 = int(ntr * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2].copy(), X_train.iloc[split2:].copy()\n",
    "    y_tr, y_val = y_train.iloc[:split2].copy(), y_train.iloc[split2:].copy()\n",
    "\n",
    "    # DONG integer encoding\n",
    "    dong_train = X_tr[\"DONG\"].astype(str).values\n",
    "    dong_val   = X_val[\"DONG\"].astype(str).values\n",
    "    dong_test  = X_test[\"DONG\"].astype(str).values\n",
    "\n",
    "    uniq = pd.Index(pd.unique(dong_train))\n",
    "    dong2id = {k: i+1 for i, k in enumerate(uniq)}  # 0: OOV\n",
    "\n",
    "    def map_dong(arr):\n",
    "        return np.array([dong2id.get(x, 0) for x in arr], dtype=np.int32)\n",
    "\n",
    "    dong_tr_id = map_dong(dong_train)\n",
    "    dong_val_id = map_dong(dong_val)\n",
    "    dong_test_id = map_dong(dong_test)\n",
    "\n",
    "    # numeric preprocess\n",
    "    X_tr_num = X_tr.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_val_num = X_val.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_test_num = X_test.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "\n",
    "    num_pipe, num_cols = build_preprocess_dl_numeric(X_tr_num)\n",
    "    X_tr_num_s = num_pipe.fit_transform(X_tr_num[num_cols])\n",
    "    X_val_num_s = num_pipe.transform(X_val_num[num_cols])\n",
    "    X_test_num_s = num_pipe.transform(X_test_num[num_cols])\n",
    "\n",
    "    # model\n",
    "    vocab_size = int(len(dong2id) + 1)\n",
    "    emb_dim = int(min(32, max(8, round(np.sqrt(vocab_size)))))\n",
    "\n",
    "    inp_dong = keras.Input(shape=(1,), dtype=\"int32\", name=\"dong_id\")\n",
    "    x_dong = layers.Embedding(input_dim=vocab_size+1, output_dim=emb_dim, name=\"dong_emb\")(inp_dong)\n",
    "    x_dong = layers.Flatten()(x_dong)\n",
    "\n",
    "    inp_num = keras.Input(shape=(X_tr_num_s.shape[1],), dtype=\"float32\", name=\"num\")\n",
    "    x = layers.Concatenate()([x_dong, inp_num])\n",
    "\n",
    "    for u in DL_CFG[\"hidden_units\"]:\n",
    "        x = layers.Dense(u, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DL_CFG[\"dropout\"])(x)\n",
    "\n",
    "    out = layers.Dense(1, name=\"amt\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[inp_dong, inp_num], outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=DL_CFG[\"lr\"]),\n",
    "        loss=\"mae\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    cb = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=DL_CFG[\"patience\"], restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\", factor=0.7, patience=3, min_lr=1e-5),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        {\"dong_id\": dong_tr_id, \"num\": X_tr_num_s},\n",
    "        y_tr.values,\n",
    "        validation_data=({\"dong_id\": dong_val_id, \"num\": X_val_num_s}, y_val.values),\n",
    "        epochs=DL_CFG[\"epochs\"],\n",
    "        batch_size=DL_CFG[\"batch_size\"],\n",
    "        verbose=0,\n",
    "        callbacks=cb\n",
    "    )\n",
    "\n",
    "    pred = model.predict({\"dong_id\": dong_test_id, \"num\": X_test_num_s}, verbose=0).reshape(-1)\n",
    "    mae, rmse, r2 = evaluate_reg(y_test.values, pred)\n",
    "\n",
    "    # save\n",
    "    model_path = os.path.join(hour_dir, \"dl_model.keras\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    pre_path = os.path.join(hour_dir, \"dl_preprocess.joblib\")\n",
    "    joblib.dump({\"dong2id\": dong2id, \"num_pipe\": num_pipe, \"num_cols\": num_cols}, pre_path)\n",
    "\n",
    "    bundle = {\"model_name\": \"DL_MLP_DONG_EMB\", \"model_path\": model_path, \"preprocess_path\": pre_path, \"type\": \"DL\"}\n",
    "    return bundle, mae, rmse, r2\n",
    "\n",
    "def predict_hour1_dl(bundle, X_new: pd.DataFrame):\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow가 없어 딥러닝 예측 불가\")\n",
    "\n",
    "    from tensorflow import keras\n",
    "    model = keras.models.load_model(bundle[\"model_path\"])\n",
    "    pre = joblib.load(bundle[\"preprocess_path\"])\n",
    "    dong2id = pre[\"dong2id\"]\n",
    "    num_pipe = pre[\"num_pipe\"]\n",
    "    num_cols = pre[\"num_cols\"]\n",
    "\n",
    "    dong_arr = X_new[\"DONG\"].astype(str).values\n",
    "    dong_id = np.array([dong2id.get(x, 0) for x in dong_arr], dtype=np.int32)\n",
    "\n",
    "    X_num = X_new.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_num_s = num_pipe.transform(X_num[num_cols])\n",
    "\n",
    "    pred = model.predict({\"dong_id\": dong_id, \"num\": X_num_s}, verbose=0).reshape(-1)\n",
    "    return pred\n",
    "\n",
    "print(\"\\n[DEBUG] >>> 학습 루프 시작\")\n",
    "for h in hours:\n",
    "    idx = (df[\"HOUR\"].astype(int) == h)\n",
    "    X_h = X_all.loc[idx].copy()\n",
    "    y_h = y_all.loc[idx].copy()\n",
    "    d_h = df.loc[idx, \"DATE\"].copy()\n",
    "\n",
    "    if len(X_h) < 800:\n",
    "        print(f\"\\n[SKIP] HOUR={h:02d}: 데이터가 너무 적음 (n={len(X_h)})\")\n",
    "        continue\n",
    "\n",
    "    # 시간순 split\n",
    "    order = np.argsort(d_h.values)\n",
    "    X_h = X_h.iloc[order].reset_index(drop=True)\n",
    "    y_h = y_h.iloc[order].reset_index(drop=True)\n",
    "\n",
    "    n = len(X_h)\n",
    "    split = int(n * (1 - TEST_RATIO))\n",
    "    X_train, X_test = X_h.iloc[:split], X_h.iloc[split:]\n",
    "    y_train, y_test = y_h.iloc[:split], y_h.iloc[split:]\n",
    "\n",
    "    print(f\"\\n===== HOUR={h:02d} | train={len(X_train)} test={len(X_test)} =====\")\n",
    "\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{h:02d}\")\n",
    "    os.makedirs(hour_dir, exist_ok=True)\n",
    "\n",
    "    # HOUR=1: 딥러닝\n",
    "    if h == 1 and HAS_TF:\n",
    "        try:\n",
    "            bundle, mae, rmse, r2 = train_hour1_deep_learning(X_train, y_train, X_test, y_test, hour_dir)\n",
    "            best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "            joblib.dump(bundle, best_path)\n",
    "\n",
    "            meta = {\n",
    "                \"HOUR\": int(h),\n",
    "                \"BEST_MODEL\": bundle[\"model_name\"],\n",
    "                \"TYPE\": bundle[\"type\"],\n",
    "                \"MAE\": float(mae),\n",
    "                \"RMSE\": float(rmse),\n",
    "                \"R2\": float(r2),\n",
    "                \"TRAIN_SIZE\": int(len(X_train)),\n",
    "                \"TEST_SIZE\": int(len(X_test)),\n",
    "                \"FEATURE_COLS\": list(X_all.columns),\n",
    "                \"DROP_COLS\": DROP_COLS,\n",
    "                \"LAGS\": LAGS,\n",
    "                \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "                \"DL_CFG\": DL_CFG,\n",
    "                \"HAS_TF\": bool(HAS_TF),\n",
    "            }\n",
    "            with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"- DL(H1)    | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "            print(f\"✅ BEST => {bundle['model_name']} 저장: {best_path}\")\n",
    "\n",
    "            all_results.append({\"HOUR\": int(h), \"BEST_MODEL\": bundle[\"model_name\"], \"TYPE\": bundle[\"type\"],\n",
    "                                \"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"SAVE_PATH\": best_path})\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ HOUR=1 딥러닝 실패 -> XGB로 대체. error={e}\")\n",
    "\n",
    "    # XGB (2~10 + HOUR=1 fallback)\n",
    "    split2 = int(len(X_train) * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2], X_train.iloc[split2:]\n",
    "    y_tr, y_val = y_train.iloc[:split2], y_train.iloc[split2:]\n",
    "\n",
    "    preprocess = build_preprocess_xgb(X_train)\n",
    "    pre = clone(preprocess)\n",
    "\n",
    "    X_tr_enc = pre.fit_transform(X_tr)\n",
    "    X_val_enc = pre.transform(X_val)\n",
    "    X_test_enc = pre.transform(X_test)\n",
    "\n",
    "    xgb_params = dict(XGB_PARAMS)\n",
    "    try:\n",
    "        model_xgb = XGBRegressor(**xgb_params)\n",
    "        model_xgb.fit(X_tr_enc, y_tr, eval_set=[(X_val_enc, y_val)], verbose=False, early_stopping_rounds=200)\n",
    "    except Exception:\n",
    "        xgb_params[\"objective\"] = \"reg:squarederror\"\n",
    "        model_xgb = XGBRegressor(**xgb_params)\n",
    "        model_xgb.fit(X_tr_enc, y_tr, eval_set=[(X_val_enc, y_val)], verbose=False, early_stopping_rounds=200)\n",
    "\n",
    "    pred = model_xgb.predict(X_test_enc)\n",
    "    mae, rmse, r2 = evaluate_reg(y_test, pred)\n",
    "    print(f\"- XGB       | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "\n",
    "    best_bundle = {\"preprocess\": pre, \"model\": model_xgb, \"model_name\": \"XGB\", \"type\": \"XGB\"}\n",
    "    best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    joblib.dump(best_bundle, best_path)\n",
    "\n",
    "    meta = {\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": \"XGB\",\n",
    "        \"TYPE\": \"XGB\",\n",
    "        \"MAE\": float(mae),\n",
    "        \"RMSE\": float(rmse),\n",
    "        \"R2\": float(r2),\n",
    "        \"TRAIN_SIZE\": int(len(X_train)),\n",
    "        \"TEST_SIZE\": int(len(X_test)),\n",
    "        \"FEATURE_COLS\": list(X_all.columns),\n",
    "        \"DROP_COLS\": DROP_COLS,\n",
    "        \"LAGS\": LAGS,\n",
    "        \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "        \"HAS_TF\": bool(HAS_TF),\n",
    "        \"XGB_PARAMS\": XGB_PARAMS,\n",
    "    }\n",
    "    with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ BEST => XGB | MAE={mae:,.3f} 저장: {best_path}\")\n",
    "\n",
    "    all_results.append({\"HOUR\": int(h), \"BEST_MODEL\": \"XGB\", \"TYPE\": \"XGB\",\n",
    "                        \"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"SAVE_PATH\": best_path})\n",
    "\n",
    "# 결과 저장\n",
    "res_df = pd.DataFrame(all_results).sort_values([\"HOUR\"])\n",
    "res_path = os.path.join(SAVE_DIR, \"results_by_hour.csv\")\n",
    "res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[DONE] 학습 완료\")\n",
    "print(f\"- 결과표 저장: {res_path}\")\n",
    "print(f\"- 모델 저장 폴더: {SAVE_DIR}\")\n",
    "print(f\"- 총 소요(초): {time.time() - t0:,.1f}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5) 로드/예측 헬퍼\n",
    "# =============================\n",
    "def load_best_bundle(hour: int):\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{hour:02d}\")\n",
    "    path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    if bundle.get(\"type\") == \"DL\":\n",
    "        return predict_hour1_dl(bundle, X_new)\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5df54a",
   "metadata": {},
   "source": [
    "## mae값 낮추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b65f2945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF 사용 여부(HOUR=1 딥러닝): True\n",
      "[INFO] log 타겟 사용: True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model_amt\n",
      "\n",
      "[DEBUG] >>> 학습 루프 시작\n",
      "\n",
      "===== HOUR=01 | train=33935 test=8484 =====\n",
      "- DL(H1)    | MAE=953,988.580 RMSE=2,532,482.998 R2=0.8255\n",
      "✅ BEST => DL_MLP_DONG_EMB 저장: data/model_amt\\HOUR_01\\best_model.joblib\n",
      "\n",
      "===== HOUR=02 | train=33998 test=8500 =====\n",
      "- XGB       | MAE=228,565.487 RMSE=926,586.537 R2=0.4460  (best_iter=730)\n",
      "✅ BEST => XGB | MAE=228,565.487 저장: data/model_amt\\HOUR_02\\best_model.joblib\n",
      "\n",
      "===== HOUR=03 | train=35715 test=8929 =====\n",
      "- XGB       | MAE=593,664.265 RMSE=2,432,275.543 R2=0.3738  (best_iter=534)\n",
      "✅ BEST => XGB | MAE=593,664.265 저장: data/model_amt\\HOUR_03\\best_model.joblib\n",
      "\n",
      "===== HOUR=04 | train=36360 test=9091 =====\n",
      "- XGB       | MAE=1,648,562.293 RMSE=5,964,550.638 R2=0.8080  (best_iter=1003)\n",
      "✅ BEST => XGB | MAE=1,648,562.293 저장: data/model_amt\\HOUR_04\\best_model.joblib\n",
      "\n",
      "===== HOUR=05 | train=36408 test=9102 =====\n",
      "- XGB       | MAE=1,796,913.895 RMSE=5,741,578.858 R2=0.8884  (best_iter=653)\n",
      "✅ BEST => XGB | MAE=1,796,913.895 저장: data/model_amt\\HOUR_05\\best_model.joblib\n",
      "\n",
      "===== HOUR=06 | train=36400 test=9101 =====\n",
      "- XGB       | MAE=1,423,164.339 RMSE=5,450,647.446 R2=0.8849  (best_iter=585)\n",
      "✅ BEST => XGB | MAE=1,423,164.339 저장: data/model_amt\\HOUR_06\\best_model.joblib\n",
      "\n",
      "===== HOUR=07 | train=36412 test=9104 =====\n",
      "- XGB       | MAE=1,627,266.986 RMSE=5,842,416.692 R2=0.8719  (best_iter=821)\n",
      "✅ BEST => XGB | MAE=1,627,266.986 저장: data/model_amt\\HOUR_07\\best_model.joblib\n",
      "\n",
      "===== HOUR=08 | train=36351 test=9088 =====\n",
      "- XGB       | MAE=2,022,603.005 RMSE=4,807,413.111 R2=0.9023  (best_iter=1839)\n",
      "✅ BEST => XGB | MAE=2,022,603.005 저장: data/model_amt\\HOUR_08\\best_model.joblib\n",
      "\n",
      "===== HOUR=09 | train=35988 test=8998 =====\n",
      "- XGB       | MAE=1,131,956.371 RMSE=2,719,898.336 R2=0.8762  (best_iter=2293)\n",
      "✅ BEST => XGB | MAE=1,131,956.371 저장: data/model_amt\\HOUR_09\\best_model.joblib\n",
      "\n",
      "===== HOUR=10 | train=33247 test=8312 =====\n",
      "- XGB       | MAE=445,193.784 RMSE=1,253,300.294 R2=0.7535  (best_iter=799)\n",
      "✅ BEST => XGB | MAE=445,193.784 저장: data/model_amt\\HOUR_10\\best_model.joblib\n",
      "\n",
      "[DONE] 학습 완료\n",
      "- 결과표 저장: data/model_amt\\results_by_hour.csv\n",
      "- 모델 저장 폴더: data/model_amt\n",
      "- 총 소요(초): 1,482.2\n",
      "CPU times: total: 3h 1min 42s\n",
      "Wall time: 24min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================\n",
    "# TensorFlow 사용 가능 여부 체크 (HOUR=1 딥러닝용)\n",
    "# =============================\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    HAS_TF = True\n",
    "except Exception:\n",
    "    HAS_TF = False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "HANSIK_PATH = \"data/수원시 한식 데이터백업.csv\"\n",
    "\n",
    "SAVE_DIR = \"data/model_amt\"\n",
    "MAX_HOURS = list(range(1, 11))    # 1~10\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ✅ AMT 꼬리/아웃라이어 많으면 MAE 개선에 도움되는 경우가 많음\n",
    "USE_LOG_TARGET = True            # <-- (추가) log1p 타겟 사용\n",
    "\n",
    "# 원핫 폭발 방지\n",
    "OHE_MIN_FREQUENCY = 10           # <-- (완화) 더 많은 카테고리 반영(동 중요도 ↑)\n",
    "OHE_MAX_CATEGORIES = 500\n",
    "\n",
    "# ✅ 피처 강화(조금만)\n",
    "LAGS = [1, 7, 14, 28]            # <-- (추가)\n",
    "ROLL_WINDOWS = [7, 14, 30, 60]   # <-- (추가)\n",
    "\n",
    "# ✅ 30분 정도 쓸 수 있게 XGB를 강하게(early stopping으로 자동 컷)\n",
    "# - learning_rate ↓, n_estimators ↑, early_stopping ↑\n",
    "# - grow_policy=lossguide + max_leaves로 더 강하게(데이터 크면 효율 좋음)\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=30000,          # <-- 크게\n",
    "    learning_rate=0.01,          # <-- 작게\n",
    "    max_depth=0,                # <-- lossguide에서 depth 대신 leaves로\n",
    "    max_leaves=256,             # <-- (추가) 모델 용량\n",
    "    grow_policy=\"lossguide\",    # <-- (추가)\n",
    "    min_child_weight=1,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_lambda=2.0,\n",
    "    reg_alpha=0.1,\n",
    "    objective=\"reg:squarederror\",   # 안정적으로 + eval_metric=mae로 MAE 최적화\n",
    "    eval_metric=\"mae\",\n",
    "    tree_method=\"hist\",\n",
    "    max_bin=512,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 1500     # <-- (증가) 더 오래 학습\n",
    "\n",
    "# ✅ 딥러닝(HOUR=1)도 조금 더 오래/강하게\n",
    "DL_CFG = dict(\n",
    "    epochs=250,                 # <-- 증가\n",
    "    batch_size=512,             # <-- 조금 작게(성능↑ 가능)\n",
    "    patience=25,                # <-- 증가\n",
    "    lr=5e-4,                    # <-- 조금 낮게\n",
    "    hidden_units=[256, 128, 64],# <-- 증가\n",
    "    dropout=0.15\n",
    ")\n",
    "\n",
    "# 누수 위험 제거\n",
    "DROP_COLS = [\"DATE\", \"AMT\", \"CNT\", \"UNIT\"]\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Utils\n",
    "# =============================\n",
    "def evaluate_reg(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return float(mae), float(rmse), float(r2)\n",
    "\n",
    "def safe_onehot_encoder():\n",
    "    try:\n",
    "        return OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            min_frequency=OHE_MIN_FREQUENCY,\n",
    "            max_categories=OHE_MAX_CATEGORIES\n",
    "        )\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "def build_preprocess_xgb(X: pd.DataFrame):\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", safe_onehot_encoder())\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "def build_preprocess_dl_numeric(X: pd.DataFrame):\n",
    "    num_cols = [c for c in X.columns if X[c].dtype != \"object\"]\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    return pipe, num_cols\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if HAS_TF:\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(RANDOM_STATE)\n",
    "\n",
    "def y_transform(y: pd.Series):\n",
    "    \"\"\"학습용 타겟 변환\"\"\"\n",
    "    if USE_LOG_TARGET:\n",
    "        return np.log1p(np.clip(y.values, 0, None))\n",
    "    return y.values.astype(float)\n",
    "\n",
    "def y_inverse(pred: np.ndarray):\n",
    "    \"\"\"예측값 역변환\"\"\"\n",
    "    if USE_LOG_TARGET:\n",
    "        return np.expm1(pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 1) Load\n",
    "# =============================\n",
    "df = pd.read_csv(HANSIK_PATH)\n",
    "\n",
    "required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"AMT\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"[한식 데이터] 필수 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "# =============================\n",
    "# 2) Date features\n",
    "# =============================\n",
    "df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"TA_YMD\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df = df[df[\"DATE\"].notna()].copy()\n",
    "\n",
    "df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "df[\"DAY_OF_MONTH\"] = df[\"DATE\"].dt.day\n",
    "df[\"WEEKOFYEAR\"] = df[\"DATE\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# =============================\n",
    "# 3) Lag/Rolling features for AMT (no leakage)\n",
    "# =============================\n",
    "df = df.sort_values([\"DONG\", \"HOUR\", \"DATE\"]).reset_index(drop=True)\n",
    "grp = df.groupby([\"DONG\", \"HOUR\"], sort=False)\n",
    "\n",
    "for lag in LAGS:\n",
    "    df[f\"AMT_LAG_{lag}\"] = grp[\"AMT\"].shift(lag)\n",
    "\n",
    "for w in ROLL_WINDOWS:\n",
    "    df[f\"AMT_ROLL_MEAN_{w}\"] = grp[\"AMT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).mean()\n",
    "    df[f\"AMT_ROLL_STD_{w}\"]  = grp[\"AMT\"].shift(1).rolling(window=w, min_periods=max(2, w // 3)).std()\n",
    "\n",
    "df[\"AMT_LAG_1_MISSING\"] = df[\"AMT_LAG_1\"].isna().astype(int)\n",
    "\n",
    "# =============================\n",
    "# 3-1) DONG 중요도 강화(누수 방지, IndexError 방지: transform 사용)\n",
    "# =============================\n",
    "def expanding_mean_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).mean()\n",
    "\n",
    "def expanding_median_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).median()\n",
    "\n",
    "def expanding_std_shift1(s: pd.Series, min_periods: int):\n",
    "    return s.shift(1).expanding(min_periods=min_periods).std()\n",
    "\n",
    "df[\"DONGHOUR_AMT_EXP_MEAN\"] = grp[\"AMT\"].transform(lambda s: expanding_mean_shift1(s, 5))\n",
    "df[\"DONGHOUR_AMT_EXP_MED\"]  = grp[\"AMT\"].transform(lambda s: expanding_median_shift1(s, 5))\n",
    "df[\"DONGHOUR_AMT_EXP_STD\"]  = grp[\"AMT\"].transform(lambda s: expanding_std_shift1(s, 5))  # <-- (추가)\n",
    "\n",
    "grp_d = df.groupby(\"DONG\", sort=False)\n",
    "df[\"DONG_AMT_EXP_MEAN\"] = grp_d[\"AMT\"].transform(lambda s: expanding_mean_shift1(s, 10))\n",
    "\n",
    "# =============================\n",
    "# Target / Features\n",
    "# =============================\n",
    "y_all = df[\"AMT\"].copy()\n",
    "X_all = df.drop(columns=[c for c in DROP_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 4) Train per HOUR\n",
    "# =============================\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "hours = sorted(pd.unique(df[\"HOUR\"].dropna()))\n",
    "hours = [int(h) for h in hours if int(h) in MAX_HOURS]\n",
    "\n",
    "print(f\"[INFO] TF 사용 여부(HOUR=1 딥러닝): {HAS_TF}\")\n",
    "print(f\"[INFO] log 타겟 사용: {USE_LOG_TARGET}\")\n",
    "print(f\"[INFO] 학습 시간대: {hours}\")\n",
    "print(f\"[INFO] 저장 폴더: {SAVE_DIR}\")\n",
    "\n",
    "all_results = []\n",
    "t0 = time.time()\n",
    "\n",
    "def train_hour1_deep_learning(X_train, y_train_raw, X_test, y_test_raw, hour_dir):\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow가 없어 딥러닝 학습 불가\")\n",
    "\n",
    "    # train/val split (시간순)\n",
    "    ntr = len(X_train)\n",
    "    split2 = int(ntr * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2].copy(), X_train.iloc[split2:].copy()\n",
    "    y_tr_raw, y_val_raw = y_train_raw.iloc[:split2].copy(), y_train_raw.iloc[split2:].copy()\n",
    "\n",
    "    # y transform\n",
    "    y_tr = y_transform(y_tr_raw)\n",
    "    y_val = y_transform(y_val_raw)\n",
    "\n",
    "    # DONG integer encoding\n",
    "    dong_train = X_tr[\"DONG\"].astype(str).values\n",
    "    dong_val   = X_val[\"DONG\"].astype(str).values\n",
    "    dong_test  = X_test[\"DONG\"].astype(str).values\n",
    "\n",
    "    uniq = pd.Index(pd.unique(dong_train))\n",
    "    dong2id = {k: i+1 for i, k in enumerate(uniq)}  # 0 OOV\n",
    "\n",
    "    def map_dong(arr):\n",
    "        return np.array([dong2id.get(x, 0) for x in arr], dtype=np.int32)\n",
    "\n",
    "    dong_tr_id  = map_dong(dong_train)\n",
    "    dong_val_id = map_dong(dong_val)\n",
    "    dong_test_id= map_dong(dong_test)\n",
    "\n",
    "    # numeric preprocess\n",
    "    X_tr_num = X_tr.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_val_num = X_val.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_test_num = X_test.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "\n",
    "    num_pipe, num_cols = build_preprocess_dl_numeric(X_tr_num)\n",
    "    X_tr_num_s = num_pipe.fit_transform(X_tr_num[num_cols])\n",
    "    X_val_num_s = num_pipe.transform(X_val_num[num_cols])\n",
    "    X_test_num_s = num_pipe.transform(X_test_num[num_cols])\n",
    "\n",
    "    # model\n",
    "    vocab_size = int(len(dong2id) + 1)\n",
    "    emb_dim = int(min(64, max(12, round(np.sqrt(vocab_size) * 2))))  # <-- 약간 확대\n",
    "\n",
    "    inp_dong = keras.Input(shape=(1,), dtype=\"int32\", name=\"dong_id\")\n",
    "    x_dong = layers.Embedding(input_dim=vocab_size+1, output_dim=emb_dim, name=\"dong_emb\")(inp_dong)\n",
    "    x_dong = layers.Flatten()(x_dong)\n",
    "\n",
    "    inp_num = keras.Input(shape=(X_tr_num_s.shape[1],), dtype=\"float32\", name=\"num\")\n",
    "    x = layers.Concatenate()([x_dong, inp_num])\n",
    "\n",
    "    x = layers.LayerNormalization()(x)  # <-- (추가) 안정성/성능 개선\n",
    "\n",
    "    for u in DL_CFG[\"hidden_units\"]:\n",
    "        x = layers.Dense(u, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DL_CFG[\"dropout\"])(x)\n",
    "\n",
    "    out = layers.Dense(1, name=\"amt\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=[inp_dong, inp_num], outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=DL_CFG[\"lr\"]),\n",
    "        loss=\"mae\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    cb = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=DL_CFG[\"patience\"], restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_mae\", factor=0.7, patience=5, min_lr=1e-5),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        {\"dong_id\": dong_tr_id, \"num\": X_tr_num_s},\n",
    "        y_tr,\n",
    "        validation_data=({\"dong_id\": dong_val_id, \"num\": X_val_num_s}, y_val),\n",
    "        epochs=DL_CFG[\"epochs\"],\n",
    "        batch_size=DL_CFG[\"batch_size\"],\n",
    "        verbose=0,\n",
    "        callbacks=cb\n",
    "    )\n",
    "\n",
    "    # predict + inverse\n",
    "    pred_t = model.predict({\"dong_id\": dong_test_id, \"num\": X_test_num_s}, verbose=0).reshape(-1)\n",
    "    pred = y_inverse(pred_t)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "\n",
    "    mae, rmse, r2 = evaluate_reg(y_test_raw.values, pred)\n",
    "\n",
    "    # save\n",
    "    model_path = os.path.join(hour_dir, \"dl_model.keras\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    pre_path = os.path.join(hour_dir, \"dl_preprocess.joblib\")\n",
    "    joblib.dump({\"dong2id\": dong2id, \"num_pipe\": num_pipe, \"num_cols\": num_cols}, pre_path)\n",
    "\n",
    "    bundle = {\"model_name\": \"DL_MLP_DONG_EMB\", \"model_path\": model_path, \"preprocess_path\": pre_path, \"type\": \"DL\"}\n",
    "    return bundle, mae, rmse, r2\n",
    "\n",
    "def predict_hour1_dl(bundle, X_new: pd.DataFrame):\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow가 없어 딥러닝 예측 불가\")\n",
    "\n",
    "    from tensorflow import keras\n",
    "    model = keras.models.load_model(bundle[\"model_path\"])\n",
    "    pre = joblib.load(bundle[\"preprocess_path\"])\n",
    "    dong2id = pre[\"dong2id\"]\n",
    "    num_pipe = pre[\"num_pipe\"]\n",
    "    num_cols = pre[\"num_cols\"]\n",
    "\n",
    "    dong_arr = X_new[\"DONG\"].astype(str).values\n",
    "    dong_id = np.array([dong2id.get(x, 0) for x in dong_arr], dtype=np.int32)\n",
    "\n",
    "    X_num = X_new.drop(columns=[\"DONG\"], errors=\"ignore\")\n",
    "    X_num_s = num_pipe.transform(X_num[num_cols])\n",
    "\n",
    "    pred_t = model.predict({\"dong_id\": dong_id, \"num\": X_num_s}, verbose=0).reshape(-1)\n",
    "    pred = y_inverse(pred_t)\n",
    "    return np.clip(pred, 0, None)\n",
    "\n",
    "print(\"\\n[DEBUG] >>> 학습 루프 시작\")\n",
    "for h in hours:\n",
    "    idx = (df[\"HOUR\"].astype(int) == h)\n",
    "    X_h = X_all.loc[idx].copy()\n",
    "    y_h_raw = y_all.loc[idx].copy()\n",
    "    d_h = df.loc[idx, \"DATE\"].copy()\n",
    "\n",
    "    if len(X_h) < 800:\n",
    "        print(f\"\\n[SKIP] HOUR={h:02d}: 데이터가 너무 적음 (n={len(X_h)})\")\n",
    "        continue\n",
    "\n",
    "    # 시간순 split\n",
    "    order = np.argsort(d_h.values)\n",
    "    X_h = X_h.iloc[order].reset_index(drop=True)\n",
    "    y_h_raw = y_h_raw.iloc[order].reset_index(drop=True)\n",
    "\n",
    "    n = len(X_h)\n",
    "    split = int(n * (1 - TEST_RATIO))\n",
    "    X_train, X_test = X_h.iloc[:split], X_h.iloc[split:]\n",
    "    y_train_raw, y_test_raw = y_h_raw.iloc[:split], y_h_raw.iloc[split:]\n",
    "\n",
    "    print(f\"\\n===== HOUR={h:02d} | train={len(X_train)} test={len(X_test)} =====\")\n",
    "\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{h:02d}\")\n",
    "    os.makedirs(hour_dir, exist_ok=True)\n",
    "\n",
    "    # HOUR=1: 딥러닝\n",
    "    if h == 1 and HAS_TF:\n",
    "        try:\n",
    "            bundle, mae, rmse, r2 = train_hour1_deep_learning(X_train, y_train_raw, X_test, y_test_raw, hour_dir)\n",
    "            best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "            joblib.dump(bundle, best_path)\n",
    "\n",
    "            meta = {\n",
    "                \"HOUR\": int(h),\n",
    "                \"BEST_MODEL\": bundle[\"model_name\"],\n",
    "                \"TYPE\": bundle[\"type\"],\n",
    "                \"MAE\": float(mae),\n",
    "                \"RMSE\": float(rmse),\n",
    "                \"R2\": float(r2),\n",
    "                \"TRAIN_SIZE\": int(len(X_train)),\n",
    "                \"TEST_SIZE\": int(len(X_test)),\n",
    "                \"FEATURE_COLS\": list(X_all.columns),\n",
    "                \"DROP_COLS\": DROP_COLS,\n",
    "                \"LAGS\": LAGS,\n",
    "                \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "                \"DL_CFG\": DL_CFG,\n",
    "                \"USE_LOG_TARGET\": USE_LOG_TARGET,\n",
    "                \"HAS_TF\": bool(HAS_TF),\n",
    "            }\n",
    "            with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"- DL(H1)    | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}\")\n",
    "            print(f\"✅ BEST => {bundle['model_name']} 저장: {best_path}\")\n",
    "\n",
    "            all_results.append({\"HOUR\": int(h), \"BEST_MODEL\": bundle[\"model_name\"], \"TYPE\": bundle[\"type\"],\n",
    "                                \"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"SAVE_PATH\": best_path})\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ HOUR=1 딥러닝 실패 -> XGB로 대체. error={e}\")\n",
    "\n",
    "    # XGB (2~10 + HOUR=1 fallback)\n",
    "    # train 마지막 10%를 valid로 early stopping\n",
    "    split2 = int(len(X_train) * 0.9)\n",
    "    X_tr, X_val = X_train.iloc[:split2], X_train.iloc[split2:]\n",
    "    y_tr_raw, y_val_raw = y_train_raw.iloc[:split2], y_train_raw.iloc[split2:]\n",
    "\n",
    "    # y transform\n",
    "    y_tr = y_transform(y_tr_raw)\n",
    "    y_val = y_transform(y_val_raw)\n",
    "\n",
    "    preprocess = build_preprocess_xgb(X_train)\n",
    "    pre = clone(preprocess)\n",
    "\n",
    "    X_tr_enc = pre.fit_transform(X_tr)\n",
    "    X_val_enc = pre.transform(X_val)\n",
    "    X_test_enc = pre.transform(X_test)\n",
    "\n",
    "    model_xgb = XGBRegressor(**XGB_PARAMS)\n",
    "    model_xgb.fit(\n",
    "        X_tr_enc, y_tr,\n",
    "        eval_set=[(X_val_enc, y_val)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    "    )\n",
    "\n",
    "    pred_t = model_xgb.predict(X_test_enc)\n",
    "    pred = y_inverse(pred_t)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "\n",
    "    mae, rmse, r2 = evaluate_reg(y_test_raw.values, pred)\n",
    "    print(f\"- XGB       | MAE={mae:,.3f} RMSE={rmse:,.3f} R2={r2:,.4f}  (best_iter={getattr(model_xgb, 'best_iteration', None)})\")\n",
    "\n",
    "    best_bundle = {\"preprocess\": pre, \"model\": model_xgb, \"model_name\": \"XGB\", \"type\": \"XGB\"}\n",
    "    best_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    joblib.dump(best_bundle, best_path)\n",
    "\n",
    "    meta = {\n",
    "        \"HOUR\": int(h),\n",
    "        \"BEST_MODEL\": \"XGB\",\n",
    "        \"TYPE\": \"XGB\",\n",
    "        \"MAE\": float(mae),\n",
    "        \"RMSE\": float(rmse),\n",
    "        \"R2\": float(r2),\n",
    "        \"TRAIN_SIZE\": int(len(X_train)),\n",
    "        \"TEST_SIZE\": int(len(X_test)),\n",
    "        \"FEATURE_COLS\": list(X_all.columns),\n",
    "        \"DROP_COLS\": DROP_COLS,\n",
    "        \"LAGS\": LAGS,\n",
    "        \"ROLL_WINDOWS\": ROLL_WINDOWS,\n",
    "        \"USE_LOG_TARGET\": USE_LOG_TARGET,\n",
    "        \"HAS_TF\": bool(HAS_TF),\n",
    "        \"XGB_PARAMS\": XGB_PARAMS,\n",
    "        \"EARLY_STOPPING_ROUNDS\": EARLY_STOPPING_ROUNDS,\n",
    "    }\n",
    "    with open(os.path.join(hour_dir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ BEST => XGB | MAE={mae:,.3f} 저장: {best_path}\")\n",
    "\n",
    "    all_results.append({\"HOUR\": int(h), \"BEST_MODEL\": \"XGB\", \"TYPE\": \"XGB\",\n",
    "                        \"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2), \"SAVE_PATH\": best_path})\n",
    "\n",
    "# 결과 저장\n",
    "res_df = pd.DataFrame(all_results).sort_values([\"HOUR\"])\n",
    "res_path = os.path.join(SAVE_DIR, \"results_by_hour.csv\")\n",
    "res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[DONE] 학습 완료\")\n",
    "print(f\"- 결과표 저장: {res_path}\")\n",
    "print(f\"- 모델 저장 폴더: {SAVE_DIR}\")\n",
    "print(f\"- 총 소요(초): {time.time() - t0:,.1f}\")\n",
    "\n",
    "# =============================\n",
    "# 5) 로드/예측 헬퍼\n",
    "# =============================\n",
    "def load_best_bundle(hour: int):\n",
    "    hour_dir = os.path.join(SAVE_DIR, f\"HOUR_{hour:02d}\")\n",
    "    path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    if bundle.get(\"type\") == \"DL\":\n",
    "        return predict_hour1_dl(bundle, X_new)\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    pred_t = model.predict(X_enc)\n",
    "    pred = y_inverse(pred_t)\n",
    "    return np.clip(pred, 0, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95797b18",
   "metadata": {},
   "source": [
    "## mae값 낮추고 성능 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b78efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF 사용 여부(HOUR=1 딥러닝): False\n",
      "[INFO] log 타겟 사용: True\n",
      "[INFO] 학습 시간대: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[INFO] 저장 폴더: data/model_amt_xgb\n",
      "\n",
      "[DEBUG] >>> 학습 루프 시작\n",
      "\n",
      "===== HOUR=01 | train=33935 test=8484 =====\n",
      "  - XGB cand#1  | MAE=822,312.078 RMSE=2,196,652.914 R2=0.8687 (best_iter=26)\n",
      "  - XGB cand#2  | MAE=912,250.451 RMSE=2,852,431.266 R2=0.7787 (best_iter=8)\n",
      "  - XGB cand#3  | MAE=905,422.539 RMSE=2,711,424.367 R2=0.8000 (best_iter=6)\n",
      "  - XGB cand#4  | MAE=836,157.679 RMSE=2,120,713.352 R2=0.8777 (best_iter=19)\n",
      "  - XGB cand#5  | MAE=822,312.078 RMSE=2,196,652.914 R2=0.8687 (best_iter=26)\n",
      "  - XGB cand#6  | MAE=909,725.543 RMSE=3,254,827.312 R2=0.7118 (best_iter=26)\n",
      "  - XGB cand#7  | MAE=909,725.543 RMSE=3,254,827.312 R2=0.7118 (best_iter=26)\n",
      "  - XGB cand#8  | MAE=905,422.539 RMSE=2,711,424.367 R2=0.8000 (best_iter=6)\n",
      "✅ BEST => XGB | MAE=822,312.078 저장: data/model_amt_xgb\\HOUR_01\\best_model.joblib\n",
      "\n",
      "===== HOUR=02 | train=33998 test=8500 =====\n",
      "  - XGB cand#1  | MAE=261,688.021 RMSE=972,487.811 R2=0.3899 (best_iter=3)\n",
      "  - XGB cand#2  | MAE=256,874.565 RMSE=982,558.696 R2=0.3772 (best_iter=4)\n",
      "  - XGB cand#3  | MAE=257,266.445 RMSE=962,346.221 R2=0.4025 (best_iter=14)\n",
      "  - XGB cand#4  | MAE=259,534.302 RMSE=966,739.048 R2=0.3971 (best_iter=2)\n",
      "  - XGB cand#5  | MAE=261,530.333 RMSE=984,631.688 R2=0.3745 (best_iter=9)\n",
      "  - XGB cand#6  | MAE=257,266.445 RMSE=962,346.221 R2=0.4025 (best_iter=14)\n",
      "  - XGB cand#7  | MAE=256,874.565 RMSE=982,558.696 R2=0.3772 (best_iter=4)\n",
      "  - XGB cand#8  | MAE=261,688.021 RMSE=972,487.811 R2=0.3899 (best_iter=3)\n",
      "✅ BEST => XGB | MAE=256,874.565 저장: data/model_amt_xgb\\HOUR_02\\best_model.joblib\n",
      "\n",
      "===== HOUR=03 | train=35715 test=8929 =====\n",
      "  - XGB cand#1  | MAE=682,509.947 RMSE=2,504,715.949 R2=0.3359 (best_iter=12)\n",
      "  - XGB cand#2  | MAE=656,765.666 RMSE=2,479,290.522 R2=0.3493 (best_iter=16)\n",
      "  - XGB cand#3  | MAE=639,996.913 RMSE=2,430,288.909 R2=0.3748 (best_iter=37)\n",
      "  - XGB cand#4  | MAE=639,996.913 RMSE=2,430,288.909 R2=0.3748 (best_iter=37)\n",
      "  - XGB cand#5  | MAE=682,509.947 RMSE=2,504,715.949 R2=0.3359 (best_iter=12)\n",
      "  - XGB cand#6  | MAE=674,131.260 RMSE=2,487,065.349 R2=0.3452 (best_iter=13)\n",
      "  - XGB cand#7  | MAE=655,843.323 RMSE=2,472,861.417 R2=0.3527 (best_iter=24)\n",
      "  - XGB cand#8  | MAE=656,765.666 RMSE=2,479,290.522 R2=0.3493 (best_iter=16)\n",
      "✅ BEST => XGB | MAE=639,996.913 저장: data/model_amt_xgb\\HOUR_03\\best_model.joblib\n",
      "\n",
      "===== HOUR=04 | train=36360 test=9091 =====\n",
      "  - XGB cand#1  | MAE=1,896,642.822 RMSE=6,233,682.771 R2=0.7903 (best_iter=16)\n",
      "  - XGB cand#2  | MAE=1,810,466.572 RMSE=6,149,855.740 R2=0.7959 (best_iter=109)\n",
      "  - XGB cand#3  | MAE=1,815,739.512 RMSE=6,013,522.155 R2=0.8049 (best_iter=47)\n",
      "  - XGB cand#4  | MAE=1,815,739.512 RMSE=6,013,522.155 R2=0.8049 (best_iter=47)\n",
      "  - XGB cand#5  | MAE=1,902,877.303 RMSE=6,222,229.570 R2=0.7911 (best_iter=19)\n",
      "  - XGB cand#6  | MAE=1,810,466.572 RMSE=6,149,855.740 R2=0.7959 (best_iter=109)\n",
      "  - XGB cand#7  | MAE=2,017,260.072 RMSE=6,396,944.316 R2=0.7792 (best_iter=22)\n",
      "  - XGB cand#8  | MAE=1,902,877.303 RMSE=6,222,229.570 R2=0.7911 (best_iter=19)\n",
      "✅ BEST => XGB | MAE=1,810,466.572 저장: data/model_amt_xgb\\HOUR_04\\best_model.joblib\n",
      "\n",
      "===== HOUR=05 | train=36408 test=9102 =====\n",
      "  - XGB cand#1  | MAE=2,239,234.785 RMSE=6,579,767.637 R2=0.8535 (best_iter=22)\n",
      "  - XGB cand#2  | MAE=2,191,414.247 RMSE=6,455,020.560 R2=0.8590 (best_iter=73)\n",
      "  - XGB cand#3  | MAE=2,219,630.020 RMSE=6,402,885.928 R2=0.8613 (best_iter=14)\n",
      "  - XGB cand#4  | MAE=2,239,234.785 RMSE=6,579,767.637 R2=0.8535 (best_iter=22)\n",
      "  - XGB cand#5  | MAE=2,219,630.020 RMSE=6,402,885.928 R2=0.8613 (best_iter=14)\n",
      "  - XGB cand#6  | MAE=2,191,414.247 RMSE=6,455,020.560 R2=0.8590 (best_iter=73)\n",
      "  - XGB cand#7  | MAE=2,233,937.712 RMSE=6,432,692.332 R2=0.8600 (best_iter=22)\n",
      "  - XGB cand#8  | MAE=2,263,873.072 RMSE=6,569,589.344 R2=0.8539 (best_iter=20)\n",
      "✅ BEST => XGB | MAE=2,191,414.247 저장: data/model_amt_xgb\\HOUR_05\\best_model.joblib\n",
      "\n",
      "===== HOUR=06 | train=36400 test=9101 =====\n",
      "  - XGB cand#1  | MAE=1,749,304.105 RMSE=6,218,931.691 R2=0.8502 (best_iter=10)\n",
      "  - XGB cand#2  | MAE=1,776,558.937 RMSE=6,315,536.702 R2=0.8455 (best_iter=15)\n",
      "  - XGB cand#3  | MAE=1,813,905.368 RMSE=6,406,990.111 R2=0.8410 (best_iter=10)\n",
      "  - XGB cand#4  | MAE=1,753,421.795 RMSE=6,343,714.606 R2=0.8441 (best_iter=23)\n",
      "  - XGB cand#5  | MAE=1,749,304.105 RMSE=6,218,931.691 R2=0.8502 (best_iter=10)\n",
      "  - XGB cand#6  | MAE=1,662,010.147 RMSE=5,910,172.609 R2=0.8647 (best_iter=70)\n",
      "  - XGB cand#7  | MAE=1,776,558.937 RMSE=6,315,536.702 R2=0.8455 (best_iter=15)\n",
      "  - XGB cand#8  | MAE=1,662,010.147 RMSE=5,910,172.609 R2=0.8647 (best_iter=70)\n",
      "✅ BEST => XGB | MAE=1,662,010.147 저장: data/model_amt_xgb\\HOUR_06\\best_model.joblib\n",
      "\n",
      "===== HOUR=07 | train=36412 test=9104 =====\n",
      "  - XGB cand#1  | MAE=2,017,875.407 RMSE=6,894,422.481 R2=0.8217 (best_iter=19)\n",
      "  - XGB cand#2  | MAE=1,986,634.301 RMSE=6,689,677.915 R2=0.8321 (best_iter=22)\n",
      "  - XGB cand#3  | MAE=1,991,321.468 RMSE=6,638,859.911 R2=0.8346 (best_iter=34)\n",
      "  - XGB cand#4  | MAE=2,036,838.303 RMSE=7,013,098.926 R2=0.8155 (best_iter=5)\n",
      "  - XGB cand#5  | MAE=1,984,516.378 RMSE=6,613,881.767 R2=0.8359 (best_iter=17)\n",
      "  - XGB cand#6  | MAE=1,986,634.301 RMSE=6,689,677.915 R2=0.8321 (best_iter=22)\n",
      "  - XGB cand#7  | MAE=1,984,516.378 RMSE=6,613,881.767 R2=0.8359 (best_iter=17)\n",
      "  - XGB cand#8  | MAE=2,017,875.407 RMSE=6,894,422.481 R2=0.8217 (best_iter=19)\n",
      "✅ BEST => XGB | MAE=1,984,516.378 저장: data/model_amt_xgb\\HOUR_07\\best_model.joblib\n",
      "\n",
      "===== HOUR=08 | train=36351 test=9088 =====\n",
      "  - XGB cand#1  | MAE=2,525,593.883 RMSE=5,828,232.003 R2=0.8564 (best_iter=35)\n",
      "  - XGB cand#2  | MAE=2,525,593.883 RMSE=5,828,232.003 R2=0.8564 (best_iter=35)\n",
      "  - XGB cand#3  | MAE=2,546,949.459 RMSE=5,741,006.596 R2=0.8607 (best_iter=23)\n",
      "  - XGB cand#4  | MAE=2,461,756.374 RMSE=5,730,910.298 R2=0.8611 (best_iter=17)\n",
      "  - XGB cand#5  | MAE=2,381,864.070 RMSE=5,750,523.376 R2=0.8602 (best_iter=89)\n",
      "  - XGB cand#6  | MAE=2,461,756.374 RMSE=5,730,910.298 R2=0.8611 (best_iter=17)\n",
      "  - XGB cand#7  | MAE=2,621,702.537 RMSE=6,391,309.955 R2=0.8273 (best_iter=9)\n",
      "  - XGB cand#8  | MAE=2,381,864.070 RMSE=5,750,523.376 R2=0.8602 (best_iter=89)\n",
      "✅ BEST => XGB | MAE=2,381,864.070 저장: data/model_amt_xgb\\HOUR_08\\best_model.joblib\n",
      "\n",
      "===== HOUR=09 | train=35988 test=8998 =====\n",
      "  - XGB cand#1  | MAE=1,374,403.441 RMSE=3,149,749.586 R2=0.8342 (best_iter=32)\n",
      "  - XGB cand#2  | MAE=1,413,935.884 RMSE=3,309,787.008 R2=0.8169 (best_iter=13)\n",
      "  - XGB cand#3  | MAE=1,374,403.441 RMSE=3,149,749.586 R2=0.8342 (best_iter=32)\n",
      "  - XGB cand#4  | MAE=1,443,972.008 RMSE=3,374,046.448 R2=0.8097 (best_iter=16)\n",
      "  - XGB cand#5  | MAE=1,304,735.205 RMSE=2,971,363.253 R2=0.8524 (best_iter=58)\n",
      "  - XGB cand#6  | MAE=1,413,935.884 RMSE=3,309,787.008 R2=0.8169 (best_iter=13)\n",
      "  - XGB cand#7  | MAE=1,304,735.205 RMSE=2,971,363.253 R2=0.8524 (best_iter=58)\n",
      "  - XGB cand#8  | MAE=1,402,703.722 RMSE=3,186,885.576 R2=0.8303 (best_iter=45)\n",
      "✅ BEST => XGB | MAE=1,304,735.205 저장: data/model_amt_xgb\\HOUR_09\\best_model.joblib\n",
      "\n",
      "===== HOUR=10 | train=33247 test=8312 =====\n",
      "  - XGB cand#1  | MAE=527,239.645 RMSE=1,474,473.893 R2=0.6592 (best_iter=15)\n",
      "  - XGB cand#2  | MAE=517,848.401 RMSE=1,466,605.700 R2=0.6628 (best_iter=22)\n",
      "  - XGB cand#3  | MAE=517,848.401 RMSE=1,466,605.700 R2=0.6628 (best_iter=22)\n",
      "  - XGB cand#4  | MAE=538,398.260 RMSE=1,462,909.584 R2=0.6645 (best_iter=11)\n",
      "  - XGB cand#5  | MAE=534,346.098 RMSE=1,489,441.176 R2=0.6522 (best_iter=14)\n",
      "  - XGB cand#6  | MAE=527,239.645 RMSE=1,474,473.893 R2=0.6592 (best_iter=15)\n",
      "  - XGB cand#7  | MAE=498,276.545 RMSE=1,324,404.476 R2=0.7250 (best_iter=8)\n",
      "  - XGB cand#8  | MAE=498,276.545 RMSE=1,324,404.476 R2=0.7250 (best_iter=8)\n",
      "✅ BEST => XGB | MAE=498,276.545 저장: data/model_amt_xgb\\HOUR_10\\best_model.joblib\n",
      "\n",
      "[DONE] 학습 완료\n",
      "- 결과표 저장: data/model_amt_xgb\\results_by_hour.csv\n",
      "- 모델 저장 폴더: data/model_amt_xgb\n",
      "- 총 소요(초): 1,015.8\n",
      "CPU times: total: 2h 6min 34s\n",
      "Wall time: 16min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    DATA_PATH: str = \"data/수원시 한식 동별 데이터백업.csv\"          \n",
    "    SAVE_ROOT: str = \"data/model_amt_xgb\"          \n",
    "    USE_LOG_TARGET: bool = True                  \n",
    "\n",
    "    # 시간대 1~10 (항목요약)\n",
    "    HOURS: Tuple[int, ...] = tuple(range(1, 11))\n",
    "    TEST_RATIO: float = 0.2                       # 마지막 20% 테스트(시간순)\n",
    "\n",
    "    # 동별 성능 핵심: dong+hour 기준 lag/rolling\n",
    "    LAGS: Tuple[int, ...] = (1, 2, 3, 7)\n",
    "    ROLL_WINDOWS: Tuple[int, ...] = (3, 7)\n",
    "\n",
    "    RANDOM_STATE: int = 42\n",
    "    TOTAL_TIME_BUDGET_SEC: int = 1200             # 20분\n",
    "\n",
    "    # 진행 로그 출력\n",
    "    VERBOSE: bool = True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def log(msg: str, cfg: CFG):\n",
    "    if cfg.VERBOSE:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "def to_dt(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.replace(\"-\", \"\", regex=False)\n",
    "    return pd.to_datetime(s, format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "def time_split_last_ratio(df: pd.DataFrame, dt_col: str, test_ratio: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = df.sort_values(dt_col).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    cut = int(np.floor(n * (1.0 - test_ratio)))\n",
    "    cut = max(1, min(cut, n - 1))\n",
    "    return df.iloc[:cut].copy(), df.iloc[cut:].copy()\n",
    "\n",
    "def add_lag_rolling(df: pd.DataFrame, target: str, group_cols: List[str], dt_col: str,\n",
    "                    lags: Tuple[int, ...], wins: Tuple[int, ...]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    dong+hour 기준 lag/rolling → 동별 예측 성능에 가장 크게 기여\n",
    "    \"\"\"\n",
    "    df = df.sort_values(group_cols + [dt_col]).reset_index(drop=True)\n",
    "    g = df.groupby(group_cols, dropna=False)[target]\n",
    "\n",
    "    for k in lags:\n",
    "        df[f\"{target}_lag{k}\"] = g.shift(k)\n",
    "\n",
    "    for w in wins:\n",
    "        s = g.shift(1)\n",
    "        df[f\"{target}_rollmean{w}\"] = s.rolling(w).mean()\n",
    "        df[f\"{target}_rollmax{w}\"] = s.rolling(w).max()\n",
    "        df[f\"{target}_rollmin{w}\"] = s.rolling(w).min()\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_preprocess(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    # 숫자/범주 자동 분리 (DONG 같은 문자열은 cat)\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"string\")]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True)),\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "def make_param_candidates(seed: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    20분 안에 끝내기 위해 랜덤서치 대신 '잘 먹는' 후보 파라미터 8개 정도만 평가.\n",
    "    (각 후보는 early stopping으로 best_iter로 컷)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    cands = []\n",
    "\n",
    "    # base families (fast + strong)\n",
    "    for max_depth in [4, 6, 8]:\n",
    "        for lr in [0.03, 0.05]:\n",
    "            cands.append(dict(\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=lr,\n",
    "                subsample=0.85,\n",
    "                colsample_bytree=0.85,\n",
    "                min_child_weight=1.0,\n",
    "                reg_alpha=0.0,\n",
    "                reg_lambda=1.0,\n",
    "                gamma=0.0,\n",
    "            ))\n",
    "\n",
    "    # 조금 더 regularized / robust\n",
    "    cands += [\n",
    "        dict(max_depth=6, learning_rate=0.03, subsample=0.75, colsample_bytree=0.75, min_child_weight=3.0, reg_alpha=0.0, reg_lambda=5.0, gamma=0.0),\n",
    "        dict(max_depth=8, learning_rate=0.02, subsample=0.75, colsample_bytree=0.85, min_child_weight=5.0, reg_alpha=1e-3, reg_lambda=10.0, gamma=0.0),\n",
    "    ]\n",
    "\n",
    "    # shuffle a bit\n",
    "    rng.shuffle(cands)\n",
    "    return cands[:8]\n",
    "\n",
    "\n",
    "def fit_xgb_one(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_valid: pd.DataFrame,\n",
    "    y_valid: np.ndarray,\n",
    "    preprocess: ColumnTransformer,\n",
    "    params: Dict,\n",
    "    seed: int\n",
    "):\n",
    "    \"\"\"\n",
    "    preprocess → transform → XGB early stopping\n",
    "    \"\"\"\n",
    "    Xtr = preprocess.fit_transform(X_train, y_train)\n",
    "    Xva = preprocess.transform(X_valid)\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective=\"reg:absoluteerror\",\n",
    "        n_estimators=8000,                 # 크게 두고 early stopping으로 best_iter 결정\n",
    "        tree_method=\"hist\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "\n",
    "        # tuned params\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, y_train,\n",
    "        eval_set=[(Xva, y_valid)],\n",
    "        eval_metric=\"mae\",\n",
    "        early_stopping_rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    return preprocess, model\n",
    "\n",
    "\n",
    "def eval_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"mae\": float(mae), \"rmse\": float(rmse), \"r2\": float(r2)}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main Training Loop (HOUR 1~10)\n",
    "# =========================\n",
    "def train_all_hours_xgb(cfg: CFG):\n",
    "    t_all0 = time.time()\n",
    "    os.makedirs(cfg.SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "    # ---------- Load ----------\n",
    "    df = pd.read_csv(cfg.DATA_PATH)\n",
    "    required = {\"TA_YMD\", \"DONG\", \"HOUR\", \"DAY\", \"AMT\", \"CNT\", \"UNIT\", \"TEMP\", \"RAIN\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"데이터 컬럼 누락: {missing}. 현재 컬럼={list(df.columns)}\")\n",
    "\n",
    "    df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(str)\n",
    "    df[\"_dt\"] = to_dt(df[\"TA_YMD\"])\n",
    "    if df[\"_dt\"].isna().all():\n",
    "        raise ValueError(\"TA_YMD 날짜 파싱 실패. 예: 20251231 형태여야 합니다.\")\n",
    "\n",
    "    # ---------- Info ----------\n",
    "    log(f\"[INFO] TF 사용 여부(HOUR=1 딥러닝): False\", cfg)\n",
    "    log(f\"[INFO] log 타겟 사용: {cfg.USE_LOG_TARGET}\", cfg)\n",
    "    log(f\"[INFO] 학습 시간대: {list(cfg.HOURS)}\", cfg)\n",
    "    log(f\"[INFO] 저장 폴더: {cfg.SAVE_ROOT}\\n\", cfg)\n",
    "    log(\"[DEBUG] >>> 학습 루프 시작\\n\", cfg)\n",
    "\n",
    "    # 결과표\n",
    "    results_rows = []\n",
    "\n",
    "    # 시간 예산을 시간대별로 쪼개기(대략 균등)\n",
    "    per_hour_budget = max(60, int(cfg.TOTAL_TIME_BUDGET_SEC / len(cfg.HOURS)))\n",
    "\n",
    "    # ---------- loop each hour ----------\n",
    "    for hour in cfg.HOURS:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # hour filter\n",
    "        d = df[df[\"HOUR\"] == hour].copy()\n",
    "        d = d.sort_values(\"_dt\").reset_index(drop=True)\n",
    "\n",
    "        # 시간기반 split\n",
    "        train_df, test_df = time_split_last_ratio(d, \"_dt\", cfg.TEST_RATIO)\n",
    "\n",
    "        log(f\"===== HOUR={hour:02d} | train={len(train_df)} test={len(test_df)} =====\", cfg)\n",
    "\n",
    "        # dong+hour 기준 lag/rolling (동별 성능)\n",
    "        # hour는 고정이지만 group_cols에 넣으면 동별 시계열로 계산되므로 DONG만 넣어도 동일.\n",
    "        train_df2 = add_lag_rolling(train_df, \"AMT\", group_cols=[\"DONG\"], dt_col=\"_dt\",\n",
    "                                    lags=cfg.LAGS, wins=cfg.ROLL_WINDOWS)\n",
    "        test_df2 = add_lag_rolling(test_df, \"AMT\", group_cols=[\"DONG\"], dt_col=\"_dt\",\n",
    "                                   lags=cfg.LAGS, wins=cfg.ROLL_WINDOWS)\n",
    "\n",
    "        # feature set (UNIT/CNT는 타겟과 유사해 누수 가능성이 커서 기본은 제외)\n",
    "        # 필요하면 아래 drop에서 빼도 됨.\n",
    "        drop_cols = [\"AMT\", \"_dt\"]  # target + helper\n",
    "        # 동별 중요: DONG 포함\n",
    "        feature_cols = [c for c in train_df2.columns if c not in drop_cols]\n",
    "\n",
    "        # (권장) 누수/의미 애매한 컬럼 제거\n",
    "        # - UNIT/CNT가 \"실제 결과\"에 가까운 값이면 예측 시점에 없으므로 빼야 함\n",
    "        for leak in [\"UNIT\", \"CNT\"]:\n",
    "            if leak in feature_cols:\n",
    "                feature_cols.remove(leak)\n",
    "\n",
    "        X_train_full = train_df2[feature_cols].copy()\n",
    "        y_train_full = train_df2[\"AMT\"].astype(float).values\n",
    "\n",
    "        X_test = test_df2[feature_cols].copy()\n",
    "        y_test = test_df2[\"AMT\"].astype(float).values\n",
    "\n",
    "        # train 내부 valid 분리(마지막 15%를 valid)\n",
    "        ntr = len(X_train_full)\n",
    "        vcut = int(ntr * 0.85)\n",
    "        vcut = max(1, min(vcut, ntr - 1))\n",
    "\n",
    "        X_train = X_train_full.iloc[:vcut].copy()\n",
    "        y_train = y_train_full[:vcut]\n",
    "\n",
    "        X_valid = X_train_full.iloc[vcut:].copy()\n",
    "        y_valid = y_train_full[vcut:]\n",
    "\n",
    "        # log target\n",
    "        if cfg.USE_LOG_TARGET:\n",
    "            y_train_fit = np.log1p(y_train)\n",
    "            y_valid_fit = np.log1p(y_valid)\n",
    "        else:\n",
    "            y_train_fit = y_train\n",
    "            y_valid_fit = y_valid\n",
    "\n",
    "        # 후보 파라미터들\n",
    "        cands = make_param_candidates(cfg.RANDOM_STATE + hour)\n",
    "\n",
    "        best = None\n",
    "        best_mae = float(\"inf\")\n",
    "        best_info = None\n",
    "\n",
    "        # preprocess는 후보마다 새로 만드는 게 안전(컬럼 동일하지만 fit 상태 분리)\n",
    "        # 시간 예산 체크하면서 후보 평가\n",
    "        for i, p in enumerate(cands, start=1):\n",
    "            if time.time() - t0 > per_hour_budget:\n",
    "                log(f\"  [WARN] 시간 예산 초과로 후보 평가 조기 종료 (i={i-1}/{len(cands)})\", cfg)\n",
    "                break\n",
    "\n",
    "            pre = build_preprocess(X_train_full)\n",
    "            pre_fitted, model = fit_xgb_one(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train_fit,\n",
    "                X_valid=X_valid,\n",
    "                y_valid=y_valid_fit,\n",
    "                preprocess=pre,\n",
    "                params=p,\n",
    "                seed=cfg.RANDOM_STATE + hour\n",
    "            )\n",
    "\n",
    "            # test 평가\n",
    "            Xte_enc = pre_fitted.transform(X_test)\n",
    "            pred = model.predict(Xte_enc)\n",
    "            if cfg.USE_LOG_TARGET:\n",
    "                pred = np.expm1(pred)\n",
    "\n",
    "            m = eval_metrics(y_test, pred)\n",
    "            best_iter = getattr(model, \"best_iteration\", None)\n",
    "            if best_iter is None:\n",
    "                best_iter = getattr(model, \"best_ntree_limit\", None)\n",
    "\n",
    "            log(f\"  - XGB cand#{i:<2} | MAE={m['mae']:,.3f} RMSE={m['rmse']:,.3f} R2={m['r2']:.4f} (best_iter={best_iter})\", cfg)\n",
    "\n",
    "            if m[\"mae\"] < best_mae:\n",
    "                best_mae = m[\"mae\"]\n",
    "                best = {\"preprocess\": pre_fitted, \"model\": model}\n",
    "                best_info = {\"params\": p, \"metrics\": m, \"best_iter\": best_iter}\n",
    "\n",
    "        if best is None:\n",
    "            raise RuntimeError(f\"HOUR={hour:02d}에서 모델 학습이 실패했습니다(후보가 모두 스킵됨).\")\n",
    "\n",
    "        # 저장\n",
    "        hour_dir = os.path.join(cfg.SAVE_ROOT, f\"HOUR_{hour:02d}\")\n",
    "        os.makedirs(hour_dir, exist_ok=True)\n",
    "        save_path = os.path.join(hour_dir, \"best_model.joblib\")\n",
    "\n",
    "        joblib.dump(\n",
    "            {\n",
    "                \"type\": \"xgb_only\",\n",
    "                \"hour\": hour,\n",
    "                \"use_log_target\": cfg.USE_LOG_TARGET,\n",
    "                \"feature_cols\": feature_cols,\n",
    "                \"preprocess\": best[\"preprocess\"],\n",
    "                \"model\": best[\"model\"],\n",
    "                \"best_params\": best_info[\"params\"],\n",
    "                \"metrics_test\": best_info[\"metrics\"],\n",
    "                \"best_iter\": best_info[\"best_iter\"],\n",
    "            },\n",
    "            save_path\n",
    "        )\n",
    "\n",
    "        log(f\"✅ BEST => XGB | MAE={best_info['metrics']['mae']:,.3f} 저장: {save_path}\\n\", cfg)\n",
    "\n",
    "        results_rows.append({\n",
    "            \"HOUR\": hour,\n",
    "            \"BEST_MODEL\": \"XGB\",\n",
    "            \"MAE\": best_info[\"metrics\"][\"mae\"],\n",
    "            \"RMSE\": best_info[\"metrics\"][\"rmse\"],\n",
    "            \"R2\": best_info[\"metrics\"][\"r2\"],\n",
    "            \"BEST_ITER\": best_info[\"best_iter\"],\n",
    "            \"MODEL_PATH\": save_path,\n",
    "        })\n",
    "\n",
    "    # 결과표 저장\n",
    "    res_df = pd.DataFrame(results_rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "    res_path = os.path.join(cfg.SAVE_ROOT, \"results_by_hour.csv\")\n",
    "    res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    total_sec = time.time() - t_all0\n",
    "    log(\"[DONE] 학습 완료\", cfg)\n",
    "    log(f\"- 결과표 저장: {res_path}\", cfg)\n",
    "    log(f\"- 모델 저장 폴더: {cfg.SAVE_ROOT}\", cfg)\n",
    "    log(f\"- 총 소요(초): {total_sec:,.1f}\", cfg)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# (선택) 예측 함수: 저장된 모델로 예측\n",
    "# =========================\n",
    "def load_hour_model(save_root: str, hour: int) -> dict:\n",
    "    path = os.path.join(save_root, f\"HOUR_{hour:02d}\", \"best_model.joblib\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"모델이 없습니다: {path}\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_amt_one_hour(model_obj: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    X_new는 feature_cols를 포함하는 DF여야 함 (DONG, TA_YMD, DAY, HOUR, TEMP, RAIN 등)\n",
    "    \"\"\"\n",
    "    pre = model_obj[\"preprocess\"]\n",
    "    mdl = model_obj[\"model\"]\n",
    "    feat_cols = model_obj[\"feature_cols\"]\n",
    "\n",
    "    X_use = X_new[feat_cols].copy()\n",
    "    X_enc = pre.transform(X_use)\n",
    "    pred = mdl.predict(X_enc)\n",
    "    if model_obj.get(\"use_log_target\", False):\n",
    "        pred = np.expm1(pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = CFG(\n",
    "        DATA_PATH=\"data/수원시 한식 동별 데이터백업.csv\",  \n",
    "        SAVE_ROOT=\"data/model_amt_xgb\",\n",
    "        USE_LOG_TARGET=True,\n",
    "        TOTAL_TIME_BUDGET_SEC=1200,         # 20분\n",
    "        VERBOSE=True\n",
    "    )\n",
    "    train_all_hours_xgb(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c34ccc",
   "metadata": {},
   "source": [
    "# 8. claude 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5684940b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "모델 학습 시작\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터: 443,523건\n",
      "이상치 제거 후: 439,087건\n",
      "\n",
      "============================================================\n",
      "HOUR 01 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 01] 저장 완료 -> data/models_hourly_amt_cnt2\\hour_01_amt_cnt.joblib\n",
      "  AMT - MAE: 728,242.0 | R²: 0.8269 | RMSE: 1,600,828.6\n",
      "  CNT - MAE: 12.98 | R²: 0.8214 | RMSE: 39.51\n",
      "\n",
      "============================================================\n",
      "HOUR 02 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 02] 저장 완료 -> data/models_hourly_amt_cnt2\\hour_02_amt_cnt.joblib\n",
      "  AMT - MAE: 244,750.7 | R²: 0.4233 | RMSE: 785,378.3\n",
      "  CNT - MAE: 7.64 | R²: 0.8720 | RMSE: 14.40\n",
      "\n",
      "============================================================\n",
      "HOUR 03 모델 학습 중...\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 436\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모델 학습 시작\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m--> 436\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_save_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m최종 성능 지표:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics_df\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[1;32mIn[3], line 330\u001b[0m, in \u001b[0;36mtrain_and_save_models\u001b[1;34m(random_state)\u001b[0m\n\u001b[0;32m    324\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m    325\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m, pre),\n\u001b[0;32m    326\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, model),\n\u001b[0;32m    327\u001b[0m ])\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# 예측 및 평가\u001b[39;00m\n\u001b[0;32m    333\u001b[0m pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    360\u001b[0m     cloned_transformer,\n\u001b[0;32m    361\u001b[0m     X,\n\u001b[0;32m    362\u001b[0m     y,\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    364\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    365\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    367\u001b[0m )\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\joblib\\memory.py:326\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:727\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m--> 727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:658\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    652\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[0;32m    654\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[0;32m    655\u001b[0m     )\n\u001b[0;32m    656\u001b[0m )\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfitted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumnTransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\joblib\\parallel.py:1986\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1984\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1985\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\joblib\\parallel.py:1914\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1914\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\base.py:862\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "시간대별 AMT/CNT 예측 (수원시 한식 동별 데이터) - 개선 버전\n",
    "- 입력: 날짜(TA_YMD), DONG, (기상청 API로 가져온) 시간대별 TEMP/RAIN\n",
    "- 출력: 시간대별 AMT, CNT\n",
    "\n",
    "개선사항:\n",
    "1. 피처 엔지니어링 강화 (온도 구간, 계절, 주말 등)\n",
    "2. 하이퍼파라미터 최적화\n",
    "3. 이상치 처리\n",
    "4. R² 스코어 평가 추가\n",
    "5. 모델별 최적화\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# =========================\n",
    "# A) 설정\n",
    "# =========================\n",
    "load_dotenv()\n",
    "SERVICE_KEY = os.getenv('RAIN_ID')\n",
    "DATA_CSV = \"data/수원시 한식 동별 데이터백업.csv\"\n",
    "MODEL_DIR = \"data/models_hourly_amt_cnt2\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ---- 기상청 단기예보(getVilageFcst) 엔드포인트 ----\n",
    "KMA_VILAGE_URL = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "BASE_TIMES = [\"2300\", \"2000\", \"1700\", \"1400\", \"1100\", \"0800\", \"0500\", \"0200\"]\n",
    "\n",
    "# HOUR(01~10) 구간 대표 시각\n",
    "HOUR_SLOT_TO_REP_TIME = {\n",
    "    1: \"0300\", 2: \"0800\", 3: \"1000\", 4: \"1200\", 5: \"1400\",\n",
    "    6: \"1600\", 7: \"1800\", 8: \"2000\", 9: \"2200\", 10: \"2300\",\n",
    "}\n",
    "\n",
    "# ---- DONG -> (nx, ny) 격자 좌표 매핑 ----\n",
    "DONG_GRID_CSV = \"data/dong_grid.csv\"\n",
    "DONG_TO_GRID = {}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# B) 유틸: 요일 계산 (1~7, 월=1 ... 일=7)\n",
    "# =========================\n",
    "def ymd_to_day_1to7(yyyymmdd: str) -> int:\n",
    "    dt = datetime.strptime(str(yyyymmdd), \"%Y%m%d\")\n",
    "    return dt.weekday() + 1\n",
    "\n",
    "\n",
    "# =========================\n",
    "# C) 피처 엔지니어링 함수 추가\n",
    "# =========================\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    추가 피처 생성:\n",
    "    - TEMP_BIN: 온도 구간 (분석에서 발견한 최적 구간 활용)\n",
    "    - IS_WEEKEND: 주말 여부\n",
    "    - SEASON: 계절\n",
    "    - TEMP_SQUARED: 온도 제곱 (비선형 관계)\n",
    "    - IS_HOT: 무더운 날씨 (27도 이상)\n",
    "    - IS_COLD: 추운 날씨 (-7~1도)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 온도 구간 (분석 결과 기반)\n",
    "    df['TEMP_BIN'] = pd.cut(df['TEMP'], \n",
    "                            bins=[-100, -7, 1, 10, 18, 27, 100],\n",
    "                            labels=['very_cold', 'cold', 'cool', 'pleasant', 'warm', 'hot'])\n",
    "    df['TEMP_BIN'] = df['TEMP_BIN'].astype(str)\n",
    "    \n",
    "    # 주말 여부 (토=6, 일=7)\n",
    "    df['IS_WEEKEND'] = (df['DAY'] >= 6).astype(int)\n",
    "    \n",
    "    # 계절 (월 추출 필요)\n",
    "    df['MONTH'] = df['TA_YMD'].astype(str).str[4:6].astype(int)\n",
    "    df['SEASON'] = df['MONTH'].apply(lambda m: \n",
    "        'spring' if 3 <= m <= 5 else\n",
    "        'summer' if 6 <= m <= 8 else\n",
    "        'fall' if 9 <= m <= 11 else 'winter'\n",
    "    )\n",
    "    \n",
    "    # 온도 제곱 (역U자 패턴 반영)\n",
    "    df['TEMP_SQUARED'] = df['TEMP'] ** 2\n",
    "    \n",
    "    # 특정 온도 구간 플래그\n",
    "    df['IS_HOT'] = (df['TEMP'] >= 27).astype(int)\n",
    "    df['IS_COLD'] = ((df['TEMP'] >= -7) & (df['TEMP'] <= 1)).astype(int)\n",
    "    df['IS_OPTIMAL'] = ((df['TEMP'] >= 10) & (df['TEMP'] <= 14)).astype(int)\n",
    "    \n",
    "    # 강수량 로그 변환 (0은 작은 값으로 대체)\n",
    "    df['RAIN_LOG'] = np.log1p(df['RAIN'])\n",
    "    \n",
    "    # 시간대 특성\n",
    "    df['IS_LUNCH'] = (df['HOUR'] == 4).astype(int)  # 점심시간대\n",
    "    df['IS_DINNER'] = (df['HOUR'].isin([7, 8])).astype(int)  # 저녁시간대\n",
    "    df['IS_DAWN'] = (df['HOUR'] == 1).astype(int)  # 새벽시간대\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(df, target_col, threshold_percentile=99):\n",
    "    \"\"\"이상치 제거\"\"\"\n",
    "    threshold = df[target_col].quantile(threshold_percentile / 100)\n",
    "    return df[df[target_col] <= threshold]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# D) 기상청 API 함수들 (원본 유지)\n",
    "# =========================\n",
    "def pick_base_datetime_kst(now_kst: datetime) -> tuple[str, str]:\n",
    "    today = now_kst.strftime(\"%Y%m%d\")\n",
    "    hhmm = now_kst.strftime(\"%H%M\")\n",
    "    for bt in BASE_TIMES:\n",
    "        if hhmm >= bt:\n",
    "            return today, bt\n",
    "    yday = (now_kst - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    return yday, \"2300\"\n",
    "\n",
    "\n",
    "def fetch_vilage_fcst_items(service_key: str, base_date: str, base_time: str, nx: int, ny: int,\n",
    "                            num_of_rows: int = 2000) -> list[dict]:\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_of_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(KMA_VILAGE_URL, params=params, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "    return items\n",
    "\n",
    "\n",
    "def parse_pcp_to_float(pcp_val) -> float:\n",
    "    if pcp_val is None:\n",
    "        return 0.0\n",
    "    s = str(pcp_val).strip()\n",
    "    if s in (\"강수없음\", \"없음\", \"\"):\n",
    "        return 0.0\n",
    "    s = s.replace(\"mm\", \"\").replace(\"미만\", \"\").strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def get_temp_rain_for_datetime(service_key: str, dong: str, target_yyyymmdd: str, target_hhmm: str) -> tuple[float, float]:\n",
    "    if not DONG_TO_GRID:\n",
    "        if os.path.exists(DONG_GRID_CSV):\n",
    "            g = pd.read_csv(DONG_GRID_CSV)\n",
    "            for _, row in g.iterrows():\n",
    "                DONG_TO_GRID[str(row[\"DONG\"])] = (int(row[\"nx\"]), int(row[\"ny\"]))\n",
    "        else:\n",
    "            raise ValueError(f\"[필수] {DONG_GRID_CSV}를 만들거나 DONG_TO_GRID dict를 채워주세요.\")\n",
    "\n",
    "    if dong not in DONG_TO_GRID:\n",
    "        raise ValueError(f\"DONG_TO_GRID에 '{dong}'의 nx/ny가 없습니다.\")\n",
    "\n",
    "    nx, ny = DONG_TO_GRID[dong]\n",
    "    now_kst = datetime.now(timezone(timedelta(hours=9)))\n",
    "    base_date, base_time = pick_base_datetime_kst(now_kst)\n",
    "    items = fetch_vilage_fcst_items(service_key, base_date, base_time, nx, ny)\n",
    "\n",
    "    tmp = None\n",
    "    pcp = None\n",
    "    for it in items:\n",
    "        if it.get(\"fcstDate\") == target_yyyymmdd and it.get(\"fcstTime\") == target_hhmm:\n",
    "            cat = it.get(\"category\")\n",
    "            if cat == \"TMP\":\n",
    "                tmp = float(it.get(\"fcstValue\"))\n",
    "            elif cat == \"PCP\":\n",
    "                pcp = parse_pcp_to_float(it.get(\"fcstValue\"))\n",
    "\n",
    "    if tmp is None:\n",
    "        tmp = 0.0\n",
    "    if pcp is None:\n",
    "        pcp = 0.0\n",
    "    return tmp, pcp\n",
    "\n",
    "\n",
    "def get_hourly_weather_features(service_key: str, dong: str, yyyymmdd: str) -> pd.DataFrame:\n",
    "    day_1to7 = ymd_to_day_1to7(yyyymmdd)\n",
    "    rows = []\n",
    "    for hour_slot in range(1, 11):\n",
    "        rep_time = HOUR_SLOT_TO_REP_TIME[hour_slot]\n",
    "        temp, rain = get_temp_rain_for_datetime(service_key, dong, yyyymmdd, rep_time)\n",
    "        rows.append({\n",
    "            \"TA_YMD\": int(yyyymmdd),\n",
    "            \"DONG\": dong,\n",
    "            \"HOUR\": hour_slot,\n",
    "            \"DAY\": day_1to7,\n",
    "            \"TEMP\": float(temp),\n",
    "            \"RAIN\": float(rain),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return create_features(df)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# E) 개선된 전처리 파이프라인\n",
    "# =========================\n",
    "def build_preprocess():\n",
    "    \"\"\"개선된 전처리 - 더 많은 피처 활용\"\"\"\n",
    "    cat_cols = [\"DONG\", \"TEMP_BIN\", \"SEASON\"]\n",
    "    num_cols = [\"DAY\", \"TEMP\", \"RAIN\", \"TEMP_SQUARED\", \"RAIN_LOG\",\n",
    "                \"IS_WEEKEND\", \"IS_HOT\", \"IS_COLD\", \"IS_OPTIMAL\",\n",
    "                \"IS_LUNCH\", \"IS_DINNER\", \"IS_DAWN\"]\n",
    "    \n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", StandardScaler(), num_cols),  # 스케일링 추가\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "# =========================\n",
    "# F) 개선된 모델 학습\n",
    "# =========================\n",
    "def train_and_save_models(random_state: int = 42):\n",
    "    df = pd.read_csv(DATA_CSV)\n",
    "    \n",
    "    # 타입 정리\n",
    "    df[\"DONG\"] = df[\"DONG\"].astype(str)\n",
    "    df[\"HOUR\"] = df[\"HOUR\"].astype(int)\n",
    "    df[\"DAY\"] = df[\"DAY\"].astype(int)\n",
    "    df[\"TEMP\"] = df[\"TEMP\"].astype(float)\n",
    "    df[\"RAIN\"] = df[\"RAIN\"].astype(float)\n",
    "    \n",
    "    # 피처 생성\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # 이상치 제거 (AMT 기준 상위 1%)\n",
    "    print(f\"원본 데이터: {len(df):,}건\")\n",
    "    df = remove_outliers(df, \"AMT\", threshold_percentile=99)\n",
    "    print(f\"이상치 제거 후: {len(df):,}건\")\n",
    "    \n",
    "    y_cols = [\"AMT\", \"CNT\"]\n",
    "    metrics = []\n",
    "\n",
    "    for h in range(1, 11):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"HOUR {h:02d} 모델 학습 중...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        sub = df[df[\"HOUR\"] == h].copy()\n",
    "        sub = sub.dropna(subset=[\"DONG\", \"DAY\", \"TEMP\", \"RAIN\", \"AMT\", \"CNT\"])\n",
    "        \n",
    "        # 피처 선택\n",
    "        feature_cols = [\"DONG\", \"DAY\", \"TEMP\", \"RAIN\", \"TEMP_BIN\", \"SEASON\",\n",
    "                       \"TEMP_SQUARED\", \"RAIN_LOG\", \"IS_WEEKEND\", \"IS_HOT\", \n",
    "                       \"IS_COLD\", \"IS_OPTIMAL\", \"IS_LUNCH\", \"IS_DINNER\", \"IS_DAWN\"]\n",
    "        \n",
    "        X = sub[feature_cols]\n",
    "        y = sub[y_cols]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        pre = build_preprocess()\n",
    "        \n",
    "        if h == 1:\n",
    "            # HOUR=1: 개선된 MLPRegressor\n",
    "            base_model = MLPRegressor(\n",
    "                hidden_layer_sizes=(256, 128, 64),  # 더 깊은 네트워크\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                alpha=1e-4,\n",
    "                batch_size=128,  # 배치 크기 감소\n",
    "                learning_rate=\"adaptive\",  # 적응형 학습률\n",
    "                learning_rate_init=5e-4,\n",
    "                max_iter=500,  # 더 많은 에포크\n",
    "                random_state=42,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.1,\n",
    "                n_iter_no_change=20,\n",
    "                tol=1e-5,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            # HOUR=2~10: 최적화된 XGBRegressor\n",
    "            base_model = XGBRegressor(\n",
    "                n_estimators=800,  # 트리 개수\n",
    "                learning_rate=0.04,  # 학습률\n",
    "                max_depth=7,  # 깊이\n",
    "                min_child_weight=3,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,  # L1 정규화\n",
    "                reg_lambda=1.5,  # L2 정규화\n",
    "                gamma=0.1,  # 최소 손실 감소\n",
    "                objective=\"reg:squarederror\",\n",
    "                random_state=random_state,\n",
    "                tree_method=\"hist\",\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        \n",
    "        model = MultiOutputRegressor(base_model)\n",
    "        \n",
    "        pipe = Pipeline([\n",
    "            (\"pre\", pre),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        \n",
    "        # 모델 학습\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        # 예측 및 평가\n",
    "        pred = pipe.predict(X_test)\n",
    "        \n",
    "        # AMT 평가\n",
    "        mae_amt = mean_absolute_error(y_test[\"AMT\"].values, pred[:, 0])\n",
    "        r2_amt = r2_score(y_test[\"AMT\"].values, pred[:, 0])\n",
    "        rmse_amt = np.sqrt(mean_squared_error(y_test[\"AMT\"].values, pred[:, 0]))\n",
    "        \n",
    "        # CNT 평가\n",
    "        mae_cnt = mean_absolute_error(y_test[\"CNT\"].values, pred[:, 1])\n",
    "        r2_cnt = r2_score(y_test[\"CNT\"].values, pred[:, 1])\n",
    "        rmse_cnt = np.sqrt(mean_squared_error(y_test[\"CNT\"].values, pred[:, 1]))\n",
    "        \n",
    "        # 모델 저장\n",
    "        model_path = os.path.join(MODEL_DIR, f\"hour_{h:02d}_amt_cnt.joblib\")\n",
    "        joblib.dump(pipe, model_path)\n",
    "        \n",
    "        metrics.append({\n",
    "            \"HOUR\": h,\n",
    "            \"MAE_AMT\": mae_amt,\n",
    "            \"R2_AMT\": r2_amt,\n",
    "            \"RMSE_AMT\": rmse_amt,\n",
    "            \"MAE_CNT\": mae_cnt,\n",
    "            \"R2_CNT\": r2_cnt,\n",
    "            \"RMSE_CNT\": rmse_cnt,\n",
    "            \"model_path\": model_path,\n",
    "            \"model_type\": \"MLP(Deep)\" if h == 1 else \"XGB\",\n",
    "            \"n_train\": len(X_train),\n",
    "            \"n_test\": len(X_test),\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n[HOUR {h:02d}] 저장 완료 -> {model_path}\")\n",
    "        print(f\"  AMT - MAE: {mae_amt:,.1f} | R²: {r2_amt:.4f} | RMSE: {rmse_amt:,.1f}\")\n",
    "        print(f\"  CNT - MAE: {mae_cnt:,.2f} | R²: {r2_cnt:.4f} | RMSE: {rmse_cnt:,.2f}\")\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics).sort_values(\"HOUR\")\n",
    "    metrics_df.to_csv(os.path.join(MODEL_DIR, \"metrics_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"전체 모델 성능 요약\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nAMT 평균 성능:\")\n",
    "    print(f\"  - MAE: {metrics_df['MAE_AMT'].mean():,.1f}\")\n",
    "    print(f\"  - R²: {metrics_df['R2_AMT'].mean():.4f}\")\n",
    "    print(f\"\\nCNT 평균 성능:\")\n",
    "    print(f\"  - MAE: {metrics_df['MAE_CNT'].mean():.2f}\")\n",
    "    print(f\"  - R²: {metrics_df['R2_CNT'].mean():.4f}\")\n",
    "    print(\"\\n=== metrics_summary.csv 저장 완료 ===\\n\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# G) 예측 함수 (원본 유지하되 피처 추가)\n",
    "# =========================\n",
    "def load_models():\n",
    "    models = {}\n",
    "    for h in range(1, 11):\n",
    "        p = os.path.join(MODEL_DIR, f\"hour_{h:02d}_amt_cnt.joblib\")\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"모델 파일이 없습니다: {p}\")\n",
    "        models[h] = joblib.load(p)\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_day(yyyymmdd: str, dong: str) -> pd.DataFrame:\n",
    "    # 기상청에서 시간대별 TEMP/RAIN 구성 (피처 포함)\n",
    "    feat_df = get_hourly_weather_features(SERVICE_KEY, dong, yyyymmdd)\n",
    "    \n",
    "    models = load_models()\n",
    "    \n",
    "    outs = []\n",
    "    for h in range(1, 11):\n",
    "        row_data = feat_df[feat_df[\"HOUR\"] == h]\n",
    "        \n",
    "        # 필요한 피처만 선택\n",
    "        feature_cols = [\"DONG\", \"DAY\", \"TEMP\", \"RAIN\", \"TEMP_BIN\", \"SEASON\",\n",
    "                       \"TEMP_SQUARED\", \"RAIN_LOG\", \"IS_WEEKEND\", \"IS_HOT\", \n",
    "                       \"IS_COLD\", \"IS_OPTIMAL\", \"IS_LUNCH\", \"IS_DINNER\", \"IS_DAWN\"]\n",
    "        row = row_data[feature_cols]\n",
    "        \n",
    "        pred_amt, pred_cnt = models[h].predict(row)[0]\n",
    "        \n",
    "        outs.append({\n",
    "            \"TA_YMD\": yyyymmdd,\n",
    "            \"DONG\": dong,\n",
    "            \"HOUR\": h,\n",
    "            \"TEMP\": float(row_data[\"TEMP\"].iloc[0]),\n",
    "            \"RAIN\": float(row_data[\"RAIN\"].iloc[0]),\n",
    "            \"PRED_AMT\": max(0, float(pred_amt)),  # 음수 방지\n",
    "            \"PRED_CNT\": max(0, float(pred_cnt)),  # 음수 방지\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(outs)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# H) 실행 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 학습 + 저장\n",
    "    print(\"=\"*60)\n",
    "    print(\"모델 학습 시작\")\n",
    "    print(\"=\"*60)\n",
    "    metrics_df = train_and_save_models()\n",
    "    print(\"\\n최종 성능 지표:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    \n",
    "    # 2) 예측 예시\n",
    "    if not SERVICE_KEY:\n",
    "        print(\"\\n[경고] 환경변수 RAIN_ID가 비었습니다. 예측을 하려면 SERVICE_KEY를 넣으세요.\\n\")\n",
    "    else:\n",
    "        try:\n",
    "            yyyymmdd = \"20251230\"\n",
    "            dong = \"매교동\"\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"예측 예시: {dong}, {yyyymmdd}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            pred_df = predict_day(yyyymmdd, dong)\n",
    "            print(pred_df.to_string(index=False))\n",
    "            \n",
    "            output_file = f\"pred_{dong}_{yyyymmdd}.csv\"\n",
    "            pred_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"\\n예측 결과 저장: {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n예측 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb67c8e",
   "metadata": {},
   "source": [
    "# 9. 이상치 제거 후 정님 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca510a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "모델 학습 시작\n",
      "============================================================\n",
      "피처 생성 후: 443,523건\n",
      "무한대/NaN 제거 후: 443,523건\n",
      "이상치 제거 후: 439,087건\n",
      "\n",
      "============================================================\n",
      "HOUR 01 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 01] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_01_amt_cnt.joblib\n",
      "  AMT - MAE: 733,359.3 | R²: 0.8268 | RMSE: 1,600,950.1\n",
      "  CNT - MAE: 12.98 | R²: 0.8215 | RMSE: 39.50\n",
      "\n",
      "============================================================\n",
      "HOUR 02 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 02] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_02_amt_cnt.joblib\n",
      "  AMT - MAE: 244,781.1 | R²: 0.4236 | RMSE: 785,191.4\n",
      "  CNT - MAE: 7.63 | R²: 0.8720 | RMSE: 14.39\n",
      "\n",
      "============================================================\n",
      "HOUR 03 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 03] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_03_amt_cnt.joblib\n",
      "  AMT - MAE: 584,866.4 | R²: 0.4755 | RMSE: 1,487,117.6\n",
      "  CNT - MAE: 14.82 | R²: 0.8495 | RMSE: 28.28\n",
      "\n",
      "============================================================\n",
      "HOUR 04 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 04] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_04_amt_cnt.joblib\n",
      "  AMT - MAE: 1,293,354.1 | R²: 0.8980 | RMSE: 2,551,237.1\n",
      "  CNT - MAE: 39.91 | R²: 0.9280 | RMSE: 75.61\n",
      "\n",
      "============================================================\n",
      "HOUR 05 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 05] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_05_amt_cnt.joblib\n",
      "  AMT - MAE: 1,408,571.2 | R²: 0.8707 | RMSE: 2,764,402.6\n",
      "  CNT - MAE: 34.39 | R²: 0.8984 | RMSE: 72.55\n",
      "\n",
      "============================================================\n",
      "HOUR 06 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 06] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_06_amt_cnt.joblib\n",
      "  AMT - MAE: 1,142,516.0 | R²: 0.7550 | RMSE: 2,667,225.5\n",
      "  CNT - MAE: 27.20 | R²: 0.8385 | RMSE: 64.61\n",
      "\n",
      "============================================================\n",
      "HOUR 07 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 07] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_07_amt_cnt.joblib\n",
      "  AMT - MAE: 1,297,076.8 | R²: 0.8280 | RMSE: 2,671,931.9\n",
      "  CNT - MAE: 32.58 | R²: 0.8336 | RMSE: 80.30\n",
      "\n",
      "============================================================\n",
      "HOUR 08 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 08] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_08_amt_cnt.joblib\n",
      "  AMT - MAE: 1,641,233.0 | R²: 0.8995 | RMSE: 2,889,629.0\n",
      "  CNT - MAE: 30.50 | R²: 0.8613 | RMSE: 73.55\n",
      "\n",
      "============================================================\n",
      "HOUR 09 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 09] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_09_amt_cnt.joblib\n",
      "  AMT - MAE: 1,093,323.3 | R²: 0.8870 | RMSE: 2,103,817.3\n",
      "  CNT - MAE: 20.82 | R²: 0.7116 | RMSE: 75.41\n",
      "\n",
      "============================================================\n",
      "HOUR 10 모델 학습 중...\n",
      "============================================================\n",
      "\n",
      "[HOUR 10] 저장 완료 -> data/models_hourly_amt_cnt_improved\\hour_10_amt_cnt.joblib\n",
      "  AMT - MAE: 486,489.1 | R²: 0.7769 | RMSE: 1,264,818.8\n",
      "  CNT - MAE: 8.45 | R²: 0.8203 | RMSE: 25.59\n",
      "\n",
      "============================================================\n",
      "전체 모델 성능 요약\n",
      "============================================================\n",
      "\n",
      "AMT 평균 성능:\n",
      "  - MAE: 992,557.0\n",
      "  - R²: 0.7641\n",
      "\n",
      "CNT 평균 성능:\n",
      "  - MAE: 22.93\n",
      "  - R²: 0.8435\n",
      "\n",
      "=== metrics_summary.csv 저장 완료 ===\n",
      "\n",
      "\n",
      "최종 성능 지표:\n",
      " HOUR      MAE_AMT   R2_AMT     RMSE_AMT   MAE_CNT   R2_CNT  RMSE_CNT                                                 model_path model_type  n_train  n_test\n",
      "    1 7.333593e+05 0.826824 1.600950e+06 12.979030 0.821513 39.496512 data/models_hourly_amt_cnt_improved\\hour_01_amt_cnt.joblib  MLP(Deep)    33699    8425\n",
      "    2 2.447811e+05 0.423565 7.851914e+05  7.632599 0.872031 14.394062 data/models_hourly_amt_cnt_improved\\hour_02_amt_cnt.joblib        XGB    33998    8500\n",
      "    3 5.848664e+05 0.475495 1.487118e+06 14.822349 0.849547 28.277508 data/models_hourly_amt_cnt_improved\\hour_03_amt_cnt.joblib        XGB    35704    8927\n",
      "    4 1.293354e+06 0.898021 2.551237e+06 39.913054 0.928041 75.607704 data/models_hourly_amt_cnt_improved\\hour_04_amt_cnt.joblib        XGB    35981    8996\n",
      "    5 1.408571e+06 0.870746 2.764403e+06 34.391579 0.898427 72.553162 data/models_hourly_amt_cnt_improved\\hour_05_amt_cnt.joblib        XGB    35698    8925\n",
      "    6 1.142516e+06 0.755039 2.667225e+06 27.195318 0.838490 64.614089 data/models_hourly_amt_cnt_improved\\hour_06_amt_cnt.joblib        XGB    35994    8999\n",
      "    7 1.297077e+06 0.828006 2.671932e+06 32.584841 0.833624 80.295905 data/models_hourly_amt_cnt_improved\\hour_07_amt_cnt.joblib        XGB    35887    8972\n",
      "    8 1.641233e+06 0.899476 2.889629e+06 30.501585 0.861264 73.554627 data/models_hourly_amt_cnt_improved\\hour_08_amt_cnt.joblib        XGB    35305    8827\n",
      "    9 1.093323e+06 0.887007 2.103817e+06 20.822217 0.711563 75.407990 data/models_hourly_amt_cnt_improved\\hour_09_amt_cnt.joblib        XGB    35752    8939\n",
      "   10 4.864891e+05 0.776927 1.264819e+06  8.454474 0.820297 25.587986 data/models_hourly_amt_cnt_improved\\hour_10_amt_cnt.joblib        XGB    33247    8312\n",
      "\n",
      "============================================================\n",
      "예측 예시: 매교동, 20251230\n",
      "============================================================\n",
      "\n",
      "예측 중 오류 발생: 429 Client Error: Too Many Requests for url: https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst?serviceKey=ab7c101889d29bfff11d9cba7295dcbe50c35eef0ad67f289ac04b54c02c52a4&pageNo=1&numOfRows=2000&dataType=JSON&base_date=20260102&base_time=1400&nx=61&ny=120\n",
      "CPU times: total: 10min 43s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "시간대별 AMT/CNT 예측 (수원시 한식 동별 데이터) - 개선 버전\n",
    "- 입력: 날짜(TA_YMD), DONG, (기상청 API로 가져온) 시간대별 TEMP/RAIN\n",
    "- 출력: 시간대별 AMT, CNT\n",
    "\n",
    "개선사항:\n",
    "1. 피처 엔지니어링 강화 (온도 구간, 계절, 주말 등)\n",
    "2. 하이퍼파라미터 최적화\n",
    "3. 이상치 처리\n",
    "4. R² 스코어 평가 추가\n",
    "5. 모델별 최적화\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# =========================\n",
    "# A) 설정\n",
    "# =========================\n",
    "load_dotenv()\n",
    "SERVICE_KEY = os.getenv('RAIN_ID')\n",
    "DATA_CSV = \"data/수원시 한식 동별 데이터백업.csv\"\n",
    "MODEL_DIR = \"data/models_hourly_amt_cnt_improved\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ---- 기상청 단기예보(getVilageFcst) 엔드포인트 ----\n",
    "KMA_VILAGE_URL = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "BASE_TIMES = [\"2300\", \"2000\", \"1700\", \"1400\", \"1100\", \"0800\", \"0500\", \"0200\"]\n",
    "\n",
    "# HOUR(01~10) 구간 대표 시각\n",
    "HOUR_SLOT_TO_REP_TIME = {\n",
    "    1: \"0300\", 2: \"0800\", 3: \"1000\", 4: \"1200\", 5: \"1400\",\n",
    "    6: \"1600\", 7: \"1800\", 8: \"2000\", 9: \"2200\", 10: \"2300\",\n",
    "}\n",
    "\n",
    "# ---- DONG -> (nx, ny) 격자 좌표 매핑 ----\n",
    "DONG_GRID_CSV = \"data/dong_grid.csv\"\n",
    "DONG_TO_GRID = {}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# B) 유틸: 요일 계산 (1~7, 월=1 ... 일=7)\n",
    "# =========================\n",
    "def ymd_to_day_1to7(yyyymmdd: str) -> int:\n",
    "    dt = datetime.strptime(str(yyyymmdd), \"%Y%m%d\")\n",
    "    return dt.weekday() + 1\n",
    "\n",
    "\n",
    "# =========================\n",
    "# C) 피처 엔지니어링 함수 추가\n",
    "# =========================\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    추가 피처 생성:\n",
    "    - TEMP_BIN: 온도 구간 (분석에서 발견한 최적 구간 활용)\n",
    "    - IS_WEEKEND: 주말 여부\n",
    "    - SEASON: 계절\n",
    "    - TEMP_SQUARED: 온도 제곱 (비선형 관계)\n",
    "    - IS_HOT: 무더운 날씨 (27도 이상)\n",
    "    - IS_COLD: 추운 날씨 (-7~1도)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 온도 구간 (분석 결과 기반)\n",
    "    df['TEMP_BIN'] = pd.cut(df['TEMP'], \n",
    "                            bins=[-100, -7, 1, 10, 18, 27, 100],\n",
    "                            labels=['very_cold', 'cold', 'cool', 'pleasant', 'warm', 'hot'])\n",
    "    df['TEMP_BIN'] = df['TEMP_BIN'].astype(str)\n",
    "    \n",
    "    # 주말 여부 (토=6, 일=7)\n",
    "    df['IS_WEEKEND'] = (df['DAY'] >= 6).astype(int)\n",
    "    \n",
    "    # 계절 (월 추출 필요)\n",
    "    df['MONTH'] = df['TA_YMD'].astype(str).str[4:6].astype(int)\n",
    "    df['SEASON'] = df['MONTH'].apply(lambda m: \n",
    "        'spring' if 3 <= m <= 5 else\n",
    "        'summer' if 6 <= m <= 8 else\n",
    "        'fall' if 9 <= m <= 11 else 'winter'\n",
    "    )\n",
    "    \n",
    "    # 온도 제곱 (역U자 패턴 반영)\n",
    "    df['TEMP_SQUARED'] = df['TEMP'] ** 2\n",
    "    \n",
    "    # 특정 온도 구간 플래그\n",
    "    df['IS_HOT'] = (df['TEMP'] >= 27).astype(int)\n",
    "    df['IS_COLD'] = ((df['TEMP'] >= -7) & (df['TEMP'] <= 1)).astype(int)\n",
    "    df['IS_OPTIMAL'] = ((df['TEMP'] >= 10) & (df['TEMP'] <= 14)).astype(int)\n",
    "    \n",
    "    # 강수량 로그 변환 - 음수 값 0으로 처리\n",
    "    df['RAIN_CLIPPED'] = df['RAIN'].clip(lower=0)  # 음수를 0으로\n",
    "    df['RAIN_LOG'] = np.log1p(df['RAIN_CLIPPED'])\n",
    "    \n",
    "    # 시간대 특성\n",
    "    df['IS_LUNCH'] = (df['HOUR'] == 4).astype(int)  # 점심시간대\n",
    "    df['IS_DINNER'] = (df['HOUR'].isin([7, 8])).astype(int)  # 저녁시간대\n",
    "    df['IS_DAWN'] = (df['HOUR'] == 1).astype(int)  # 새벽시간대\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(df, target_col, threshold_percentile=99):\n",
    "    \"\"\"이상치 제거\"\"\"\n",
    "    threshold = df[target_col].quantile(threshold_percentile / 100)\n",
    "    return df[df[target_col] <= threshold]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# D) 기상청 API 함수들 (원본 유지)\n",
    "# =========================\n",
    "def pick_base_datetime_kst(now_kst: datetime) -> tuple[str, str]:\n",
    "    today = now_kst.strftime(\"%Y%m%d\")\n",
    "    hhmm = now_kst.strftime(\"%H%M\")\n",
    "    for bt in BASE_TIMES:\n",
    "        if hhmm >= bt:\n",
    "            return today, bt\n",
    "    yday = (now_kst - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    return yday, \"2300\"\n",
    "\n",
    "\n",
    "def fetch_vilage_fcst_items(service_key: str, base_date: str, base_time: str, nx: int, ny: int,\n",
    "                            num_of_rows: int = 2000) -> list[dict]:\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_of_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(KMA_VILAGE_URL, params=params, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "    return items\n",
    "\n",
    "\n",
    "def parse_pcp_to_float(pcp_val) -> float:\n",
    "    if pcp_val is None:\n",
    "        return 0.0\n",
    "    s = str(pcp_val).strip()\n",
    "    if s in (\"강수없음\", \"없음\", \"\"):\n",
    "        return 0.0\n",
    "    s = s.replace(\"mm\", \"\").replace(\"미만\", \"\").strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def get_temp_rain_for_datetime(service_key: str, dong: str, target_yyyymmdd: str, target_hhmm: str) -> tuple[float, float]:\n",
    "    if not DONG_TO_GRID:\n",
    "        if os.path.exists(DONG_GRID_CSV):\n",
    "            g = pd.read_csv(DONG_GRID_CSV)\n",
    "            for _, row in g.iterrows():\n",
    "                DONG_TO_GRID[str(row[\"DONG\"])] = (int(row[\"nx\"]), int(row[\"ny\"]))\n",
    "        else:\n",
    "            raise ValueError(f\"[필수] {DONG_GRID_CSV}를 만들거나 DONG_TO_GRID dict를 채워주세요.\")\n",
    "\n",
    "    if dong not in DONG_TO_GRID:\n",
    "        raise ValueError(f\"DONG_TO_GRID에 '{dong}'의 nx/ny가 없습니다.\")\n",
    "\n",
    "    nx, ny = DONG_TO_GRID[dong]\n",
    "    now_kst = datetime.now(timezone(timedelta(hours=9)))\n",
    "    base_date, base_time = pick_base_datetime_kst(now_kst)\n",
    "    items = fetch_vilage_fcst_items(service_key, base_date, base_time, nx, ny)\n",
    "\n",
    "    tmp = None\n",
    "    pcp = None\n",
    "    for it in items:\n",
    "        if it.get(\"fcstDate\") == target_yyyymmdd and it.get(\"fcstTime\") == target_hhmm:\n",
    "            cat = it.get(\"category\")\n",
    "            if cat == \"TMP\":\n",
    "                tmp = float(it.get(\"fcstValue\"))\n",
    "            elif cat == \"PCP\":\n",
    "                pcp = parse_pcp_to_float(it.get(\"fcstValue\"))\n",
    "\n",
    "    if tmp is None:\n",
    "        tmp = 0.0\n",
    "    if pcp is None:\n",
    "        pcp = 0.0\n",
    "    return tmp, pcp\n",
    "\n",
    "\n",
    "def get_hourly_weather_features(service_key: str, dong: str, yyyymmdd: str) -> pd.DataFrame:\n",
    "    day_1to7 = ymd_to_day_1to7(yyyymmdd)\n",
    "    rows = []\n",
    "    for hour_slot in range(1, 11):\n",
    "        rep_time = HOUR_SLOT_TO_REP_TIME[hour_slot]\n",
    "        temp, rain = get_temp_rain_for_datetime(service_key, dong, yyyymmdd, rep_time)\n",
    "        rows.append({\n",
    "            \"TA_YMD\": int(yyyymmdd),\n",
    "            \"DONG\": dong,\n",
    "            \"HOUR\": hour_slot,\n",
    "            \"DAY\": day_1to7,\n",
    "            \"TEMP\": float(temp),\n",
    "            \"RAIN\": float(rain),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return create_features(df)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# E) 개선된 전처리 파이프라인\n",
    "# =========================\n",
    "def build_preprocess():\n",
    "    \"\"\"개선된 전처리 - 더 많은 피처 활용\"\"\"\n",
    "    cat_cols = [\"DONG\", \"TEMP_BIN\", \"SEASON\"]\n",
    "    num_cols = [\"DAY\", \"TEMP\", \"RAIN\", \"TEMP_SQUARED\", \"RAIN_LOG\",\n",
    "                \"IS_WEEKEND\", \"IS_HOT\", \"IS_COLD\", \"IS_OPTIMAL\",\n",
    "                \"IS_LUNCH\", \"IS_DINNER\", \"IS_DAWN\"]\n",
    "    \n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", StandardScaler(), num_cols),  # 스케일링 추가\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "# =========================\n",
    "# F) 개선된 모델 학습\n",
    "# =========================\n",
    "def train_and_save_models(random_state: int = 42):\n",
    "    df = pd.read_csv(DATA_CSV)\n",
    "    \n",
    "    # 타입 정리\n",
    "    df[\"DONG\"] = df[\"DONG\"].astype(str)\n",
    "    df[\"HOUR\"] = df[\"HOUR\"].astype(int)\n",
    "    df[\"DAY\"] = df[\"DAY\"].astype(int)\n",
    "    df[\"TEMP\"] = df[\"TEMP\"].astype(float)\n",
    "    df[\"RAIN\"] = df[\"RAIN\"].astype(float)\n",
    "    \n",
    "    # 피처 생성\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # 무한대 및 NaN 제거\n",
    "    print(f\"피처 생성 후: {len(df):,}건\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna()\n",
    "    print(f\"무한대/NaN 제거 후: {len(df):,}건\")\n",
    "    \n",
    "    # 이상치 제거 (AMT 기준 상위 1%)\n",
    "    df = remove_outliers(df, \"AMT\", threshold_percentile=99)\n",
    "    print(f\"이상치 제거 후: {len(df):,}건\")\n",
    "    \n",
    "    y_cols = [\"AMT\", \"CNT\"]\n",
    "    metrics = []\n",
    "\n",
    "    for h in range(1, 11):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"HOUR {h:02d} 모델 학습 중...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        sub = df[df[\"HOUR\"] == h].copy()\n",
    "        sub = sub.dropna(subset=[\"DONG\", \"DAY\", \"TEMP\", \"RAIN\", \"AMT\", \"CNT\"])\n",
    "        \n",
    "        # 피처 선택\n",
    "        feature_cols = [\"DONG\", \"DAY\", \"TEMP\", \"RAIN\", \"TEMP_BIN\", \"SEASON\",\n",
    "                       \"TEMP_SQUARED\", \"RAIN_LOG\", \"IS_WEEKEND\", \"IS_HOT\", \n",
    "                       \"IS_COLD\", \"IS_OPTIMAL\", \"IS_LUNCH\", \"IS_DINNER\", \"IS_DAWN\"]\n",
    "        \n",
    "        X = sub[feature_cols]\n",
    "        y = sub[y_cols]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        pre = build_preprocess()\n",
    "        \n",
    "        if h == 1:\n",
    "            # HOUR=1: 개선된 MLPRegressor\n",
    "            base_model = MLPRegressor(\n",
    "                hidden_layer_sizes=(256, 128, 64),  # 더 깊은 네트워크\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                alpha=1e-4,\n",
    "                batch_size=128,  # 배치 크기 감소\n",
    "                learning_rate=\"adaptive\",  # 적응형 학습률\n",
    "                learning_rate_init=5e-4,\n",
    "                max_iter=500,  # 더 많은 에포크\n",
    "                random_state=42,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.1,\n",
    "                n_iter_no_change=20,\n",
    "                tol=1e-5,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            # HOUR=2~10: 최적화된 XGBRegressor\n",
    "            base_model = XGBRegressor(\n",
    "                n_estimators=800,  # 트리 개수\n",
    "                learning_rate=0.04,  # 학습률\n",
    "                max_depth=7,  # 깊이\n",
    "                min_child_weight=3,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,  # L1 정규화\n",
    "                reg_lambda=1.5,  # L2 정규화\n",
    "                gamma=0.1,  # 최소 손실 감소\n",
    "                objective=\"reg:squarederror\",\n",
    "                random_state=random_state,\n",
    "                tree_method=\"hist\",\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        \n",
    "        model = MultiOutputRegressor(base_model)\n",
    "        \n",
    "        pipe = Pipeline([\n",
    "            (\"pre\", pre),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "        \n",
    "        # 모델 학습\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        # 예측 및 평가\n",
    "        pred = pipe.predict(X_test)\n",
    "        \n",
    "        # AMT 평가\n",
    "        mae_amt = mean_absolute_error(y_test[\"AMT\"].values, pred[:, 0])\n",
    "        r2_amt = r2_score(y_test[\"AMT\"].values, pred[:, 0])\n",
    "        rmse_amt = np.sqrt(mean_squared_error(y_test[\"AMT\"].values, pred[:, 0]))\n",
    "        \n",
    "        # CNT 평가\n",
    "        mae_cnt = mean_absolute_error(y_test[\"CNT\"].values, pred[:, 1])\n",
    "        r2_cnt = r2_score(y_test[\"CNT\"].values, pred[:, 1])\n",
    "        rmse_cnt = np.sqrt(mean_squared_error(y_test[\"CNT\"].values, pred[:, 1]))\n",
    "        \n",
    "        # 모델 저장\n",
    "        model_path = os.path.join(MODEL_DIR, f\"hour_{h:02d}_amt_cnt.joblib\")\n",
    "        joblib.dump(pipe, model_path)\n",
    "        \n",
    "        metrics.append({\n",
    "            \"HOUR\": h,\n",
    "            \"MAE_AMT\": mae_amt,\n",
    "            \"R2_AMT\": r2_amt,\n",
    "            \"RMSE_AMT\": rmse_amt,\n",
    "            \"MAE_CNT\": mae_cnt,\n",
    "            \"R2_CNT\": r2_cnt,\n",
    "            \"RMSE_CNT\": rmse_cnt,\n",
    "            \"model_path\": model_path,\n",
    "            \"model_type\": \"MLP(Deep)\" if h == 1 else \"XGB\",\n",
    "            \"n_train\": len(X_train),\n",
    "            \"n_test\": len(X_test),\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n[HOUR {h:02d}] 저장 완료 -> {model_path}\")\n",
    "        print(f\"  AMT - MAE: {mae_amt:,.1f} | R²: {r2_amt:.4f} | RMSE: {rmse_amt:,.1f}\")\n",
    "        print(f\"  CNT - MAE: {mae_cnt:,.2f} | R²: {r2_cnt:.4f} | RMSE: {rmse_cnt:,.2f}\")\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics).sort_values(\"HOUR\")\n",
    "    metrics_df.to_csv(os.path.join(MODEL_DIR, \"metrics_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"전체 모델 성능 요약\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nAMT 평균 성능:\")\n",
    "    print(f\"  - MAE: {metrics_df['MAE_AMT'].mean():,.1f}\")\n",
    "    print(f\"  - R²: {metrics_df['R2_AMT'].mean():.4f}\")\n",
    "    print(f\"\\nCNT 평균 성능:\")\n",
    "    print(f\"  - MAE: {metrics_df['MAE_CNT'].mean():.2f}\")\n",
    "    print(f\"  - R²: {metrics_df['R2_CNT'].mean():.4f}\")\n",
    "    print(\"\\n=== metrics_summary.csv 저장 완료 ===\\n\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# G) 예측 함수 (원본 유지하되 피처 추가)\n",
    "# =========================\n",
    "def load_models():\n",
    "    models = {}\n",
    "    for h in range(1, 11):\n",
    "        p = os.path.join(MODEL_DIR, f\"hour_{h:02d}_amt_cnt.joblib\")\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"모델 파일이 없습니다: {p}\")\n",
    "        models[h] = joblib.load(p)\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_day(yyyymmdd: str, dong: str) -> pd.DataFrame:\n",
    "    # 기상청에서 시간대별 TEMP/RAIN 구성 (피처 포함)\n",
    "    feat_df = get_hourly_weather_features(SERVICE_KEY, dong, yyyymmdd)\n",
    "    \n",
    "    models = load_models()\n",
    "    \n",
    "    outs = []\n",
    "    for h in range(1, 11):\n",
    "        row_data = feat_df[feat_df[\"HOUR\"] == h]\n",
    "        \n",
    "        # 필요한 피처만 선택\n",
    "        feature_cols = [\"DONG\", \"DAY\", \"TEMP\", \"RAIN\", \"TEMP_BIN\", \"SEASON\",\n",
    "                       \"TEMP_SQUARED\", \"RAIN_LOG\", \"IS_WEEKEND\", \"IS_HOT\", \n",
    "                       \"IS_COLD\", \"IS_OPTIMAL\", \"IS_LUNCH\", \"IS_DINNER\", \"IS_DAWN\"]\n",
    "        row = row_data[feature_cols]\n",
    "        \n",
    "        pred_amt, pred_cnt = models[h].predict(row)[0]\n",
    "        \n",
    "        outs.append({\n",
    "            \"TA_YMD\": yyyymmdd,\n",
    "            \"DONG\": dong,\n",
    "            \"HOUR\": h,\n",
    "            \"TEMP\": float(row_data[\"TEMP\"].iloc[0]),\n",
    "            \"RAIN\": float(row_data[\"RAIN\"].iloc[0]),\n",
    "            \"PRED_AMT\": max(0, float(pred_amt)),  # 음수 방지\n",
    "            \"PRED_CNT\": max(0, float(pred_cnt)),  # 음수 방지\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24535028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# G) 실행 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 학습 + 저장\n",
    "    print(\"=\"*60)\n",
    "    print(\"모델 학습 시작\")\n",
    "    print(\"=\"*60)\n",
    "    metrics_df = train_and_save_models()\n",
    "    print(\"\\n최종 성능 지표:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "\n",
    "    # 2) 예측 예시\n",
    "    if not SERVICE_KEY:\n",
    "        print(\"\\n[경고] 환경변수 RAIN_ID가 비었습니다. 예측을 하려면 SERVICE_KEY를 넣으세요.\\n\")\n",
    "    else:\n",
    "        try:\n",
    "            yyyymmdd = \"20251230\"\n",
    "            dong = \"매교동\"\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"예측 예시: {dong}, {yyyymmdd}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            pred_df = predict_day(yyyymmdd, dong)\n",
    "            print(pred_df.to_string(index=False))\n",
    "            \n",
    "            output_file = f\"pred_{dong}_{yyyymmdd}.csv\"\n",
    "            pred_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"\\n예측 결과 저장: {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n예측 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ee77ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HOUR 01 학습\n",
      "============================================================\n",
      "[HOUR 01] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_01_bundle.joblib\n",
      "  CNT - MAE 18.82 | R2 0.7004 | RMSE 68.10\n",
      "  AMT - MAE 816,196.0 | R2 0.8010 | RMSE 1,785,111.3\n",
      "\n",
      "============================================================\n",
      "HOUR 02 학습\n",
      "============================================================\n",
      "[HOUR 02] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_02_bundle.joblib\n",
      "  CNT - MAE 8.92 | R2 0.8807 | RMSE 15.63\n",
      "  AMT - MAE 288,859.0 | R2 0.3426 | RMSE 1,009,549.0\n",
      "\n",
      "============================================================\n",
      "HOUR 03 학습\n",
      "============================================================\n",
      "[HOUR 03] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_03_bundle.joblib\n",
      "  CNT - MAE 18.14 | R2 0.8473 | RMSE 31.59\n",
      "  AMT - MAE 656,629.0 | R2 0.4790 | RMSE 1,764,263.3\n",
      "\n",
      "============================================================\n",
      "HOUR 04 학습\n",
      "============================================================\n",
      "[HOUR 04] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_04_bundle.joblib\n",
      "  CNT - MAE 49.45 | R2 0.9024 | RMSE 92.13\n",
      "  AMT - MAE 1,387,803.1 | R2 0.8945 | RMSE 2,638,367.1\n",
      "\n",
      "============================================================\n",
      "HOUR 05 학습\n",
      "============================================================\n",
      "[HOUR 05] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_05_bundle.joblib\n",
      "  CNT - MAE 42.15 | R2 0.8766 | RMSE 83.23\n",
      "  AMT - MAE 1,508,768.1 | R2 0.8754 | RMSE 2,755,842.7\n",
      "\n",
      "============================================================\n",
      "HOUR 06 학습\n",
      "============================================================\n",
      "[HOUR 06] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_06_bundle.joblib\n",
      "  CNT - MAE 35.14 | R2 0.7922 | RMSE 89.19\n",
      "  AMT - MAE 1,181,445.0 | R2 0.7984 | RMSE 2,651,470.0\n",
      "\n",
      "============================================================\n",
      "HOUR 07 학습\n",
      "============================================================\n",
      "[HOUR 07] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_07_bundle.joblib\n",
      "  CNT - MAE 41.56 | R2 0.8116 | RMSE 96.63\n",
      "  AMT - MAE 1,387,054.6 | R2 0.8430 | RMSE 2,641,768.2\n",
      "\n",
      "============================================================\n",
      "HOUR 08 학습\n",
      "============================================================\n",
      "[HOUR 08] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_08_bundle.joblib\n",
      "  CNT - MAE 42.71 | R2 0.7888 | RMSE 99.86\n",
      "  AMT - MAE 1,866,421.8 | R2 0.8735 | RMSE 3,206,067.3\n",
      "\n",
      "============================================================\n",
      "HOUR 09 학습\n",
      "============================================================\n",
      "[HOUR 09] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_09_bundle.joblib\n",
      "  CNT - MAE 34.48 | R2 0.5753 | RMSE 135.82\n",
      "  AMT - MAE 1,225,038.8 | R2 0.8528 | RMSE 2,373,285.2\n",
      "\n",
      "============================================================\n",
      "HOUR 10 학습\n",
      "============================================================\n",
      "[HOUR 10] 저장 완료 -> data/models_hourly_amt_cnt_2stage_te\\hour_10_bundle.joblib\n",
      "  CNT - MAE 12.38 | R2 0.6316 | RMSE 45.82\n",
      "  AMT - MAE 529,410.9 | R2 0.7112 | RMSE 1,356,377.0\n",
      "\n",
      "=== metrics_summary.csv 저장 완료: data/models_hourly_amt_cnt_2stage_te\\metrics_summary.csv ===\n",
      "   HOUR       MAE_AMT    R2_AMT      RMSE_AMT    MAE_CNT    R2_CNT  \\\n",
      "0     1  8.161960e+05  0.800996  1.785111e+06  18.824849  0.700436   \n",
      "1     2  2.888590e+05  0.342618  1.009549e+06   8.920131  0.880664   \n",
      "2     3  6.566290e+05  0.479041  1.764263e+06  18.137866  0.847293   \n",
      "3     4  1.387803e+06  0.894513  2.638367e+06  49.452803  0.902410   \n",
      "4     5  1.508768e+06  0.875360  2.755843e+06  42.154494  0.876609   \n",
      "5     6  1.181445e+06  0.798420  2.651470e+06  35.144356  0.792242   \n",
      "6     7  1.387055e+06  0.843009  2.641768e+06  41.555471  0.811629   \n",
      "7     8  1.866422e+06  0.873524  3.206067e+06  42.714221  0.788793   \n",
      "8     9  1.225039e+06  0.852827  2.373285e+06  34.477600  0.575265   \n",
      "9    10  5.294109e+05  0.711215  1.356377e+06  12.382925  0.631595   \n",
      "\n",
      "     RMSE_CNT  n_train  n_test  \\\n",
      "0   68.095159    33699    8425   \n",
      "1   15.625818    33998    8500   \n",
      "2   31.587086    35704    8927   \n",
      "3   92.126772    35981    8996   \n",
      "4   83.232568    35698    8925   \n",
      "5   89.189456    35994    8999   \n",
      "6   96.631697    35887    8972   \n",
      "7   99.858181    35305    8827   \n",
      "8  135.822386    35752    8939   \n",
      "9   45.816178    33247    8312   \n",
      "\n",
      "                                          model_path  \n",
      "0  data/models_hourly_amt_cnt_2stage_te\\hour_01_b...  \n",
      "1  data/models_hourly_amt_cnt_2stage_te\\hour_02_b...  \n",
      "2  data/models_hourly_amt_cnt_2stage_te\\hour_03_b...  \n",
      "3  data/models_hourly_amt_cnt_2stage_te\\hour_04_b...  \n",
      "4  data/models_hourly_amt_cnt_2stage_te\\hour_05_b...  \n",
      "5  data/models_hourly_amt_cnt_2stage_te\\hour_06_b...  \n",
      "6  data/models_hourly_amt_cnt_2stage_te\\hour_07_b...  \n",
      "7  data/models_hourly_amt_cnt_2stage_te\\hour_08_b...  \n",
      "8  data/models_hourly_amt_cnt_2stage_te\\hour_09_b...  \n",
      "9  data/models_hourly_amt_cnt_2stage_te\\hour_10_b...  \n",
      "CPU times: total: 13min 43s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "수원시 한식 동별 데이터 - 시간대별 AMT/CNT 예측 (MAE 개선 반영)\n",
    "✅ Target Encoding (AMT 기준, 누수 방지: train 기준 mapping)\n",
    "✅ 시계열 Train/Test split (TA_YMD 정렬 후 마지막 20% 테스트)\n",
    "✅ HOUR별 XGB 파라미터 분리\n",
    "✅ CNT → AMT 2-Stage (AMT 입력에 PRED_CNT 포함, train에선 OOF PRED_CNT로 누수 방지)\n",
    "✅ 기상청 API 429 방지: \"하루 예측 시 API 1회 호출\" + 429 backoff 재시도\n",
    "\n",
    "[학습]\n",
    "- train_and_save_models() 실행 → MODEL_DIR에 hour_01_bundle.joblib ~ hour_10_bundle.joblib 저장\n",
    "\n",
    "[예측]\n",
    "- predict_day(\"YYYYMMDD\", \"행궁동\") → 시간대별 PRED_AMT, PRED_CNT 반환\n",
    "\n",
    "필수 파일\n",
    "- DATA_CSV: data/수원시 한식 동별 데이터백업.csv  (TA_YMD,DONG,HOUR,DAY,TEMP,RAIN,AMT,CNT 포함)\n",
    "- DONG_GRID_CSV: data/dong_grid.csv  (DONG,nx,ny 포함)\n",
    "- .env에 RAIN_ID(서비스키) 또는 환경변수 RAIN_ID 설정\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# =========================\n",
    "# A) 설정\n",
    "# =========================\n",
    "load_dotenv()\n",
    "SERVICE_KEY = os.getenv(\"RAIN_ID\")  # 기상청 단기예보 서비스키 (필수)\n",
    "\n",
    "DATA_CSV = \"data/수원시 한식 동별 데이터백업.csv\"\n",
    "MODEL_DIR = \"data/models_hourly_amt_cnt_2stage_te\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ---- 기상청 단기예보(getVilageFcst) 엔드포인트 ----\n",
    "KMA_VILAGE_URL = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "BASE_TIMES = [\"2300\", \"2000\", \"1700\", \"1400\", \"1100\", \"0800\", \"0500\", \"0200\"]\n",
    "\n",
    "# HOUR(01~10) 구간 대표 시각\n",
    "HOUR_SLOT_TO_REP_TIME = {\n",
    "    1: \"0300\", 2: \"0800\", 3: \"1000\", 4: \"1200\", 5: \"1400\",\n",
    "    6: \"1600\", 7: \"1800\", 8: \"2000\", 9: \"2200\", 10: \"2300\",\n",
    "}\n",
    "\n",
    "# ---- DONG -> (nx, ny) 격자 좌표 매핑 ----\n",
    "DONG_GRID_CSV = \"data/dong_grid.csv\"\n",
    "DONG_TO_GRID = {}  # lazy load\n",
    "\n",
    "\n",
    "# =========================\n",
    "# B) 유틸: 요일 계산 (1~7, 월=1 ... 일=7)\n",
    "# =========================\n",
    "def ymd_to_day_1to7(yyyymmdd: str) -> int:\n",
    "    dt = datetime.strptime(str(yyyymmdd), \"%Y%m%d\")\n",
    "    return dt.weekday() + 1\n",
    "\n",
    "\n",
    "# =========================\n",
    "# C) 피처 엔지니어링\n",
    "# =========================\n",
    "def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    파생 피처:\n",
    "    - TEMP_BIN, IS_WEEKEND, SEASON\n",
    "    - TEMP_SQUARED, RAIN_LOG\n",
    "    - IS_HOT/IS_COLD/IS_OPTIMAL\n",
    "    - 시간대 플래그(IS_LUNCH/IS_DINNER/IS_DAWN)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"TEMP_BIN\"] = pd.cut(\n",
    "        df[\"TEMP\"],\n",
    "        bins=[-100, -7, 1, 10, 18, 27, 100],\n",
    "        labels=[\"very_cold\", \"cold\", \"cool\", \"pleasant\", \"warm\", \"hot\"],\n",
    "    ).astype(str)\n",
    "\n",
    "    df[\"IS_WEEKEND\"] = (df[\"DAY\"] >= 6).astype(int)\n",
    "\n",
    "    df[\"MONTH\"] = df[\"TA_YMD\"].astype(str).str[4:6].astype(int)\n",
    "    df[\"SEASON\"] = df[\"MONTH\"].apply(lambda m:\n",
    "        \"spring\" if 3 <= m <= 5 else\n",
    "        \"summer\" if 6 <= m <= 8 else\n",
    "        \"fall\"   if 9 <= m <= 11 else\n",
    "        \"winter\"\n",
    "    )\n",
    "\n",
    "    df[\"TEMP_SQUARED\"] = df[\"TEMP\"] ** 2\n",
    "\n",
    "    df[\"IS_HOT\"] = (df[\"TEMP\"] >= 27).astype(int)\n",
    "    df[\"IS_COLD\"] = ((df[\"TEMP\"] >= -7) & (df[\"TEMP\"] <= 1)).astype(int)\n",
    "    df[\"IS_OPTIMAL\"] = ((df[\"TEMP\"] >= 10) & (df[\"TEMP\"] <= 14)).astype(int)\n",
    "\n",
    "    df[\"RAIN_CLIPPED\"] = df[\"RAIN\"].clip(lower=0)\n",
    "    df[\"RAIN_LOG\"] = np.log1p(df[\"RAIN_CLIPPED\"])\n",
    "\n",
    "    df[\"IS_LUNCH\"] = (df[\"HOUR\"] == 4).astype(int)\n",
    "    df[\"IS_DINNER\"] = (df[\"HOUR\"].isin([7, 8])).astype(int)\n",
    "    df[\"IS_DAWN\"] = (df[\"HOUR\"] == 1).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame, target_col: str, threshold_percentile: int = 99) -> pd.DataFrame:\n",
    "    thr = df[target_col].quantile(threshold_percentile / 100)\n",
    "    return df[df[target_col] <= thr]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# D) 기상청 API (429 방지: 1회 호출 + backoff)\n",
    "# =========================\n",
    "def pick_base_datetime_kst(now_kst: datetime) -> tuple[str, str]:\n",
    "    today = now_kst.strftime(\"%Y%m%d\")\n",
    "    hhmm = now_kst.strftime(\"%H%M\")\n",
    "    for bt in BASE_TIMES:\n",
    "        if hhmm >= bt:\n",
    "            return today, bt\n",
    "    yday = (now_kst - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    return yday, \"2300\"\n",
    "\n",
    "\n",
    "def fetch_vilage_fcst_items(\n",
    "    service_key: str, base_date: str, base_time: str, nx: int, ny: int,\n",
    "    num_of_rows: int = 2000, max_retries: int = 6\n",
    ") -> list[dict]:\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_of_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "\n",
    "    wait = 1.0\n",
    "    last_status = None\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(KMA_VILAGE_URL, params=params, timeout=20)\n",
    "        last_status = r.status_code\n",
    "\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(wait)\n",
    "            wait = min(wait * 2, 30.0)\n",
    "            continue\n",
    "\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return data[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "\n",
    "    raise requests.HTTPError(f\"HTTP {last_status}: Too Many Requests(429) - 재시도 {max_retries}회 실패\")\n",
    "\n",
    "\n",
    "def parse_pcp_to_float(pcp_val) -> float:\n",
    "    if pcp_val is None:\n",
    "        return 0.0\n",
    "    s = str(pcp_val).strip()\n",
    "    if s in (\"강수없음\", \"없음\", \"\"):\n",
    "        return 0.0\n",
    "    s = s.replace(\"mm\", \"\").replace(\"미만\", \"\").strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def _load_dong_grid_if_needed():\n",
    "    global DONG_TO_GRID\n",
    "    if DONG_TO_GRID:\n",
    "        return\n",
    "    if not os.path.exists(DONG_GRID_CSV):\n",
    "        raise ValueError(f\"[필수] {DONG_GRID_CSV} 파일이 없습니다. (DONG,nx,ny 필요)\")\n",
    "    g = pd.read_csv(DONG_GRID_CSV)\n",
    "    if not {\"DONG\", \"nx\", \"ny\"}.issubset(set(g.columns)):\n",
    "        raise ValueError(f\"[오류] {DONG_GRID_CSV} 컬럼이 DONG,nx,ny를 포함해야 합니다. 현재={list(g.columns)}\")\n",
    "    for _, row in g.iterrows():\n",
    "        DONG_TO_GRID[str(row[\"DONG\"])] = (int(row[\"nx\"]), int(row[\"ny\"]))\n",
    "\n",
    "\n",
    "def get_vilage_items_once_for_day(service_key: str, dong: str) -> dict:\n",
    "    \"\"\"\n",
    "    예보 items를 한 번만 받아서 (fcstDate, fcstTime) -> {TMP, PCP} 형태 dict로 캐시화\n",
    "    \"\"\"\n",
    "    _load_dong_grid_if_needed()\n",
    "\n",
    "    if dong not in DONG_TO_GRID:\n",
    "        raise ValueError(f\"DONG_GRID에 '{dong}'의 nx/ny가 없습니다.\")\n",
    "\n",
    "    nx, ny = DONG_TO_GRID[dong]\n",
    "    now_kst = datetime.now(timezone(timedelta(hours=9)))\n",
    "    base_date, base_time = pick_base_datetime_kst(now_kst)\n",
    "\n",
    "    items = fetch_vilage_fcst_items(service_key, base_date, base_time, nx, ny)\n",
    "\n",
    "    d = {}\n",
    "    for it in items:\n",
    "        fcstDate = it.get(\"fcstDate\")\n",
    "        fcstTime = it.get(\"fcstTime\")\n",
    "        cat = it.get(\"category\")\n",
    "        val = it.get(\"fcstValue\")\n",
    "        key = (fcstDate, fcstTime)\n",
    "        if key not in d:\n",
    "            d[key] = {}\n",
    "        d[key][cat] = val\n",
    "    return d\n",
    "\n",
    "\n",
    "def get_hourly_weather_features(service_key: str, dong: str, yyyymmdd: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ✅ 예측 1회당 API 1회 호출\n",
    "    \"\"\"\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)가 비어있습니다. .env 또는 환경변수에 RAIN_ID를 설정하세요.\")\n",
    "\n",
    "    day_1to7 = ymd_to_day_1to7(yyyymmdd)\n",
    "    items_dict = get_vilage_items_once_for_day(service_key, dong)\n",
    "\n",
    "    rows = []\n",
    "    for hour_slot in range(1, 11):\n",
    "        rep_time = HOUR_SLOT_TO_REP_TIME[hour_slot]\n",
    "        key = (yyyymmdd, rep_time)\n",
    "\n",
    "        tmp = 0.0\n",
    "        pcp = 0.0\n",
    "        if key in items_dict:\n",
    "            if \"TMP\" in items_dict[key]:\n",
    "                try:\n",
    "                    tmp = float(items_dict[key][\"TMP\"])\n",
    "                except Exception:\n",
    "                    tmp = 0.0\n",
    "            if \"PCP\" in items_dict[key]:\n",
    "                pcp = parse_pcp_to_float(items_dict[key][\"PCP\"])\n",
    "\n",
    "        rows.append({\n",
    "            \"TA_YMD\": int(yyyymmdd),\n",
    "            \"DONG\": dong,\n",
    "            \"HOUR\": hour_slot,\n",
    "            \"DAY\": day_1to7,\n",
    "            \"TEMP\": float(tmp),\n",
    "            \"RAIN\": float(pcp),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return create_features(df)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# E) Target Encoding (AMT 기준, 누수 방지)\n",
    "# =========================\n",
    "class DongTargetEncoder:\n",
    "    \"\"\"\n",
    "    DONG -> AMT mean encoding\n",
    "    fit: train 데이터로 dong별 평균 계산\n",
    "    transform: 매핑 적용, 없으면 global mean\n",
    "    \"\"\"\n",
    "    def __init__(self, target_col: str = \"AMT\", dong_col: str = \"DONG\", out_col: str = \"DONG_TE\"):\n",
    "        self.target_col = target_col\n",
    "        self.dong_col = dong_col\n",
    "        self.out_col = out_col\n",
    "        self.mapping_ = {}\n",
    "        self.global_mean_ = 0.0\n",
    "\n",
    "    def fit(self, train_df: pd.DataFrame):\n",
    "        if self.dong_col not in train_df.columns or self.target_col not in train_df.columns:\n",
    "            raise ValueError(f\"TE fit에 {self.dong_col},{self.target_col} 컬럼이 필요합니다.\")\n",
    "        self.global_mean_ = float(train_df[self.target_col].mean())\n",
    "        self.mapping_ = train_df.groupby(self.dong_col)[self.target_col].mean().to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X2 = X.copy()\n",
    "        X2[self.out_col] = X2[self.dong_col].map(self.mapping_).fillna(self.global_mean_)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================\n",
    "# F) 전처리\n",
    "# - DONG은 OneHot 하지 않고 DONG_TE 사용\n",
    "# =========================\n",
    "def build_preprocess(use_pred_cnt: bool) -> ColumnTransformer:\n",
    "    cat_cols = [\"TEMP_BIN\", \"SEASON\"]\n",
    "    num_cols = [\n",
    "        \"DAY\", \"TEMP\", \"RAIN\",\n",
    "        \"TEMP_SQUARED\", \"RAIN_LOG\",\n",
    "        \"IS_WEEKEND\", \"IS_HOT\", \"IS_COLD\", \"IS_OPTIMAL\",\n",
    "        \"IS_LUNCH\", \"IS_DINNER\", \"IS_DAWN\",\n",
    "        \"DONG_TE\",\n",
    "    ]\n",
    "    if use_pred_cnt:\n",
    "        num_cols += [\"PRED_CNT\"]\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", StandardScaler(), num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "# =========================\n",
    "# G) HOUR별 XGB 파라미터 분리\n",
    "# =========================\n",
    "def get_xgb_params_cnt(h: int, random_state: int = 42) -> dict:\n",
    "    # CNT는 상대적으로 안정적: 과적합 방지 쪽\n",
    "    if h in [4, 5, 7, 8]:  # 피크\n",
    "        return dict(\n",
    "            n_estimators=900, learning_rate=0.04, max_depth=7,\n",
    "            subsample=0.85, colsample_bytree=0.85,\n",
    "            min_child_weight=3, reg_alpha=0.0, reg_lambda=1.2, gamma=0.0,\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=random_state, tree_method=\"hist\", n_jobs=-1\n",
    "        )\n",
    "    return dict(\n",
    "        n_estimators=700, learning_rate=0.05, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        min_child_weight=4, reg_alpha=0.0, reg_lambda=1.3, gamma=0.0,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=random_state, tree_method=\"hist\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def get_xgb_params_amt(h: int, random_state: int = 42) -> dict:\n",
    "    # AMT는 변동이 큼: 표현력 조금 더\n",
    "    if h in [4, 5]:  # 점심\n",
    "        return dict(\n",
    "            n_estimators=1200, learning_rate=0.03, max_depth=8,\n",
    "            subsample=0.85, colsample_bytree=0.85,\n",
    "            min_child_weight=3, reg_alpha=0.05, reg_lambda=1.2, gamma=0.05,\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=random_state, tree_method=\"hist\", n_jobs=-1\n",
    "        )\n",
    "    if h in [7, 8]:  # 저녁\n",
    "        return dict(\n",
    "            n_estimators=1400, learning_rate=0.028, max_depth=9,\n",
    "            subsample=0.85, colsample_bytree=0.85,\n",
    "            min_child_weight=3, reg_alpha=0.05, reg_lambda=1.3, gamma=0.08,\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=random_state, tree_method=\"hist\", n_jobs=-1\n",
    "        )\n",
    "    return dict(\n",
    "        n_estimators=900, learning_rate=0.04, max_depth=7,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        min_child_weight=4, reg_alpha=0.05, reg_lambda=1.5, gamma=0.05,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=random_state, tree_method=\"hist\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# H) 학습/저장 (2-stage + TE + 시계열 split)\n",
    "# =========================\n",
    "def train_and_save_models(\n",
    "    random_state: int = 42,\n",
    "    test_ratio: float = 0.2,\n",
    "    oof_folds: int = 5,\n",
    "    outlier_percentile: int = 99\n",
    ") -> pd.DataFrame:\n",
    "    if not os.path.exists(DATA_CSV):\n",
    "        raise FileNotFoundError(f\"데이터 파일이 없습니다: {DATA_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "    required_cols = {\"TA_YMD\",\"DONG\",\"HOUR\",\"DAY\",\"TEMP\",\"RAIN\",\"AMT\",\"CNT\"}\n",
    "    if not required_cols.issubset(set(df.columns)):\n",
    "        raise ValueError(f\"DATA_CSV에 필요한 컬럼이 부족합니다. 필요={sorted(required_cols)} / 현재={list(df.columns)}\")\n",
    "\n",
    "    # 타입 정리\n",
    "    df[\"TA_YMD\"] = df[\"TA_YMD\"].astype(int)\n",
    "    df[\"DONG\"] = df[\"DONG\"].astype(str)\n",
    "    df[\"HOUR\"] = df[\"HOUR\"].astype(int)\n",
    "    df[\"DAY\"] = df[\"DAY\"].astype(int)\n",
    "    df[\"TEMP\"] = df[\"TEMP\"].astype(float)\n",
    "    df[\"RAIN\"] = df[\"RAIN\"].astype(float)\n",
    "    df[\"AMT\"] = df[\"AMT\"].astype(float)\n",
    "    df[\"CNT\"] = df[\"CNT\"].astype(float)\n",
    "\n",
    "    # 피처 생성\n",
    "    df = create_features(df)\n",
    "\n",
    "    # 무한/NaN 제거\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # 이상치 제거 (AMT 기준 상위 1% 등)\n",
    "    df = remove_outliers(df, \"AMT\", threshold_percentile=outlier_percentile)\n",
    "\n",
    "    feature_base = [\n",
    "        \"TA_YMD\", \"DONG\", \"DAY\", \"TEMP\", \"RAIN\", \"TEMP_BIN\", \"SEASON\",\n",
    "        \"TEMP_SQUARED\", \"RAIN_LOG\", \"IS_WEEKEND\", \"IS_HOT\", \"IS_COLD\",\n",
    "        \"IS_OPTIMAL\", \"IS_LUNCH\", \"IS_DINNER\", \"IS_DAWN\"\n",
    "    ]\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for h in range(1, 11):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"HOUR {h:02d} 학습\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        sub = df[df[\"HOUR\"] == h].copy()\n",
    "        sub = sub.sort_values(\"TA_YMD\")\n",
    "\n",
    "        if len(sub) < 200:\n",
    "            print(f\"[경고] HOUR {h:02d} 데이터가 너무 적습니다: n={len(sub)} (그래도 진행)\")\n",
    "        if len(sub) < (oof_folds + 2):\n",
    "            raise ValueError(f\"HOUR {h:02d} 데이터가 너무 적어 OOF fold 불가: n={len(sub)}\")\n",
    "\n",
    "        # ✅ 시계열 split\n",
    "        split = int(len(sub) * (1 - test_ratio))\n",
    "        train_df = sub.iloc[:split].copy()\n",
    "        test_df  = sub.iloc[split:].copy()\n",
    "\n",
    "        # ✅ Target Encoding (AMT 기준, train만으로 fit)\n",
    "        te = DongTargetEncoder(target_col=\"AMT\", dong_col=\"DONG\", out_col=\"DONG_TE\").fit(train_df)\n",
    "\n",
    "        train_X = te.transform(train_df[feature_base].copy())\n",
    "        test_X  = te.transform(test_df[feature_base].copy())\n",
    "\n",
    "        y_train_cnt = train_df[\"CNT\"].values\n",
    "        y_test_cnt  = test_df[\"CNT\"].values\n",
    "        y_train_amt = train_df[\"AMT\"].values\n",
    "        y_test_amt  = test_df[\"AMT\"].values\n",
    "\n",
    "        # -------- (1) CNT 모델 --------\n",
    "        pre_cnt = build_preprocess(use_pred_cnt=False)\n",
    "        pipe_cnt = Pipeline([\n",
    "            (\"pre\", pre_cnt),\n",
    "            (\"model\", XGBRegressor(**get_xgb_params_cnt(h, random_state=random_state))),\n",
    "        ])\n",
    "\n",
    "        # -------- (2) AMT 입력용 OOF PRED_CNT (누수 방지) --------\n",
    "        # 시계열 느낌 유지: shuffle=False\n",
    "        kf = KFold(n_splits=oof_folds, shuffle=False)\n",
    "        train_X_reset = train_X.reset_index(drop=True)\n",
    "\n",
    "        oof_pred_cnt = np.zeros(len(train_X_reset), dtype=float)\n",
    "\n",
    "        for tr_idx, va_idx in kf.split(train_X_reset):\n",
    "            X_tr = train_X_reset.iloc[tr_idx]\n",
    "            X_va = train_X_reset.iloc[va_idx]\n",
    "            y_tr = y_train_cnt[tr_idx]\n",
    "\n",
    "            tmp_pipe = Pipeline([\n",
    "                (\"pre\", pre_cnt),\n",
    "                (\"model\", XGBRegressor(**get_xgb_params_cnt(h, random_state=random_state))),\n",
    "            ])\n",
    "            tmp_pipe.fit(X_tr, y_tr)\n",
    "            oof_pred_cnt[va_idx] = tmp_pipe.predict(X_va)\n",
    "\n",
    "        # CNT 최종 fit (전체 train)\n",
    "        pipe_cnt.fit(train_X, y_train_cnt)\n",
    "\n",
    "        # test용 pred_cnt\n",
    "        pred_cnt_test = pipe_cnt.predict(test_X)\n",
    "\n",
    "        # -------- (3) AMT 모델 (PRED_CNT 포함) --------\n",
    "        train_X_amt = train_X.copy()\n",
    "        test_X_amt  = test_X.copy()\n",
    "        train_X_amt[\"PRED_CNT\"] = oof_pred_cnt\n",
    "        test_X_amt[\"PRED_CNT\"] = pred_cnt_test\n",
    "\n",
    "        pre_amt = build_preprocess(use_pred_cnt=True)\n",
    "        pipe_amt = Pipeline([\n",
    "            (\"pre\", pre_amt),\n",
    "            (\"model\", XGBRegressor(**get_xgb_params_amt(h, random_state=random_state))),\n",
    "        ])\n",
    "        pipe_amt.fit(train_X_amt, y_train_amt)\n",
    "\n",
    "        # -------- (4) 평가 --------\n",
    "        pred_cnt = pipe_cnt.predict(test_X)\n",
    "        pred_amt = pipe_amt.predict(test_X_amt)\n",
    "\n",
    "        mae_cnt = mean_absolute_error(y_test_cnt, pred_cnt)\n",
    "        r2_cnt  = r2_score(y_test_cnt, pred_cnt)\n",
    "        rmse_cnt = np.sqrt(mean_squared_error(y_test_cnt, pred_cnt))\n",
    "\n",
    "        mae_amt = mean_absolute_error(y_test_amt, pred_amt)\n",
    "        r2_amt  = r2_score(y_test_amt, pred_amt)\n",
    "        rmse_amt = np.sqrt(mean_squared_error(y_test_amt, pred_amt))\n",
    "\n",
    "        # -------- (5) 저장 --------\n",
    "        bundle = {\n",
    "            \"hour\": h,\n",
    "            \"te_mapping\": te.mapping_,\n",
    "            \"te_global_mean\": te.global_mean_,\n",
    "            \"feature_base\": feature_base,\n",
    "            \"pipe_cnt\": pipe_cnt,\n",
    "            \"pipe_amt\": pipe_amt,\n",
    "        }\n",
    "        model_path = os.path.join(MODEL_DIR, f\"hour_{h:02d}_bundle.joblib\")\n",
    "        joblib.dump(bundle, model_path)\n",
    "\n",
    "        metrics.append({\n",
    "            \"HOUR\": h,\n",
    "            \"MAE_AMT\": mae_amt,\n",
    "            \"R2_AMT\": r2_amt,\n",
    "            \"RMSE_AMT\": rmse_amt,\n",
    "            \"MAE_CNT\": mae_cnt,\n",
    "            \"R2_CNT\": r2_cnt,\n",
    "            \"RMSE_CNT\": rmse_cnt,\n",
    "            \"n_train\": len(train_df),\n",
    "            \"n_test\": len(test_df),\n",
    "            \"model_path\": model_path,\n",
    "        })\n",
    "\n",
    "        print(f\"[HOUR {h:02d}] 저장 완료 -> {model_path}\")\n",
    "        print(f\"  CNT - MAE {mae_cnt:,.2f} | R2 {r2_cnt:.4f} | RMSE {rmse_cnt:,.2f}\")\n",
    "        print(f\"  AMT - MAE {mae_amt:,.1f} | R2 {r2_amt:.4f} | RMSE {rmse_amt:,.1f}\")\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics).sort_values(\"HOUR\")\n",
    "    out_csv = os.path.join(MODEL_DIR, \"metrics_summary.csv\")\n",
    "    metrics_df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n=== metrics_summary.csv 저장 완료: {out_csv} ===\")\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# I) 로드/예측\n",
    "# =========================\n",
    "def load_bundles() -> dict:\n",
    "    bundles = {}\n",
    "    for h in range(1, 11):\n",
    "        p = os.path.join(MODEL_DIR, f\"hour_{h:02d}_bundle.joblib\")\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(\n",
    "                f\"모델 번들 파일이 없습니다: {p}\\n\"\n",
    "                f\"먼저 train_and_save_models()를 실행해서 모델을 저장하세요.\"\n",
    "            )\n",
    "        bundles[h] = joblib.load(p)\n",
    "    return bundles\n",
    "\n",
    "\n",
    "def apply_te_from_bundle(bundle: dict, X: pd.DataFrame) -> pd.DataFrame:\n",
    "    mp = bundle[\"te_mapping\"]\n",
    "    gm = bundle[\"te_global_mean\"]\n",
    "    X2 = X.copy()\n",
    "    X2[\"DONG_TE\"] = X2[\"DONG\"].map(mp).fillna(gm)\n",
    "    return X2\n",
    "\n",
    "\n",
    "def predict_day(yyyymmdd: str, dong: str) -> pd.DataFrame:\n",
    "    # 1) 기상/피처 (✅ API 1회)\n",
    "    feat_df = get_hourly_weather_features(SERVICE_KEY, dong, yyyymmdd)\n",
    "\n",
    "    # 2) 모델 번들 로드\n",
    "    bundles = load_bundles()\n",
    "\n",
    "    outs = []\n",
    "    for h in range(1, 11):\n",
    "        row = feat_df[feat_df[\"HOUR\"] == h].copy()\n",
    "        bundle = bundles[h]\n",
    "        feature_base = bundle[\"feature_base\"]\n",
    "\n",
    "        X = row[feature_base].copy()\n",
    "        X[\"DONG\"] = dong  # 안전\n",
    "\n",
    "        # TE 적용\n",
    "        X = apply_te_from_bundle(bundle, X)\n",
    "\n",
    "        # CNT 예측\n",
    "        pred_cnt = float(bundle[\"pipe_cnt\"].predict(X)[0])\n",
    "\n",
    "        # AMT 예측 (PRED_CNT 추가)\n",
    "        X_amt = X.copy()\n",
    "        X_amt[\"PRED_CNT\"] = pred_cnt\n",
    "        pred_amt = float(bundle[\"pipe_amt\"].predict(X_amt)[0])\n",
    "\n",
    "        outs.append({\n",
    "            \"TA_YMD\": str(yyyymmdd),\n",
    "            \"DONG\": dong,\n",
    "            \"HOUR\": h,\n",
    "            \"TEMP\": float(row[\"TEMP\"].iloc[0]),\n",
    "            \"RAIN\": float(row[\"RAIN\"].iloc[0]),\n",
    "            \"PRED_CNT\": int(max(0, round(pred_cnt))),\n",
    "            \"PRED_AMT\": int(max(0, round(pred_amt))),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(outs)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# J) 실행 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # ✅ 1) 학습이 먼저입니다. (최초 1회)\n",
    "    metrics = train_and_save_models()\n",
    "    print(metrics)\n",
    "\n",
    "    # ✅ 2) 학습 후 예측\n",
    "#     df_pred = predict_day(\"20260102\", \"행궁동\")\n",
    "#     print(df_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d372adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TA_YMD  DONG  HOUR  TEMP  RAIN  PRED_CNT  PRED_AMT\n",
      "0  20260103  세류1동     1  -8.0   0.0         7    250257\n",
      "1  20260103  세류1동     2  -8.0   0.0         2         0\n",
      "2  20260103  세류1동     3  -5.0   0.0         8    124071\n",
      "3  20260103  세류1동     4  -1.0   0.0         0     29494\n",
      "4  20260103  세류1동     5   1.0   0.0        13    206055\n",
      "5  20260103  세류1동     6   2.0   0.0        10    287139\n",
      "6  20260103  세류1동     7   0.0   0.0        31     25590\n",
      "7  20260103  세류1동     8  -1.0   0.0        10    390020\n",
      "8  20260103  세류1동     9  -1.0   0.0         0     80202\n",
      "9  20260103  세류1동    10   0.0   0.0         1    152991\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_pred = predict_day(\"20260103\", \"세류1동\")\n",
    "    print(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e778c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TA_YMD DONG  HOUR  TEMP  RAIN  PRED_CNT  PRED_AMT\n",
      "0  20260103  매산동     1  -8.0   0.0       433   9834727\n",
      "1  20260103  매산동     2  -8.0   0.0       210   1108525\n",
      "2  20260103  매산동     3  -5.0   0.0       425   5161708\n",
      "3  20260103  매산동     4  -1.0   0.0       845  19481378\n",
      "4  20260103  매산동     5   1.0   0.0       915  36747444\n",
      "5  20260103  매산동     6   2.0   0.0       768  38014476\n",
      "6  20260103  매산동     7   0.0   0.0       872  27775718\n",
      "7  20260103  매산동     8  -1.0   0.0       839  34793352\n",
      "8  20260103  매산동     9  -1.0   0.0       893  24592880\n",
      "9  20260103  매산동    10   0.0   0.0       399   7321910\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_pred = predict_day(\"20260103\", \"매산동\")\n",
    "    print(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc72f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b961c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e87b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a2a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c5811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3652970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp (ipykernel)",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
