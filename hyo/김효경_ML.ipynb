{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353ae70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.text_cell_render.rendered_html{font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.text_cell_render.rendered_html{font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc8040",
   "metadata": {},
   "source": [
    "# íŒ¨í‚¤ì§€ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9f3fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d33be",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ë°±ì—…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9187c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101513 entries, 0 to 101512\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   TA_YMD  101513 non-null  int64  \n",
      " 1   HOUR    101513 non-null  int64  \n",
      " 2   DAY     101513 non-null  int64  \n",
      " 3   DONG    101513 non-null  object \n",
      " 4   CNT     101513 non-null  int64  \n",
      " 5   UNIT    101513 non-null  int64  \n",
      " 6   TEMP    101513 non-null  float64\n",
      " 7   RAIN    101513 non-null  float64\n",
      " 8   AMT     101513 non-null  int64  \n",
      "dtypes: float64(2), int64(6), object(1)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "sw_data = pd.read_csv('c:/Users/Admin/AI/downloads/ì¹´ë“œì†Œë¹„ë°ì´í„° ìˆ˜ì›ì‹œ/SUWON_S_DATA_TABLE_GENDER_SUM.csv')\n",
    "sw_data = sw_data[['TA_YMD', 'HOUR', 'DAY', 'DONG', 'CNT', 'UNIT', 'TEMP', 'RAIN', 'AMT']]\n",
    "sw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b68d9",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5802a50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101513, 8), (101513,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_X = sw_data.iloc[:,:-1]\n",
    "sw_y = sw_data.iloc[:,-1]\n",
    "sw_X.shape, sw_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a65234b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "sw_X['DONG_code'] = le.fit_transform(sw_X['DONG'])   # ìƒˆ ìˆ«ì ì»¬ëŸ¼\n",
    "sw_X = sw_X.drop(columns=['DONG'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d85f124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 61969.80921891931\n",
      "MAE: 52584.15421381469\n",
      "MAE: 41544.993122032734\n",
      "MAE: 43963.04344594928\n",
      "MAE: 42599.578318370295\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for train, test in tscv.split(sw_X):\n",
    "    X_train, X_test = sw_X.iloc[train], sw_X.iloc[test]\n",
    "    y_train, y_test = sw_y.iloc[train], sw_y.iloc[test]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21a48308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_sales.csv ìƒì„± ì™„ë£Œ -> c:/Users/Admin/AI/downloads/ì¹´ë“œì†Œë¹„ë°ì´í„° ìˆ˜ì›ì‹œ/store_sales.csv\n",
      "     ta_ymd      store_amt\n",
      "0  20251001  129934.283060\n",
      "1  20251002  117234.713977\n",
      "2  20251003  132953.770762\n",
      "3  20251004  174460.597128\n",
      "4  20251005  139316.932506\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# ë”ë¯¸ ê°€ê²Œ ë§¤ì¶œ ë°ì´í„° ìƒì„±\n",
    "# ---------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "# ìµœê·¼ 60ì¼ (ì‚¬ì¥ë‹˜ì´ ì…ë ¥í–ˆë‹¤ê³  ê°€ì •)\n",
    "dates = pd.date_range(\"2025-10-01\", \"2025-12-20\", freq=\"D\")\n",
    "\n",
    "# ê°€ê²Œ ì¼ë§¤ì¶œ (ì„ì˜ ìƒì„±, í˜„ì‹¤ì ì¸ ë²”ìœ„)\n",
    "# í‰ì¼ ë‚®ê³  / ì£¼ë§ ì¡°ê¸ˆ ë†’ê²Œ\n",
    "sales = []\n",
    "for d in dates:\n",
    "    base = 120_000\n",
    "    if d.weekday() >= 5:  # ì£¼ë§\n",
    "        base *= 1.2\n",
    "    noise = np.random.normal(0, 20_000)\n",
    "    sales.append(max(20_000, base + noise))\n",
    "\n",
    "store_df = pd.DataFrame({\n",
    "    \"ta_ymd\": dates.strftime(\"%Y%m%d\").astype(int),\n",
    "    \"store_amt\": sales\n",
    "})\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ (ë„ˆê°€ ì“°ë˜ ê²½ë¡œ ê·¸ëŒ€ë¡œ)\n",
    "out_path = r\"c:/Users/Admin/AI/downloads/ì¹´ë“œì†Œë¹„ë°ì´í„° ìˆ˜ì›ì‹œ/store_sales.csv\"\n",
    "store_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"store_sales.csv ìƒì„± ì™„ë£Œ -> {out_path}\")\n",
    "print(store_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12f65181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5664\n",
      "[LightGBM] [Info] Number of data points in the train set: 5624, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 16.047767\n",
      "[Model] LightGBM (rain_mean + rain_max)\n",
      "[Test] MAE   : 2,157,716\n",
      "[Test] WMAPE : 12.73%\n",
      "Saved -> c:/Users/Admin/AI/downloads/ì¹´ë“œì†Œë¹„ë°ì´í„° ìˆ˜ì›ì‹œ/models\\suwon_daily_amt_meanmax.pkl\n",
      "\n",
      "============================================================\n",
      "âœ… [ê³ ê° í™”ë©´ ì¶œë ¥ ì˜ˆì‹œ]\n",
      "============================================================\n",
      "ğŸ“ì§€ì—­(ë™): ê³ ë“±ë™\n",
      "ğŸ“…ë‚ ì§œ: 20251031\n",
      "ğŸŒ¦ï¸ë‚ ì”¨(ì¶œì²˜): DATA_WEATHER(A ê²€ì¦)\n",
      "   - í‰ê· ê¸°ì˜¨(ì¼í‰ê· ): 13.6Â°C\n",
      "   - ì‹œê°„ëŒ€ í‰ê·  ê°•ìˆ˜ëŸ‰(ì¼í‰ê· ): 0.19 mm\n",
      "   - âœ…ì‹œê°„ëŒ€ í‰ê·  ê°•ìˆ˜ëŸ‰(ì¼ìµœëŒ€, í”¼í¬): 1.50 mm  â† ê³ ê°ì—ê²Œ ë³´ì—¬ì¤„ í•µì‹¬\n",
      "ğŸ’°ì˜ˆìƒ ë™ ì „ì²´ ì¼ë§¤ì¶œ: 3,082,178 ì›\n",
      "ğŸªê°€ê²Œ ì ìœ ìœ¨(ìµœê·¼ 56ì¼): 4.635%\n",
      "ğŸªë³´ì •ê³„ìˆ˜(adj): 0.930\n",
      "ğŸªì˜ˆìƒ ê°€ê²Œ ì¼ë§¤ì¶œ: 132,851 ì›\n",
      "============================================================\n",
      "\n",
      "[RAW RESULT]\n",
      "{'dong': 'ê³ ë“±ë™', 'date': 20251031, 'temp': 13.6, 'rain_mean': 0.19, 'rain_max': 1.5, 'dong_pred': 3082177, 'share_pct': 4.635, 'store_pred': 132850, 'customer_message': 'ğŸ“ì§€ì—­(ë™): ê³ ë“±ë™\\nğŸ“…ë‚ ì§œ: 20251031\\nğŸŒ¦ï¸ë‚ ì”¨(ì¶œì²˜): DATA_WEATHER(A ê²€ì¦)\\n   - í‰ê· ê¸°ì˜¨(ì¼í‰ê· ): 13.6Â°C\\n   - ì‹œê°„ëŒ€ í‰ê·  ê°•ìˆ˜ëŸ‰(ì¼í‰ê· ): 0.19 mm\\n   - âœ…ì‹œê°„ëŒ€ í‰ê·  ê°•ìˆ˜ëŸ‰(ì¼ìµœëŒ€, í”¼í¬): 1.50 mm  â† ê³ ê°ì—ê²Œ ë³´ì—¬ì¤„ í•µì‹¬\\nğŸ’°ì˜ˆìƒ ë™ ì „ì²´ ì¼ë§¤ì¶œ: 3,082,178 ì›\\nğŸªê°€ê²Œ ì ìœ ìœ¨(ìµœê·¼ 56ì¼): 4.635%\\nğŸªë³´ì •ê³„ìˆ˜(adj): 0.930\\nğŸªì˜ˆìƒ ê°€ê²Œ ì¼ë§¤ì¶œ: 132,851 ì›'}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Option 2 (ì¶”ì²œ)\n",
    "- ëª¨ë¸ í”¼ì²˜: temp + rain_mean + rain_max + íŒŒìƒ(temp_clip, log) + calendar + holiday + lag/rolling\n",
    "- ê³ ê° ì¶œë ¥: rain_max(í”¼í¬) ì¤‘ì‹¬ìœ¼ë¡œ ì•ˆë‚´\n",
    "- A ê²€ì¦: ë°ì´í„° ë²”ìœ„ ë‚´(<= daily_max_ymd)ë©´ ë°ì´í„°ì—ì„œ ë‚ ì”¨ ì‚¬ìš©\n",
    "- B ìš´ì˜ ì¤€ë¹„: ë°ì´í„° ë²”ìœ„ ë°–ì´ë©´ getVilageFcstë¡œ temp/rain_mean/rain_max ìƒì„±(ì¤€ë¹„ë§Œ)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import joblib\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    dong_data_path: str = r\"c:/Users/Admin/AI/downloads/ì¹´ë“œì†Œë¹„ë°ì´í„° ìˆ˜ì›ì‹œ/SUWON_S_DATA_TABLE_GENDER_SUM.csv\"\n",
    "    store_sales_path: str = r\"c:/Users/Admin/AI/downloads/ì¹´ë“œì†Œë¹„ë°ì´í„° ìˆ˜ì›ì‹œ/store_sales.csv\"\n",
    "\n",
    "    model_out_dir: str = r\"c:/Users/Admin/AI/downloads/ì¹´ë“œì†Œë¹„ë°ì´í„° ìˆ˜ì›ì‹œ/models\"\n",
    "    model_name: str = \"suwon_daily_amt_meanmax.pkl\"\n",
    "\n",
    "    train_end: int = 20241231\n",
    "    test_start: int = 20250101\n",
    "\n",
    "    lags: tuple = (1, 7, 14, 28)\n",
    "    rolling_windows: tuple = (7, 14, 28)\n",
    "\n",
    "    use_log_target: bool = True\n",
    "\n",
    "    share_window_days: int = 56\n",
    "    min_overlap_days: int = 7\n",
    "\n",
    "    # simple adjustment\n",
    "    adj_weekend: float = 1.05\n",
    "    adj_holiday: float = 1.08\n",
    "    adj_rain: float = 0.93\n",
    "\n",
    "    # KMA key env name\n",
    "    service_key_env: str = \"RAIN_ID\"\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# (ì˜ˆë³´ìš©) nx, ny ë§¤í•‘ (ì¼ë‹¨ ë™ì¼)\n",
    "DONG_TO_GRID = {k: (61, 121) for k in [\"ê³ ë“±ë™\",\"ì¸ê³„ë™\",\"í–‰ê¶ë™\",\"ë§¤ì‚°ë™\",\"ë§¤êµë™\",\"ìš°ë§Œ1ë™\",\"ìš°ë§Œ2ë™\",\"ì§€ë™\",\"í™”ì„œ1ë™\",\"í™”ì„œ2ë™\"]}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Metrics\n",
    "# =========================\n",
    "def wmape(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    denom = np.maximum(np.sum(np.abs(y_true)), eps)\n",
    "    return float(np.sum(np.abs(y_true - y_pred)) / denom * 100.0)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Holidays (2023-01-01 ~ 2025-10-31)\n",
    "# =========================\n",
    "def holiday_set_2023_2025_upto_oct31() -> set[pd.Timestamp]:\n",
    "    dates = []\n",
    "    dates += [\"2023-01-01\",\"2023-01-21\",\"2023-01-22\",\"2023-01-23\",\"2023-01-24\",\"2023-03-01\",\n",
    "              \"2023-05-05\",\"2023-05-27\",\"2023-05-29\",\"2023-06-06\",\"2023-08-15\",\"2023-09-28\",\n",
    "              \"2023-09-29\",\"2023-09-30\",\"2023-10-03\",\"2023-10-09\",\"2023-12-25\"]\n",
    "    dates += [\"2024-01-01\",\"2024-02-09\",\"2024-02-10\",\"2024-02-11\",\"2024-02-12\",\"2024-03-01\",\n",
    "              \"2024-05-05\",\"2024-05-06\",\"2024-05-15\",\"2024-06-06\",\"2024-08-15\",\"2024-09-16\",\n",
    "              \"2024-09-17\",\"2024-09-18\",\"2024-10-03\",\"2024-10-09\",\"2024-12-25\"]\n",
    "    dates += [\"2025-01-01\",\"2025-01-28\",\"2025-01-29\",\"2025-01-30\",\"2025-03-01\",\"2025-03-03\",\n",
    "              \"2025-05-05\",\"2025-05-06\",\"2025-06-06\",\"2025-08-15\",\n",
    "              \"2025-10-03\",\"2025-10-05\",\"2025-10-06\",\"2025-10-07\",\"2025-10-08\",\"2025-10-09\"]\n",
    "    s = set(pd.to_datetime(dates))\n",
    "    start, end = pd.Timestamp(\"2023-01-01\"), pd.Timestamp(\"2025-10-31\")\n",
    "    return {d for d in s if start <= d <= end}\n",
    "\n",
    "HOLIDAYS = holiday_set_2023_2025_upto_oct31()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def ymd_int_to_ts(ymd: int) -> pd.Timestamp:\n",
    "    return pd.to_datetime(str(int(ymd)), format=\"%Y%m%d\")\n",
    "\n",
    "def make_calendar_features(date: pd.Timestamp) -> dict:\n",
    "    # âœ… 0~6 ì²´ê³„ë¡œ ê³ ì • (ì¤‘ë³µ/í˜¼ì„  ë°©ì§€)\n",
    "    dow = int(date.dayofweek)  # 0=Mon ... 6=Sun\n",
    "    is_weekend = int(dow in [5, 6])\n",
    "    m = date.month\n",
    "    return {\n",
    "        \"dow\": dow,\n",
    "        \"is_weekend\": is_weekend,\n",
    "        \"year\": date.year,\n",
    "        \"month\": m,\n",
    "        \"day\": date.day,\n",
    "        \"weekofyear\": int(date.isocalendar().week),\n",
    "        \"month_sin\": float(np.sin(2*np.pi*m/12.0)),\n",
    "        \"month_cos\": float(np.cos(2*np.pi*m/12.0)),\n",
    "        \"dow_sin\": float(np.sin(2*np.pi*dow/7.0)),\n",
    "        \"dow_cos\": float(np.cos(2*np.pi*dow/7.0)),\n",
    "    }\n",
    "\n",
    "def make_holiday_features(date: pd.Timestamp) -> dict:\n",
    "    is_h = int(date in HOLIDAYS)\n",
    "    is_before = int((date + pd.Timedelta(days=1)) in HOLIDAYS)\n",
    "    is_after  = int((date - pd.Timedelta(days=1)) in HOLIDAYS)\n",
    "    around = int(any((date + pd.Timedelta(days=k)) in HOLIDAYS for k in [-2,-1,0,1,2]))\n",
    "    return {\n",
    "        \"is_holiday\": is_h,\n",
    "        \"is_before_holiday\": is_before,\n",
    "        \"is_after_holiday\": is_after,\n",
    "        \"is_around_holiday_2d\": around\n",
    "    }\n",
    "\n",
    "def make_weather_derived_features(temp: float, rain_mean: float, rain_max: float) -> dict:\n",
    "    \"\"\"âœ… ì›ë³¸ temp/rain_mean/rain_maxëŠ” dailyì— ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” 'íŒŒìƒ'ë§Œ ë§Œë“ ë‹¤.\"\"\"\n",
    "    temp = float(temp)\n",
    "    rain_mean = max(0.0, float(rain_mean))\n",
    "    rain_max = max(0.0, float(rain_max))\n",
    "    return {\n",
    "        \"temp_clip\": float(np.clip(temp, -20, 45)),\n",
    "        \"is_rain\": int(max(rain_mean, rain_max) > 0),\n",
    "        \"rain_mean_log1p\": float(np.log1p(rain_mean)),\n",
    "        \"rain_max_log1p\": float(np.log1p(rain_max)),\n",
    "    }\n",
    "\n",
    "def adjust_factor(is_weekend: bool, is_holiday: bool, is_rain: bool, cfg: Config) -> float:\n",
    "    adj = 1.0\n",
    "    if is_weekend: adj *= cfg.adj_weekend\n",
    "    if is_holiday: adj *= cfg.adj_holiday\n",
    "    if is_rain:    adj *= cfg.adj_rain\n",
    "    return float(adj)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Load raw + daily aggregation\n",
    "# =========================\n",
    "def load_raw(cfg: Config) -> pd.DataFrame:\n",
    "    df = pd.read_csv(cfg.dong_data_path)\n",
    "    rename_map = {\n",
    "        \"TA_YMD\": \"ta_ymd\",\n",
    "        \"HOUR\": \"hour\",\n",
    "        \"DAY\": \"dow_raw\",\n",
    "        \"DONG\": \"dong\",\n",
    "        \"AMT\": \"amt\",\n",
    "        \"CNT\": \"cnt\",\n",
    "        \"UNIT\": \"unit\",\n",
    "        \"TEMP\": \"temp\",\n",
    "        \"RAIN\": \"rain\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    needed = {\"ta_ymd\",\"hour\",\"dong\",\"amt\",\"cnt\",\"unit\",\"temp\",\"rain\"}\n",
    "    miss = needed - set(df.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing columns: {miss}\")\n",
    "\n",
    "    df[\"ta_ymd\"] = pd.to_numeric(df[\"ta_ymd\"], errors=\"raise\").astype(int)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"ta_ymd\"].astype(str), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    df[\"hour\"] = pd.to_numeric(df[\"hour\"], errors=\"coerce\").astype(int)\n",
    "    df[\"dong\"] = df[\"dong\"].astype(str)\n",
    "\n",
    "    for c in [\"amt\",\"cnt\",\"unit\",\"temp\",\"rain\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    return df.sort_values([\"dong\",\"date\",\"hour\"]).reset_index(drop=True)\n",
    "\n",
    "def make_daily(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - amt: ì¼í•©\n",
    "    - cnt: ì¼í•©\n",
    "    - temp: ì¼í‰ê· \n",
    "    - rain: ì‹œê°„ëŒ€ í‰ê·  ê°•ìˆ˜ëŸ‰(ë„¤ ì •ì˜) -> ì¼í‰ê· (rain_mean), ì¼ìµœëŒ€(rain_max)\n",
    "    \"\"\"\n",
    "    daily = df.groupby([\"dong\",\"ta_ymd\",\"date\"], as_index=False).agg(\n",
    "        amt=(\"amt\",\"sum\"),\n",
    "        cnt=(\"cnt\",\"sum\"),\n",
    "        temp=(\"temp\",\"mean\"),\n",
    "        rain_mean=(\"rain\",\"mean\"),\n",
    "        rain_max=(\"rain\",\"max\"),\n",
    "    )\n",
    "    daily[\"unit\"] = (daily[\"amt\"] / daily[\"cnt\"].replace(0, np.nan)).fillna(0)\n",
    "    return daily.sort_values([\"dong\",\"ta_ymd\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) lag/rolling\n",
    "# =========================\n",
    "def add_lag_rolling(daily: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    g = daily.groupby(\"dong\", sort=False)\n",
    "\n",
    "    for lag in cfg.lags:\n",
    "        daily[f\"amt_lag_{lag}\"] = g[\"amt\"].shift(lag)\n",
    "        daily[f\"cnt_lag_{lag}\"] = g[\"cnt\"].shift(lag)\n",
    "        daily[f\"unit_lag_{lag}\"] = g[\"unit\"].shift(lag)\n",
    "\n",
    "    for w in cfg.rolling_windows:\n",
    "        minp = max(3, w//3)\n",
    "        daily[f\"amt_roll_{w}_mean\"] = g[\"amt\"].shift(1).rolling(w, min_periods=minp).mean()\n",
    "        daily[f\"amt_roll_{w}_std\"]  = g[\"amt\"].shift(1).rolling(w, min_periods=minp).std()\n",
    "\n",
    "    return daily\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) dataset build (âœ… ì¤‘ë³µ ì»¬ëŸ¼ ì™„ì „ ë°©ì§€)\n",
    "# =========================\n",
    "def build_dataset(daily: pd.DataFrame, cfg: Config):\n",
    "    cal_df = pd.DataFrame(list(daily[\"date\"].apply(make_calendar_features)))\n",
    "    hol_df = pd.DataFrame(list(daily[\"date\"].apply(make_holiday_features)))\n",
    "    wea_df = pd.DataFrame(list(daily.apply(lambda r: make_weather_derived_features(r[\"temp\"], r[\"rain_mean\"], r[\"rain_max\"]), axis=1)))\n",
    "\n",
    "    out = pd.concat([daily.reset_index(drop=True), cal_df, hol_df, wea_df], axis=1)\n",
    "\n",
    "    out = add_lag_rolling(out, cfg)\n",
    "\n",
    "    # âœ… ì¤‘ë³µ ì»¬ëŸ¼ ê²€ì‚¬ (ë¬¸ì œ ìˆìœ¼ë©´ ì—¬ê¸°ì„œ ë°”ë¡œ ì•Œë¦¼)\n",
    "    dup = out.columns[out.columns.duplicated()].tolist()\n",
    "    if dup:\n",
    "        raise ValueError(f\"Duplicate columns detected: {dup}\")\n",
    "\n",
    "    feature_cols = [\n",
    "        \"dong\",\n",
    "        # calendar\n",
    "        \"dow\",\"is_weekend\",\"year\",\"month\",\"day\",\"weekofyear\",\n",
    "        \"month_sin\",\"month_cos\",\"dow_sin\",\"dow_cos\",\n",
    "        # raw weather\n",
    "        \"temp\",\n",
    "        \"rain_mean\",\"rain_max\",\n",
    "        # derived weather\n",
    "        \"temp_clip\",\"is_rain\",\"rain_mean_log1p\",\"rain_max_log1p\",\n",
    "        # holiday\n",
    "        \"is_holiday\",\"is_before_holiday\",\"is_after_holiday\",\"is_around_holiday_2d\",\n",
    "    ]\n",
    "    for lag in cfg.lags:\n",
    "        feature_cols += [f\"amt_lag_{lag}\", f\"cnt_lag_{lag}\", f\"unit_lag_{lag}\"]\n",
    "    for w in cfg.rolling_windows:\n",
    "        feature_cols += [f\"amt_roll_{w}_mean\", f\"amt_roll_{w}_std\"]\n",
    "\n",
    "    data = out.dropna(subset=feature_cols + [\"amt\"]).copy()\n",
    "\n",
    "    if cfg.use_log_target:\n",
    "        data[\"target\"] = np.log1p(data[\"amt\"])\n",
    "    else:\n",
    "        data[\"target\"] = data[\"amt\"]\n",
    "\n",
    "    return data, feature_cols\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) train (LightGBM)\n",
    "# =========================\n",
    "def train_and_eval(data: pd.DataFrame, feature_cols, cfg: Config):\n",
    "    train = data[data[\"ta_ymd\"] <= cfg.train_end].copy()\n",
    "    test  = data[data[\"ta_ymd\"] >= cfg.test_start].copy()\n",
    "\n",
    "    if train.empty or test.empty:\n",
    "        raise ValueError(\"train/test split empty. ë‚ ì§œ ë²”ìœ„ í™•ì¸\")\n",
    "\n",
    "    # time-aware validation split\n",
    "    tr_sorted = train.sort_values(\"ta_ymd\")\n",
    "    split = int(len(tr_sorted) * 0.8)\n",
    "    tr_idx = tr_sorted.index[:split]\n",
    "    va_idx = tr_sorted.index[split:]\n",
    "\n",
    "    X_train = train.loc[tr_idx, feature_cols].copy()\n",
    "    y_train = train.loc[tr_idx, \"target\"].values\n",
    "    X_valid = train.loc[va_idx, feature_cols].copy()\n",
    "    y_valid = train.loc[va_idx, \"target\"].values\n",
    "\n",
    "    X_test = test[feature_cols].copy()\n",
    "    y_test = test[\"target\"].values\n",
    "\n",
    "    # categorical\n",
    "    X_train[\"dong\"] = X_train[\"dong\"].astype(\"category\")\n",
    "    X_valid[\"dong\"] = X_valid[\"dong\"].astype(\"category\")\n",
    "    X_test[\"dong\"] = X_test[\"dong\"].astype(\"category\")\n",
    "\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=63,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric=\"l1\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    if cfg.use_log_target:\n",
    "        y_true = np.expm1(y_test)\n",
    "        y_pred = np.expm1(pred)\n",
    "    else:\n",
    "        y_true = y_test\n",
    "        y_pred = pred\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    w = wmape(y_true, y_pred)\n",
    "\n",
    "    print(\"[Model] LightGBM (rain_mean + rain_max)\")\n",
    "    print(f\"[Test] MAE   : {mae:,.0f}\")\n",
    "    print(f\"[Test] WMAPE : {w:.2f}%\")\n",
    "\n",
    "    os.makedirs(cfg.model_out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(cfg.model_out_dir, cfg.model_name)\n",
    "    joblib.dump({\"model\": model, \"feature_cols\": feature_cols, \"cfg\": cfg}, out_path)\n",
    "    print(f\"Saved -> {out_path}\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) (B ì¤€ë¹„) forecast weather (VilageFcst)\n",
    "# =========================\n",
    "def _get_service_key(cfg: Config) -> str:\n",
    "    k = os.getenv(cfg.service_key_env)\n",
    "    if not k:\n",
    "        raise ValueError(f\"í™˜ê²½ë³€ìˆ˜ {cfg.service_key_env}ê°€ ì—†ìŠµë‹ˆë‹¤. .envì— {cfg.service_key_env}=... ë„£ì–´ì£¼ì„¸ìš”.\")\n",
    "    return k\n",
    "\n",
    "def _pick_base_datetime_kst():\n",
    "    now = pd.Timestamp.now(tz=\"Asia/Seoul\")\n",
    "    base_times = [\"2300\",\"2000\",\"1700\",\"1400\",\"1100\",\"0800\",\"0500\",\"0200\"]\n",
    "    for bt in base_times:\n",
    "        cand = pd.Timestamp(f\"{now.strftime('%Y-%m-%d')} {bt[:2]}:{bt[2:]}\", tz=\"Asia/Seoul\")\n",
    "        if now >= cand + pd.Timedelta(minutes=40):\n",
    "            return now.strftime(\"%Y%m%d\"), bt\n",
    "    yday = (now - pd.Timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    return yday, \"2300\"\n",
    "\n",
    "def _parse_pcp(v: str) -> float:\n",
    "    if v is None: return 0.0\n",
    "    v = str(v).strip()\n",
    "    if v == \"\" or \"ê°•ìˆ˜ì—†ìŒ\" in v: return 0.0\n",
    "    if \"ë¯¸ë§Œ\" in v: return 0.5\n",
    "    if \"~\" in v: v = v.split(\"~\")[0]\n",
    "    m = re.findall(r\"[-+]?\\d*\\.?\\d+\", v)\n",
    "    return float(m[0]) if m else 0.0\n",
    "\n",
    "def get_forecast_weather(dong: str, target_ymd: int, cfg: Config):\n",
    "    sk = _get_service_key(cfg)\n",
    "    base_date, base_time = _pick_base_datetime_kst()\n",
    "\n",
    "    if dong not in DONG_TO_GRID:\n",
    "        raise ValueError(f\"ë™ nx/ny ë§¤í•‘ ì—†ìŒ: {dong}\")\n",
    "\n",
    "    nx, ny = DONG_TO_GRID[dong]\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    full_url = f\"{url}?serviceKey={sk}\"\n",
    "\n",
    "    params = {\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": 2000,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": int(nx),\n",
    "        \"ny\": int(ny),\n",
    "    }\n",
    "\n",
    "    r = requests.get(full_url, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    header = data[\"response\"][\"header\"]\n",
    "    if header.get(\"resultCode\") != \"00\":\n",
    "        raise ValueError(f\"KMA Vilage error: {header.get('resultMsg')} (code={header.get('resultCode')})\")\n",
    "\n",
    "    items = data[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "\n",
    "    temps, pcps = [], []\n",
    "    for it in items:\n",
    "        if it.get(\"fcstDate\") != str(target_ymd):\n",
    "            continue\n",
    "        if it.get(\"category\") == \"TMP\":\n",
    "            temps.append(float(it.get(\"fcstValue\")))\n",
    "        elif it.get(\"category\") == \"PCP\":\n",
    "            pcps.append(_parse_pcp(it.get(\"fcstValue\")))\n",
    "\n",
    "    if not temps:\n",
    "        raise ValueError(f\"ì˜ˆë³´(TMP) ì—†ìŒ: target={target_ymd} base={base_date}{base_time}\")\n",
    "\n",
    "    temp = float(np.mean(temps))\n",
    "    rain_mean = float(np.mean(pcps)) if pcps else 0.0\n",
    "    rain_max = float(np.max(pcps)) if pcps else 0.0\n",
    "    return temp, rain_mean, rain_max\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) store share + inference + customer demo\n",
    "# =========================\n",
    "def load_store_sales(path: str) -> pd.DataFrame:\n",
    "    s = pd.read_csv(path)\n",
    "    if \"ta_ymd\" not in s.columns or \"store_amt\" not in s.columns:\n",
    "        raise ValueError(\"store_sales.csv needs columns: ta_ymd, store_amt\")\n",
    "    s[\"ta_ymd\"] = pd.to_numeric(s[\"ta_ymd\"], errors=\"coerce\").astype(int)\n",
    "    s[\"date\"] = pd.to_datetime(s[\"ta_ymd\"].astype(str), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    s[\"store_amt\"] = pd.to_numeric(s[\"store_amt\"], errors=\"coerce\")\n",
    "    return s.dropna(subset=[\"store_amt\"]).sort_values(\"ta_ymd\").reset_index(drop=True)\n",
    "\n",
    "def compute_share(daily: pd.DataFrame, store: pd.DataFrame, dong: str, target_ymd: int, cfg: Config) -> float:\n",
    "    end_date = ymd_int_to_ts(target_ymd) - pd.Timedelta(days=1)\n",
    "    start_date = end_date - pd.Timedelta(days=cfg.share_window_days)\n",
    "\n",
    "    dong_hist = daily[(daily[\"dong\"]==dong) & (daily[\"date\"]>=start_date) & (daily[\"date\"]<=end_date)][[\"date\",\"amt\"]]\n",
    "    store_hist = store[(store[\"date\"]>=start_date) & (store[\"date\"]<=end_date)][[\"date\",\"store_amt\"]]\n",
    "    merged = dong_hist.merge(store_hist, on=\"date\", how=\"inner\")\n",
    "\n",
    "    if len(merged) < cfg.min_overlap_days:\n",
    "        return 0.0\n",
    "\n",
    "    denom = merged[\"amt\"].sum()\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    share = float(merged[\"store_amt\"].sum() / denom)\n",
    "    return float(np.clip(share, 0.0, 0.2))\n",
    "\n",
    "def build_one_row_for_predict(daily: pd.DataFrame, dong: str, target_ymd: int,\n",
    "                              temp: float, rain_mean: float, rain_max: float, cfg: Config,\n",
    "                              feature_cols: list) -> pd.DataFrame:\n",
    "    # dongë³„ ê³¼ê±°\n",
    "    h = daily[daily[\"dong\"]==dong].copy().sort_values(\"ta_ymd\")\n",
    "\n",
    "    target_date = ymd_int_to_ts(target_ymd)\n",
    "    cal = make_calendar_features(target_date)\n",
    "    hol = make_holiday_features(target_date)\n",
    "    wea_der = make_weather_derived_features(temp, rain_mean, rain_max)\n",
    "\n",
    "    # target row ì¶”ê°€\n",
    "    row = {\n",
    "        \"dong\": dong,\n",
    "        \"ta_ymd\": int(target_ymd),\n",
    "        \"date\": target_date,\n",
    "        \"amt\": np.nan,\n",
    "        \"cnt\": np.nan,\n",
    "        \"unit\": np.nan,\n",
    "        \"temp\": float(temp),\n",
    "        \"rain_mean\": float(rain_mean),\n",
    "        \"rain_max\": float(rain_max),\n",
    "    }\n",
    "    tmp = pd.concat([h, pd.DataFrame([row])], ignore_index=True).sort_values(\"ta_ymd\").reset_index(drop=True)\n",
    "\n",
    "    # lag/rolling\n",
    "    g = tmp.groupby(\"dong\", sort=False)\n",
    "    for lag in cfg.lags:\n",
    "        tmp[f\"amt_lag_{lag}\"] = g[\"amt\"].shift(lag)\n",
    "        tmp[f\"cnt_lag_{lag}\"] = g[\"cnt\"].shift(lag)\n",
    "        tmp[f\"unit_lag_{lag}\"] = g[\"unit\"].shift(lag)\n",
    "\n",
    "    for w in cfg.rolling_windows:\n",
    "        minp = max(3, w//3)\n",
    "        tmp[f\"amt_roll_{w}_mean\"] = g[\"amt\"].shift(1).rolling(w, min_periods=minp).mean()\n",
    "        tmp[f\"amt_roll_{w}_std\"]  = g[\"amt\"].shift(1).rolling(w, min_periods=minp).std()\n",
    "\n",
    "    last = tmp.iloc[[-1]].copy()\n",
    "    # attach derived features\n",
    "    for k,v in cal.items(): last[k] = v\n",
    "    for k,v in hol.items(): last[k] = v\n",
    "    for k,v in wea_der.items(): last[k] = v\n",
    "\n",
    "    # final duplicate check\n",
    "    dup = last.columns[last.columns.duplicated()].tolist()\n",
    "    if dup:\n",
    "        raise ValueError(f\"Duplicate columns in inference row: {dup}\")\n",
    "\n",
    "    return last[feature_cols].copy()\n",
    "\n",
    "def predict_customer_sales(dong: str, target_ymd: int, model_pack_path: str, cfg: Config):\n",
    "    pack = joblib.load(model_pack_path)\n",
    "    model = pack[\"model\"]\n",
    "    feature_cols = pack[\"feature_cols\"]\n",
    "\n",
    "    raw = load_raw(cfg)\n",
    "    daily = make_daily(raw)\n",
    "    daily_max_ymd = int(daily[\"ta_ymd\"].max())\n",
    "\n",
    "    # A ê²€ì¦: ë°ì´í„° ë‚ ì”¨ ì‚¬ìš© / B ìš´ì˜ ì¤€ë¹„: ì˜ˆë³´ ì‚¬ìš©\n",
    "    if target_ymd <= daily_max_ymd:\n",
    "        row = daily[(daily[\"dong\"]==dong) & (daily[\"ta_ymd\"]==int(target_ymd))]\n",
    "        if row.empty:\n",
    "            raise ValueError(f\"dailyì— í•´ë‹¹ dong/date ì—†ìŒ: {dong}, {target_ymd}\")\n",
    "        temp = float(row[\"temp\"].iloc[0])\n",
    "        rain_mean = float(row[\"rain_mean\"].iloc[0])\n",
    "        rain_max = float(row[\"rain_max\"].iloc[0])\n",
    "        weather_src = \"DATA_WEATHER(A ê²€ì¦)\"\n",
    "    else:\n",
    "        temp, rain_mean, rain_max = get_forecast_weather(dong, target_ymd, cfg)\n",
    "        weather_src = \"KMA_FORECAST(B ìš´ì˜)\"\n",
    "\n",
    "    # ë™ ì˜ˆì¸¡\n",
    "    X = build_one_row_for_predict(daily, dong, target_ymd, temp, rain_mean, rain_max, cfg, feature_cols)\n",
    "    X[\"dong\"] = X[\"dong\"].astype(\"category\")\n",
    "\n",
    "    pred = model.predict(X)[0]\n",
    "    dong_pred = float(np.expm1(pred) if cfg.use_log_target else pred)\n",
    "    dong_pred = max(0.0, dong_pred)\n",
    "\n",
    "    # store share + adjustment (ê°€ëŠ¥í•˜ë©´)\n",
    "    store_pred = None\n",
    "    share = None\n",
    "    adj = None\n",
    "    try:\n",
    "        store = load_store_sales(cfg.store_sales_path)\n",
    "        share = compute_share(daily, store, dong, target_ymd, cfg)\n",
    "\n",
    "        date_ts = ymd_int_to_ts(target_ymd)\n",
    "        cal = make_calendar_features(date_ts)\n",
    "        hol = make_holiday_features(date_ts)\n",
    "        adj = adjust_factor(bool(cal[\"is_weekend\"]), bool(hol[\"is_holiday\"]), bool(rain_max>0 or rain_mean>0), cfg)\n",
    "        store_pred = max(0.0, dong_pred * share * adj)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ê³ ê° ë©”ì‹œì§€ (rain_max ì¤‘ì‹¬)\n",
    "    lines = []\n",
    "    lines.append(f\"ğŸ“ì§€ì—­(ë™): {dong}\")\n",
    "    lines.append(f\"ğŸ“…ë‚ ì§œ: {target_ymd}\")\n",
    "    lines.append(f\"ğŸŒ¦ï¸ë‚ ì”¨(ì¶œì²˜): {weather_src}\")\n",
    "    lines.append(f\"   - í‰ê· ê¸°ì˜¨(ì¼í‰ê· ): {temp:.1f}Â°C\")\n",
    "    lines.append(f\"   - ì‹œê°„ëŒ€ í‰ê·  ê°•ìˆ˜ëŸ‰(ì¼í‰ê· ): {rain_mean:.2f} mm\")\n",
    "    lines.append(f\"   - âœ…ì‹œê°„ëŒ€ í‰ê·  ê°•ìˆ˜ëŸ‰(ì¼ìµœëŒ€, í”¼í¬): {rain_max:.2f} mm  â† ê³ ê°ì—ê²Œ ë³´ì—¬ì¤„ í•µì‹¬\")\n",
    "    lines.append(f\"ğŸ’°ì˜ˆìƒ ë™ ì „ì²´ ì¼ë§¤ì¶œ: {dong_pred:,.0f} ì›\")\n",
    "\n",
    "    if store_pred is None:\n",
    "        lines.append(\"ğŸªê°€ê²Œ ì˜ˆì¸¡: store_sales.csvê°€ ì—†ê±°ë‚˜ í˜•ì‹ ì˜¤ë¥˜ â†’ ë™ ì˜ˆì¸¡ë§Œ ì œê³µ\")\n",
    "    else:\n",
    "        lines.append(f\"ğŸªê°€ê²Œ ì ìœ ìœ¨(ìµœê·¼ {cfg.share_window_days}ì¼): {share*100:.3f}%\")\n",
    "        lines.append(f\"ğŸªë³´ì •ê³„ìˆ˜(adj): {adj:.3f}\")\n",
    "        lines.append(f\"ğŸªì˜ˆìƒ ê°€ê²Œ ì¼ë§¤ì¶œ: {store_pred:,.0f} ì›\")\n",
    "\n",
    "    return {\n",
    "        \"dong\": dong,\n",
    "        \"date\": int(target_ymd),\n",
    "        \"temp\": round(temp, 1),\n",
    "        \"rain_mean\": round(rain_mean, 2),\n",
    "        \"rain_max\": round(rain_max, 2),\n",
    "        \"dong_pred\": int(dong_pred),\n",
    "        \"share_pct\": None if share is None else round(share*100, 3),\n",
    "        \"store_pred\": None if store_pred is None else int(store_pred),\n",
    "        \"customer_message\": \"\\n\".join(lines),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN: train + customer demo\n",
    "# =========================\n",
    "def main():\n",
    "    raw = load_raw(CFG)\n",
    "    daily = make_daily(raw)\n",
    "    data, feature_cols = build_dataset(daily, CFG)\n",
    "\n",
    "    model_path = train_and_eval(data, feature_cols, CFG)\n",
    "\n",
    "    # ===== ê³ ê° ì…ë ¥ ì‹œë‚˜ë¦¬ì˜¤ =====\n",
    "    customer_dong = \"ê³ ë“±ë™\"\n",
    "    customer_date = int(daily[\"ta_ymd\"].max())  # ì˜ˆ: 20251031 (A ê²€ì¦)\n",
    "\n",
    "    result = predict_customer_sales(customer_dong, customer_date, model_path, CFG)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… [ê³ ê° í™”ë©´ ì¶œë ¥ ì˜ˆì‹œ]\")\n",
    "    print(\"=\"*60)\n",
    "    print(result[\"customer_message\"])\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n[RAW RESULT]\")\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5a850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a314ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ac2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bcebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp (ipykernel)",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
