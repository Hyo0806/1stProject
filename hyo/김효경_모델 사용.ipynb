{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f135e957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.text_cell_render.rendered_html{font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.text_cell_render.rendered_html{font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6c29e",
   "metadata": {},
   "source": [
    "# ë™ë³„ ê°ë‹¨ê°€ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2af876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TA_YMD DONG  DAY  HOUR BEST_MODEL   TEMP  RAIN  PRED_UNIT  KMA_BASE_USED\n",
      "0  20251231  í–‰ê¶ë™    3     1        XGB  -6.43   0.0      36440  20251230-1100\n",
      "1  20251231  í–‰ê¶ë™    3     2        XGB  -8.00   0.0      11853  20251230-1100\n",
      "2  20251231  í–‰ê¶ë™    3     3        XGB  -7.50   0.0      16005  20251230-1100\n",
      "3  20251231  í–‰ê¶ë™    3     4  EXTRATREE  -5.50   0.0      24844  20251230-1100\n",
      "4  20251231  í–‰ê¶ë™    3     5  EXTRATREE  -5.00   0.0      25553  20251230-1100\n",
      "5  20251231  í–‰ê¶ë™    3     6        XGB  -5.00   0.0      21377  20251230-1100\n",
      "6  20251231  í–‰ê¶ë™    3     7  EXTRATREE  -6.00   0.0      26994  20251230-1100\n",
      "7  20251231  í–‰ê¶ë™    3     8  EXTRATREE  -7.50   0.0      45340  20251230-1100\n",
      "8  20251231  í–‰ê¶ë™    3     9        XGB  -8.50   0.0      48005  20251230-1100\n",
      "9  20251231  í–‰ê¶ë™    3    10        XGB -10.00   0.0      43143  20251230-1100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =========================\n",
    "# ì„¤ì •\n",
    "# =========================\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "SERVICE_KEY = os.getenv(\"RAIN_ID\")  # ê¸°ìƒì²­ ë‹¨ê¸°ì˜ˆë³´ ì„œë¹„ìŠ¤í‚¤\n",
    "\n",
    "MODELS_ROOT = \"data/model\"\n",
    "BEST_MAP_PATH = os.path.join(MODELS_ROOT, \"results_by_hour.csv\")\n",
    "BUNDLE_PATH_TEMPLATE = os.path.join(MODELS_ROOT, \"HOUR_{hour:02d}\", \"best_model.joblib\")\n",
    "\n",
    "# í•™ìŠµ ë•Œ ë¡œê·¸íƒ€ê²Ÿì´ë©´ True\n",
    "USE_LOG_TARGET = False\n",
    "\n",
    "# (í•™ìŠµ ë•Œì™€ ë™ì¼í•´ì•¼ í•¨) ì•ˆì „ì¥ì¹˜ë¡œ drop\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "# ê²©ì ì—‘ì…€\n",
    "GRID_XLSX_PATH = \"data/ìˆ˜ì›ì‹œ ê²©ì.xlsx\"\n",
    "_GRID_CACHE = None  # ë°˜ë³µ ë¡œë”© ë°©ì§€ ìºì‹œ\n",
    "\n",
    "# =========================\n",
    "# í•­ëª©ìš”ì•½ ì‹œê°„ëŒ€(01~10)\n",
    "# =========================\n",
    "HOUR_BINS = {\n",
    "    1: list(range(0, 7)),     # 00~06\n",
    "    2: [7, 8],\n",
    "    3: [9, 10],\n",
    "    4: [11, 12],\n",
    "    5: [13, 14],\n",
    "    6: [15, 16],\n",
    "    7: [17, 18],\n",
    "    8: [19, 20],\n",
    "    9: [21, 22],\n",
    "    10: [23],\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# ë‚ ì§œ íŒŒìƒ + DAY(01=ì›”..07=ì¼)\n",
    "# =========================\n",
    "def compute_day_code(date_yyyymmdd: str) -> int:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return int(dt.weekday() + 1)\n",
    "\n",
    "def make_date_features(date_yyyymmdd: str) -> dict:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return {\n",
    "        \"TA_YMD\": str(date_yyyymmdd),\n",
    "        \"YEAR\": int(dt.year),\n",
    "        \"MONTH\": int(dt.month),\n",
    "        \"DAY_OF_MONTH\": int(dt.day),\n",
    "        \"WEEKOFYEAR\": int(dt.isocalendar().week),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# ê²©ì: DONG -> (nx, ny)\n",
    "# =========================\n",
    "def load_grid_table(path: str = GRID_XLSX_PATH) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    data/ìˆ˜ì›ì‹œ ê²©ì.xlsx\n",
    "    - Aì—´: ë™ì´ë¦„, Bì—´: nx, Cì—´: ny (ì‚¬ìš©ì ì„¤ëª… ê¸°ì¤€)\n",
    "    - headerê°€ ì—†ë‹¤ê³  ê°€ì •í•˜ê³  A/B/Cë¥¼ ê³ ì •ìœ¼ë¡œ ì½ìŒ\n",
    "      (ë§Œì•½ ì‹¤ì œ íŒŒì¼ì— í—¤ë”ê°€ ìˆìœ¼ë©´ header=None -> header=0 ìœ¼ë¡œ ë³€ê²½)\n",
    "    \"\"\"\n",
    "    global _GRID_CACHE\n",
    "    if _GRID_CACHE is not None:\n",
    "        return _GRID_CACHE\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"ê²©ì íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
    "\n",
    "    df = pd.read_excel(path, header=None)\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(f\"ê²©ì íŒŒì¼ ì»¬ëŸ¼ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤(A,B,C í•„ìš”): {path}\")\n",
    "\n",
    "    df = df.iloc[:, :3].copy()\n",
    "    df.columns = [\"DONG_NAME\", \"nx\", \"ny\"]\n",
    "\n",
    "    df[\"DONG_NAME\"] = df[\"DONG_NAME\"].astype(str).str.strip()\n",
    "    df[\"nx\"] = pd.to_numeric(df[\"nx\"], errors=\"coerce\")\n",
    "    df[\"ny\"] = pd.to_numeric(df[\"ny\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"nx\", \"ny\"]).copy()\n",
    "    df[\"nx\"] = df[\"nx\"].astype(int)\n",
    "    df[\"ny\"] = df[\"ny\"].astype(int)\n",
    "\n",
    "    _GRID_CACHE = df\n",
    "    return df\n",
    "\n",
    "def find_grid_by_dong(dong: str, path: str = GRID_XLSX_PATH) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ìê°€ ì…ë ¥í•œ dong(ì˜ˆ: 'í–‰ê¶ë™', 'ìˆ˜ì›ì‹œ íŒ”ë‹¬êµ¬ í–‰ê¶ë™')ìœ¼ë¡œ\n",
    "    ì—‘ì…€ Aì—´(DONG_NAME)ì—ì„œ ê²©ì(nx,ny) ì°¾ê¸°.\n",
    "    - ì™„ì „ì¼ì¹˜ ìš°ì„ , ì—†ìœ¼ë©´ í¬í•¨ê²€ìƒ‰(ì–‘ë°©í–¥)\n",
    "    - í›„ë³´ê°€ ì—¬ëŸ¬ ê°œë©´ DONG_NAME ê¸¸ì´ê°€ ì§§ì€ ê²ƒì„ ìš°ì„  ì„ íƒ\n",
    "    \"\"\"\n",
    "    dong = str(dong).strip()\n",
    "    if not dong:\n",
    "        raise ValueError(\"dong(ë™) ì…ë ¥ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    grid = load_grid_table(path)\n",
    "\n",
    "    # 1) ì™„ì „ì¼ì¹˜\n",
    "    exact = grid[grid[\"DONG_NAME\"] == dong]\n",
    "    if not exact.empty:\n",
    "        r = exact.iloc[0]\n",
    "        return int(r[\"nx\"]), int(r[\"ny\"])\n",
    "\n",
    "    # 2) í¬í•¨ê²€ìƒ‰(ì–‘ë°©í–¥)\n",
    "    a = grid[\"DONG_NAME\"].str.contains(dong, na=False)\n",
    "    b = grid[\"DONG_NAME\"].apply(lambda x: str(dong).find(str(x)) >= 0)\n",
    "    cand = grid[a | b].copy()\n",
    "\n",
    "    # fallback: ë§ˆì§€ë§‰ í† í°(ë³´í†µ 'OOë™')ë¡œ ì¬ì‹œë„\n",
    "    if cand.empty:\n",
    "        tok = dong.split()[-1]\n",
    "        cand = grid[grid[\"DONG_NAME\"].str.contains(tok, na=False)].copy()\n",
    "\n",
    "    if cand.empty:\n",
    "        raise ValueError(f\"'{dong}'ì— í•´ë‹¹í•˜ëŠ” ê²©ì(nx,ny)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì—‘ì…€ Aì—´ ë™ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    cand[\"name_len\"] = cand[\"DONG_NAME\"].str.len()\n",
    "    cand = cand.sort_values([\"name_len\"]).reset_index(drop=True)\n",
    "    r = cand.iloc[0]\n",
    "    return int(r[\"nx\"]), int(r[\"ny\"])\n",
    "\n",
    "# =========================\n",
    "# ê¸°ìƒì²­ ë‹¨ê¸°ì˜ˆë³´ TMP/PCP\n",
    "# =========================\n",
    "def _parse_pcp_to_mm(x):\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s in (\"ê°•ìˆ˜ì—†ìŒ\", \"ì—†ìŒ\", \"\", \"nan\", \"NaN\"):\n",
    "        return 0.0\n",
    "    if \"ë¯¸ë§Œ\" in s:\n",
    "        try:\n",
    "            num = float(s.replace(\"mm\", \"\").replace(\"ë¯¸ë§Œ\", \"\").strip())\n",
    "            return max(0.0, num * 0.5)\n",
    "        except:\n",
    "            return 0.0\n",
    "    if \"~\" in s:\n",
    "        try:\n",
    "            a, b = s.replace(\"mm\", \"\").split(\"~\")\n",
    "            return (float(a) + float(b)) / 2.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    try:\n",
    "        return float(s.replace(\"mm\", \"\"))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def fetch_vilage_fcst_json(service_key, base_date, base_time, nx, ny, num_rows=3000):\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_hourly_tmp_pcp_for_date(service_key, target_yyyymmdd, nx, ny):\n",
    "    base_times = [\"2300\", \"2000\", \"1700\", \"1400\", \"1100\", \"0800\", \"0500\", \"0200\"]\n",
    "    now = datetime.now()\n",
    "    today = now.strftime(\"%Y%m%d\")\n",
    "    yesterday = (now - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    last_err = None\n",
    "    for bd in [today, yesterday]:\n",
    "        for bt in base_times:\n",
    "            try:\n",
    "                js = fetch_vilage_fcst_json(service_key, bd, bt, nx, ny)\n",
    "                items = js[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "                df = pd.DataFrame(items)\n",
    "\n",
    "                df = df[df[\"category\"].isin([\"TMP\", \"PCP\"])]\n",
    "                df = df[df[\"fcstDate\"].astype(str) == str(target_yyyymmdd)]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                df[\"fcstTime\"] = df[\"fcstTime\"].astype(str).str.zfill(4)\n",
    "                piv = df.pivot_table(\n",
    "                    index=\"fcstTime\",\n",
    "                    columns=\"category\",\n",
    "                    values=\"fcstValue\",\n",
    "                    aggfunc=\"first\"\n",
    "                ).reset_index()\n",
    "\n",
    "                if \"TMP\" not in piv.columns:\n",
    "                    continue\n",
    "\n",
    "                piv[\"TMP\"] = piv[\"TMP\"].astype(float)\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].apply(_parse_pcp_to_mm) if \"PCP\" in piv.columns else 0.0\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].astype(float)\n",
    "                piv[\"base_used\"] = f\"{bd}-{bt}\"\n",
    "                return piv.sort_values(\"fcstTime\").reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(f\"í•´ë‹¹ ë‚ ì§œì˜ ê¸°ìƒì²­ ì˜ˆë³´ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. last_err={last_err}\")\n",
    "\n",
    "def aggregate_weather_to_item_slots(hourly_df: pd.DataFrame) -> dict:\n",
    "    hourly_df = hourly_df.copy()\n",
    "    hourly_df[\"HOUR_OF_DAY\"] = hourly_df[\"fcstTime\"].str[:2].astype(int)\n",
    "\n",
    "    out = {}\n",
    "    for h, hours in HOUR_BINS.items():\n",
    "        sub = hourly_df[hourly_df[\"HOUR_OF_DAY\"].isin(hours)]\n",
    "        out[h] = {\n",
    "            \"TEMP\": float(sub[\"TMP\"].mean()) if not sub.empty else np.nan,\n",
    "            \"RAIN\": float(sub[\"PCP\"].sum()) if not sub.empty else 0.0,\n",
    "        }\n",
    "\n",
    "    temps = [v[\"TEMP\"] for v in out.values() if not pd.isna(v[\"TEMP\"])]\n",
    "    fill_temp = float(np.mean(temps)) if temps else 0.0\n",
    "    for h in out:\n",
    "        if pd.isna(out[h][\"TEMP\"]):\n",
    "            out[h][\"TEMP\"] = fill_temp\n",
    "\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# âœ… ë™ ì…ë ¥ -> ê·¸ ë™ì˜ \"ê¸°ì˜¨\" ë°˜í™˜(ì›í•˜ëŠ” í•¨ìˆ˜)\n",
    "# =========================\n",
    "def get_temperature_by_dong(\n",
    "    date_yyyymmdd: str,\n",
    "    dong: str,\n",
    "    slot_hour: int = 5,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    dong ì…ë ¥ -> ê²©ì(nx,ny) ì¡°íšŒ -> ê¸°ìƒì²­ ì˜ˆë³´ -> slot_hour(1~10)ì˜ TEMP ë°˜í™˜\n",
    "    \"\"\"\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    if not (1 <= int(slot_hour) <= 10):\n",
    "        raise ValueError(\"slot_hourëŠ” 1~10 ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    temp = float(weather_by_hour[int(slot_hour)][\"TEMP\"])\n",
    "    return round(temp, 1)  # âœ… ì†Œìˆ˜ì  1ìë¦¬ ë°˜í™˜\n",
    "\n",
    "# =========================\n",
    "# ëª¨ë¸ ë¡œë“œ + \"ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì»¬ëŸ¼\" ìë™ ë§ì¶¤\n",
    "# (í•™ìŠµ ì €ì¥ë¬¼: best_model.joblib = {\"preprocess\": ColumnTransformer, \"model\": estimator, \"model_name\": str})\n",
    "# =========================\n",
    "def load_best_model_map():\n",
    "    if not os.path.exists(BEST_MAP_PATH):\n",
    "        raise FileNotFoundError(f\"ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {BEST_MAP_PATH}\")\n",
    "    df = pd.read_csv(BEST_MAP_PATH)\n",
    "    required_cols = {\"HOUR\", \"BEST_MODEL\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"{BEST_MAP_PATH}ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í•„ìš”: {required_cols}, í˜„ì¬: {set(df.columns)}\")\n",
    "    return df.set_index(\"HOUR\")[\"BEST_MODEL\"].to_dict()\n",
    "\n",
    "def load_best_bundle(hour: int):\n",
    "    path = BUNDLE_PATH_TEMPLATE.format(hour=hour)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"ë²ˆë“¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
    "    bundle = joblib.load(path)\n",
    "    if not isinstance(bundle, dict) or \"preprocess\" not in bundle or \"model\" not in bundle:\n",
    "        raise ValueError(f\"ë²ˆë“¤ í¬ë§·ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {path}\")\n",
    "    return bundle\n",
    "\n",
    "def get_required_feature_names_from_preprocess(preprocess) -> list:\n",
    "    names = []\n",
    "    for name, trans, cols in getattr(preprocess, \"transformers_\", []):\n",
    "        if cols is None:\n",
    "            continue\n",
    "        if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "            names.extend(list(cols))\n",
    "    return sorted(set([str(x) for x in names]))\n",
    "\n",
    "def align_row_to_required_columns(X_row: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
    "    X = X_row.copy()\n",
    "\n",
    "    if required_cols:\n",
    "        drop_cols = [c for c in X.columns if c not in required_cols]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        for c in required_cols:\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "\n",
    "        X = X[required_cols]\n",
    "\n",
    "    return X\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "# =========================\n",
    "# í•µì‹¬: ë‚ ì§œ 1ê°œ â†’ 1~10ì‹œê°„ëŒ€ ì˜ˆì¸¡\n",
    "# - (nx, ny ì§ì ‘ ì…ë ¥ ë²„ì „)\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather(\n",
    "    date_yyyymmdd: str,\n",
    "    nx: int,\n",
    "    ny: int,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    day_code = compute_day_code(date_yyyymmdd)\n",
    "\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    best_map = load_best_model_map()\n",
    "    rows = []\n",
    "\n",
    "    for hour in range(1, 11):\n",
    "        if hour not in best_map:\n",
    "            continue\n",
    "\n",
    "        bundle = load_best_bundle(hour)\n",
    "        best_name = str(best_map[hour])\n",
    "\n",
    "        row = {}\n",
    "        row.update(make_date_features(date_yyyymmdd))\n",
    "        row.update({\n",
    "            \"HOUR\": int(hour),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "            \"DELIV_TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"DELIV_RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "        })\n",
    "\n",
    "        X_row = pd.DataFrame([row])\n",
    "        X_row = X_row.drop(columns=[c for c in DROP_COLS if c in X_row.columns], errors=\"ignore\")\n",
    "\n",
    "        required = get_required_feature_names_from_preprocess(bundle[\"preprocess\"])\n",
    "        X_row_aligned = align_row_to_required_columns(X_row, required)\n",
    "\n",
    "        pred = float(predict_with_bundle(bundle, X_row_aligned)[0])\n",
    "        if USE_LOG_TARGET:\n",
    "            pred = float(np.expm1(pred))\n",
    "\n",
    "        rows.append({\n",
    "            \"TA_YMD\": str(date_yyyymmdd),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"HOUR\": int(hour),\n",
    "            \"BEST_MODEL\": best_name,\n",
    "            \"TEMP\": round(float(row[\"TEMP\"]), 2), \n",
    "            \"RAIN\": float(row[\"RAIN\"]),\n",
    "            \"PRED_UNIT\": int(round(pred)),\n",
    "            \"KMA_BASE_USED\": str(hourly_weather.loc[0, \"base_used\"]),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# âœ… í•µì‹¬: ì‚¬ìš©ì DONG ì…ë ¥ ë²„ì „ (ìš”ì²­í•œ íë¦„)\n",
    "# - DONGìœ¼ë¡œ nx,ny ìë™ ì¶”ì¶œ\n",
    "# - ê·¸ nx,nyë¡œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "# =========================\n",
    "def predict_date_all_hours_with_weather_by_dong(\n",
    "    date_yyyymmdd: str,\n",
    "    dong: str,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "    df_pred = predict_date_all_hours_with_weather(\n",
    "        date_yyyymmdd=date_yyyymmdd,\n",
    "        nx=nx,\n",
    "        ny=ny,\n",
    "        service_key=service_key\n",
    "    )\n",
    "    df_pred.insert(1, \"DONG\", str(dong))\n",
    "#     df_pred.insert(2, \"nx\", int(nx))\n",
    "#     df_pred.insert(3, \"ny\", int(ny))\n",
    "    return df_pred\n",
    "\n",
    "# =========================\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) ë™ì˜ íŠ¹ì • ì‹œê°„ëŒ€(í•­ëª©ìš”ì•½) ê¸°ì˜¨ë§Œ ë½‘ê¸°\n",
    "#     temp = get_temperature_by_dong(\"20251231\", dong=\"ì •ì2ë™\", slot_hour=5)\n",
    "#     print(\"TEMP(slot_hour=5):\", temp, \"Â°C\")\n",
    "\n",
    "    # 2) ë™ ì…ë ¥ìœ¼ë¡œ 1~10 ì‹œê°„ëŒ€ ë§¤ì¶œ(UNIT) ì˜ˆì¸¡\n",
    "    df_pred = predict_date_all_hours_with_weather_by_dong(\n",
    "        date_yyyymmdd=\"20251231\",\n",
    "        dong=\"í–‰ê¶ë™\"\n",
    "    )\n",
    "    print(df_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29172cbd",
   "metadata": {},
   "source": [
    "# ë™ë³„ ë§¤ì¶œ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6eba77b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics_summary.csv ì €ì¥ ì™„ë£Œ\n",
      "ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: pred_ë§¤êµë™_20251230.csv\n",
      "     TA_YMD DONG  HOUR  TEMP  RAIN  PRED_AMT  PRED_CNT\n",
      "0  20251230  ë§¤êµë™     1   0.0   0.0      4549         5\n",
      "1  20251230  ë§¤êµë™     2   0.0   0.0    131480         5\n",
      "2  20251230  ë§¤êµë™     3   0.0   0.0    223747        13\n",
      "3  20251230  ë§¤êµë™     4   0.0   0.0    976706        75\n",
      "4  20251230  ë§¤êµë™     5   0.0   0.0    999598        46\n",
      "5  20251230  ë§¤êµë™     6   0.0   0.0   1306299        31\n",
      "6  20251230  ë§¤êµë™     7   1.0   0.0    976212        50\n",
      "7  20251230  ë§¤êµë™     8  -1.0   0.0   1759243        46\n",
      "8  20251230  ë§¤êµë™     9  -1.0   0.0   1033210        21\n",
      "9  20251230  ë§¤êµë™    10  -2.0   0.0    223855         5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ì‹œê°„ëŒ€ë³„ AMT/CNT ì˜ˆì¸¡ (ìˆ˜ì›ì‹œ í•œì‹ ë™ë³„ ë°ì´í„°)\n",
    "- ì…ë ¥: ë‚ ì§œ(TA_YMD), DONG, (ê¸°ìƒì²­ APIë¡œ ê°€ì ¸ì˜¨) ì‹œê°„ëŒ€ë³„ TEMP/RAIN\n",
    "- ì¶œë ¥: ì‹œê°„ëŒ€ë³„ AMT, CNT\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# =========================\n",
    "# A) ì„¤ì •\n",
    "# =========================\n",
    "load_dotenv('')\n",
    "SERVICE_KEY = os.getenv('RAIN_ID')\n",
    "\n",
    "DATA_CSV = \"data/ìˆ˜ì›ì‹œ í•œì‹ ë™ë³„ ë°ì´í„°ë°±ì—….csv\"\n",
    "MODEL_DIR = \"data/models_hourly_amt_cnt\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ í´ë”(ëª…ì‹œ)\n",
    "OUTPUT_DIR = \"data/pred_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "KMA_VILAGE_URL = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "BASE_TIMES = [\"2300\", \"2000\", \"1700\", \"1400\", \"1100\", \"0800\", \"0500\", \"0200\"]\n",
    "\n",
    "HOUR_SLOT_TO_REP_TIME = {\n",
    "    1: \"0300\",\n",
    "    2: \"0800\",\n",
    "    3: \"1000\",\n",
    "    4: \"1200\",\n",
    "    5: \"1400\",\n",
    "    6: \"1600\",\n",
    "    7: \"1800\",\n",
    "    8: \"2000\",\n",
    "    9: \"2200\",\n",
    "    10: \"2300\",\n",
    "}\n",
    "\n",
    "DONG_GRID_XLSX = \"data/ìˆ˜ì›ì‹œ ê²©ì.xlsx\"\n",
    "_DONG_TO_GRID = None  # lazy cache\n",
    "\n",
    "\n",
    "# =========================\n",
    "# B) ìœ í‹¸: ìš”ì¼ ê³„ì‚° (1~7, ì›”=1 ... ì¼=7)\n",
    "# =========================\n",
    "def ymd_to_day_1to7(yyyymmdd: str) -> int:\n",
    "    dt = datetime.strptime(str(yyyymmdd), \"%Y%m%d\")\n",
    "    return dt.weekday() + 1\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DONG -> (nx,ny) ë¡œë”©/ì¡°íšŒ\n",
    "# =========================\n",
    "def load_dong_to_grid(xlsx_path: str = DONG_GRID_XLSX) -> dict:\n",
    "    global _DONG_TO_GRID\n",
    "    if _DONG_TO_GRID is not None:\n",
    "        return _DONG_TO_GRID\n",
    "\n",
    "    if not os.path.exists(xlsx_path):\n",
    "        raise FileNotFoundError(f\"ê²©ì íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {xlsx_path}\")\n",
    "\n",
    "    df = pd.read_excel(xlsx_path, header=None)\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(f\"ê²©ì íŒŒì¼ ì»¬ëŸ¼ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤(A,B,C í•„ìš”): {xlsx_path}\")\n",
    "\n",
    "    df = df.iloc[:, :3].copy()\n",
    "    df.columns = [\"DONG_NAME\", \"nx\", \"ny\"]\n",
    "\n",
    "    df[\"DONG_NAME\"] = df[\"DONG_NAME\"].astype(str).str.strip()\n",
    "    df[\"nx\"] = pd.to_numeric(df[\"nx\"], errors=\"coerce\")\n",
    "    df[\"ny\"] = pd.to_numeric(df[\"ny\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"nx\", \"ny\"]).copy()\n",
    "    df[\"nx\"] = df[\"nx\"].astype(int)\n",
    "    df[\"ny\"] = df[\"ny\"].astype(int)\n",
    "\n",
    "    _DONG_TO_GRID = {row[\"DONG_NAME\"]: (int(row[\"nx\"]), int(row[\"ny\"])) for _, row in df.iterrows()}\n",
    "    return _DONG_TO_GRID\n",
    "\n",
    "\n",
    "def find_grid_by_dong(dong: str, xlsx_path: str = DONG_GRID_XLSX) -> tuple[int, int]:\n",
    "    dong = str(dong).strip()\n",
    "    if not dong:\n",
    "        raise ValueError(\"dong ì…ë ¥ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    dmap = load_dong_to_grid(xlsx_path)\n",
    "\n",
    "    if dong in dmap:\n",
    "        return dmap[dong]\n",
    "\n",
    "    keys = list(dmap.keys())\n",
    "    cand = []\n",
    "    for k in keys:\n",
    "        if (k in dong) or (dong in k):\n",
    "            cand.append(k)\n",
    "\n",
    "    if not cand:\n",
    "        tok = dong.split()[-1]\n",
    "        for k in keys:\n",
    "            if tok in k or k in tok:\n",
    "                cand.append(k)\n",
    "\n",
    "    if not cand:\n",
    "        raise ValueError(f\"'{dong}'ì— í•´ë‹¹í•˜ëŠ” nx/nyë¥¼ ê²©ì ì—‘ì…€ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Aì—´ ë™ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    cand = sorted(cand, key=len)\n",
    "    return dmap[cand[0]]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# C) ê¸°ìƒì²­ API: base_date/base_time ê²°ì •\n",
    "# =========================\n",
    "def pick_base_datetime_kst(now_kst: datetime) -> tuple[str, str]:\n",
    "    today = now_kst.strftime(\"%Y%m%d\")\n",
    "    hhmm = now_kst.strftime(\"%H%M\")\n",
    "\n",
    "    for bt in BASE_TIMES:\n",
    "        if hhmm >= bt:\n",
    "            return today, bt\n",
    "\n",
    "    yday = (now_kst - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    return yday, \"2300\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# D) ê¸°ìƒì²­ ë‹¨ê¸°ì˜ˆë³´ í˜¸ì¶œ + íŠ¹ì • ë‚ ì§œ/ì‹œê° TMP, PCP ì¶”ì¶œ\n",
    "# =========================\n",
    "def fetch_vilage_fcst_items(service_key: str, base_date: str, base_time: str, nx: int, ny: int,\n",
    "                            num_of_rows: int = 2000) -> list[dict]:\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_of_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(KMA_VILAGE_URL, params=params, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "\n",
    "\n",
    "def parse_pcp_to_float(pcp_val) -> float:\n",
    "    if pcp_val is None:\n",
    "        return 0.0\n",
    "    s = str(pcp_val).strip()\n",
    "    if s in (\"ê°•ìˆ˜ì—†ìŒ\", \"ì—†ìŒ\", \"\"):\n",
    "        return 0.0\n",
    "    s = s.replace(\"mm\", \"\").replace(\"ë¯¸ë§Œ\", \"\").strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def get_temp_rain_for_datetime(service_key: str, dong: str, target_yyyymmdd: str, target_hhmm: str) -> tuple[float, float]:\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "\n",
    "    now_kst = datetime.now(timezone(timedelta(hours=9)))\n",
    "    base_date, base_time = pick_base_datetime_kst(now_kst)\n",
    "\n",
    "    items = fetch_vilage_fcst_items(service_key, base_date, base_time, nx, ny)\n",
    "\n",
    "    tmp = None\n",
    "    pcp = None\n",
    "    for it in items:\n",
    "        if it.get(\"fcstDate\") == target_yyyymmdd and it.get(\"fcstTime\") == target_hhmm:\n",
    "            cat = it.get(\"category\")\n",
    "            if cat == \"TMP\":\n",
    "                tmp = float(it.get(\"fcstValue\"))\n",
    "            elif cat == \"PCP\":\n",
    "                pcp = parse_pcp_to_float(it.get(\"fcstValue\"))\n",
    "\n",
    "    if tmp is None:\n",
    "        tmp = 0.0\n",
    "    if pcp is None:\n",
    "        pcp = 0.0\n",
    "\n",
    "    return tmp, pcp\n",
    "\n",
    "\n",
    "def get_hourly_weather_features(service_key: str, dong: str, yyyymmdd: str) -> pd.DataFrame:\n",
    "    day_1to7 = ymd_to_day_1to7(yyyymmdd)\n",
    "\n",
    "    rows = []\n",
    "    for hour_slot in range(1, 11):\n",
    "        rep_time = HOUR_SLOT_TO_REP_TIME[hour_slot]\n",
    "        temp, rain = get_temp_rain_for_datetime(service_key, dong, yyyymmdd, rep_time)\n",
    "        rows.append({\n",
    "            \"TA_YMD\": int(yyyymmdd),\n",
    "            \"DONG\": dong,\n",
    "            \"HOUR\": hour_slot,\n",
    "            \"DAY\": day_1to7,\n",
    "            \"TEMP\": float(temp),\n",
    "            \"RAIN\": float(rain),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# E) ëª¨ë¸ í•™ìŠµ (ì‹œê°„ëŒ€ë³„ ì €ì¥)\n",
    "# =========================\n",
    "def build_preprocess():\n",
    "    cat_cols = [\"DONG\"]\n",
    "    num_cols = [\"DAY\", \"TEMP\", \"RAIN\"]\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "def train_and_save_models(random_state: int = 42) -> pd.DataFrame:\n",
    "    df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "    df[\"DONG\"] = df[\"DONG\"].astype(str)\n",
    "    df[\"HOUR\"] = df[\"HOUR\"].astype(int)\n",
    "    df[\"DAY\"] = df[\"DAY\"].astype(int)\n",
    "    df[\"TEMP\"] = df[\"TEMP\"].astype(float)\n",
    "    df[\"RAIN\"] = df[\"RAIN\"].astype(float)\n",
    "\n",
    "    y_cols = [\"AMT\", \"CNT\"]\n",
    "    metrics = []\n",
    "\n",
    "    for h in range(1, 11):\n",
    "        sub = df[df[\"HOUR\"] == h].copy()\n",
    "        sub = sub.dropna(subset=[\"DONG\", \"DAY\", \"TEMP\", \"RAIN\", \"AMT\", \"CNT\"])\n",
    "\n",
    "        X = sub[[\"DONG\", \"DAY\", \"TEMP\", \"RAIN\"]]\n",
    "        y = sub[y_cols]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state, shuffle=True\n",
    "        )\n",
    "\n",
    "        pre = build_preprocess()\n",
    "\n",
    "        if h == 1:\n",
    "            base_model = MLPRegressor(\n",
    "                hidden_layer_sizes=(128, 64),\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                alpha=1e-4,\n",
    "                batch_size=256,\n",
    "                learning_rate_init=1e-3,\n",
    "                max_iter=300,\n",
    "                random_state=42,\n",
    "                early_stopping=True,\n",
    "                n_iter_no_change=10,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            base_model = XGBRegressor(\n",
    "                n_estimators=600,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                reg_alpha=0.0,\n",
    "                reg_lambda=1.0,\n",
    "                objective=\"reg:squarederror\",\n",
    "                random_state=random_state,\n",
    "                tree_method=\"hist\",\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"pre\", pre),\n",
    "            (\"model\", model),\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        pred = pipe.predict(X_test)\n",
    "        mae_amt = mean_absolute_error(y_test[\"AMT\"].values, pred[:, 0])\n",
    "        mae_cnt = mean_absolute_error(y_test[\"CNT\"].values, pred[:, 1])\n",
    "\n",
    "        model_path = os.path.join(MODEL_DIR, f\"hour_{h:02d}_amt_cnt.joblib\")\n",
    "        joblib.dump(pipe, model_path)\n",
    "\n",
    "        metrics.append({\n",
    "            \"HOUR\": h,\n",
    "            \"MAE_AMT\": mae_amt,\n",
    "            \"MAE_CNT\": mae_cnt,\n",
    "            \"model_path\": model_path,\n",
    "            \"model_type\": \"MLP(Deep)\" if h == 1 else \"XGB\",\n",
    "            \"n_train\": len(X_train),\n",
    "            \"n_test\": len(X_test),\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics).sort_values(\"HOUR\")\n",
    "    metrics_df.to_csv(os.path.join(MODEL_DIR, \"metrics_summary.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # âœ… ë„ˆê°€ ì›í•œ ì¶œë ¥ 1) ìœ ì§€\n",
    "    print(\"metrics_summary.csv ì €ì¥ ì™„ë£Œ\", flush=True)\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# F) ì˜ˆì¸¡ (ë‚ ì§œ+ë™ ì…ë ¥ -> ì‹œê°„ëŒ€ë³„ AMT/CNT)\n",
    "# =========================\n",
    "def load_models():\n",
    "    models = {}\n",
    "    for h in range(1, 11):\n",
    "        p = os.path.join(MODEL_DIR, f\"hour_{h:02d}_amt_cnt.joblib\")\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {p} (ë¨¼ì € train_and_save_models() ì‹¤í–‰)\")\n",
    "        models[h] = joblib.load(p)\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_day(yyyymmdd: str, dong: str, save_csv: bool = True) -> pd.DataFrame:\n",
    "    feat_df = get_hourly_weather_features(SERVICE_KEY, dong, yyyymmdd)\n",
    "    models = load_models()\n",
    "\n",
    "    outs = []\n",
    "    for h in range(1, 11):\n",
    "        row = feat_df[feat_df[\"HOUR\"] == h][[\"DONG\", \"DAY\", \"TEMP\", \"RAIN\"]]\n",
    "        pred_amt, pred_cnt = models[h].predict(row)[0]\n",
    "        outs.append({\n",
    "            \"TA_YMD\": yyyymmdd,\n",
    "            \"DONG\": dong,\n",
    "            \"HOUR\": h,\n",
    "            \"TEMP\": float(feat_df.loc[feat_df[\"HOUR\"] == h, \"TEMP\"].iloc[0]),\n",
    "            \"RAIN\": float(feat_df.loc[feat_df[\"HOUR\"] == h, \"RAIN\"].iloc[0]),\n",
    "            \"PRED_AMT\": int(round(pred_amt)),\n",
    "            \"PRED_CNT\": int(round(pred_cnt)),\n",
    "        })\n",
    "\n",
    "    pred_df = pd.DataFrame(outs)\n",
    "\n",
    "    if save_csv:\n",
    "        filename = f\"pred_{dong}_{yyyymmdd}.csv\"\n",
    "        saved_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        pred_df.to_csv(saved_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {filename}\", flush=True)\n",
    "        # ë§Œì•½ \"ì „ì²´ ê²½ë¡œ\"ë„ ë³´ê³ ì‹¶ìœ¼ë©´ ì•„ë˜ë¡œ êµì²´:\n",
    "        # print(f\"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {saved_path}\", flush=True)\n",
    "\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# G) ì‹¤í–‰ ì˜ˆì‹œ (ì›í•˜ëŠ” 3ê°œë§Œ ì¶œë ¥)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    _ = train_and_save_models()\n",
    "\n",
    "    if not SERVICE_KEY:\n",
    "        raise ValueError(\"í™˜ê²½ë³€ìˆ˜ RAIN_ID(SERVICE_KEY)ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ì„ í•˜ë ¤ë©´ .envë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    yyyymmdd = \"20251230\"\n",
    "    dong = \"ë§¤êµë™\"\n",
    "    pred_df = predict_day(yyyymmdd, dong, save_csv=True)\n",
    "\n",
    "    print(pred_df, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd939363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\ml-dl-nlp\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics_summary.csv ì €ì¥ ì™„ë£Œ\n",
      "ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: pred_í‰ë™_20251230.csv\n",
      "     TA_YMD DONG  HOUR  TEMP  RAIN  PRED_AMT  PRED_CNT\n",
      "0  20251230   í‰ë™     1   0.0   0.0    758525        47\n",
      "1  20251230   í‰ë™     2   0.0   0.0   1472177        55\n",
      "2  20251230   í‰ë™     3   0.0   0.0   3691709       116\n",
      "3  20251230   í‰ë™     4   0.0   0.0  15582414       720\n",
      "4  20251230   í‰ë™     5   0.0   0.0  10255817       385\n",
      "5  20251230   í‰ë™     6   0.0   0.0   4840355       193\n",
      "6  20251230   í‰ë™     7   0.0   0.0   8269984       249\n",
      "7  20251230   í‰ë™     8  -1.0   0.0  10334836       246\n",
      "8  20251230   í‰ë™     9  -2.0   0.0   3836450        75\n",
      "9  20251230   í‰ë™    10  -2.0   0.0    770741        17\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = train_and_save_models()\n",
    "\n",
    "    if not SERVICE_KEY:\n",
    "        raise ValueError(\"í™˜ê²½ë³€ìˆ˜ RAIN_ID(SERVICE_KEY)ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ì„ í•˜ë ¤ë©´ .envë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    yyyymmdd = \"20251230\"\n",
    "    dong = \"í‰ë™\"\n",
    "    pred_df = predict_day(yyyymmdd, dong, save_csv=True)\n",
    "\n",
    "    print(pred_df, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31486f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "073cf6fa",
   "metadata": {},
   "source": [
    "# í .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394e1dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TA_YMD  DONG  DAY  HOUR HOUR_LABEL BEST_MODEL  TEMP  RAIN  PRED_AMT  \\\n",
      "0  20251231  ì •ì2ë™    3     1      00~06        XGB -5.43   0.0     38545   \n",
      "1  20251231  ì •ì2ë™    3     2      07~08        XGB -8.00   0.0     11827   \n",
      "2  20251231  ì •ì2ë™    3     3      09~10        XGB -7.00   0.0     16934   \n",
      "3  20251231  ì •ì2ë™    3     4      11~12  EXTRATREE -5.50   0.0     25851   \n",
      "4  20251231  ì •ì2ë™    3     5      13~14  EXTRATREE -4.00   0.0     24799   \n",
      "5  20251231  ì •ì2ë™    3     6      15~16        XGB -4.00   0.0     20502   \n",
      "6  20251231  ì •ì2ë™    3     7      17~18  EXTRATREE -5.50   0.0     32290   \n",
      "7  20251231  ì •ì2ë™    3     8      19~20  EXTRATREE -6.50   0.0     38535   \n",
      "8  20251231  ì •ì2ë™    3     9      21~22        XGB -8.00   0.0     34702   \n",
      "9  20251231  ì •ì2ë™    3    10         23        XGB -9.00   0.0     51965   \n",
      "\n",
      "   KMA_BASE_USED  PRED_AMT_TOTAL_DAY  \n",
      "0  20251230-1100              295950  \n",
      "1  20251230-1100              295950  \n",
      "2  20251230-1100              295950  \n",
      "3  20251230-1100              295950  \n",
      "4  20251230-1100              295950  \n",
      "5  20251230-1100              295950  \n",
      "6  20251230-1100              295950  \n",
      "7  20251230-1100              295950  \n",
      "8  20251230-1100              295950  \n",
      "9  20251230-1100              295950  \n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ë§¤ì¶œ(AMT) ì˜ˆì¸¡ ê²°ê³¼\n",
      "================================================================================\n",
      "ğŸ“ì§€ì—­(ë™): ì •ì2ë™\n",
      "ğŸ“…ë‚ ì§œ: 20251231 (DAYì½”ë“œ=3)\n",
      "--------------------------------------------------------------------------------\n",
      "â° 01 (00~06) | ğŸŒ¡ï¸  -5.4Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     38,545 ì›\n",
      "â° 02 (07~08) | ğŸŒ¡ï¸  -8.0Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     11,827 ì›\n",
      "â° 03 (09~10) | ğŸŒ¡ï¸  -7.0Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     16,934 ì›\n",
      "â° 04 (11~12) | ğŸŒ¡ï¸  -5.5Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     25,851 ì›\n",
      "â° 05 (13~14) | ğŸŒ¡ï¸  -4.0Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     24,799 ì›\n",
      "â° 06 (15~16) | ğŸŒ¡ï¸  -4.0Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     20,502 ì›\n",
      "â° 07 (17~18) | ğŸŒ¡ï¸  -5.5Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     32,290 ì›\n",
      "â° 08 (19~20) | ğŸŒ¡ï¸  -6.5Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     38,535 ì›\n",
      "â° 09 (21~22) | ğŸŒ¡ï¸  -8.0Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     34,702 ì›\n",
      "â° 10 (23) | ğŸŒ¡ï¸  -9.0Â°C | ğŸŒ§ï¸    0.0mm | ğŸ’°     51,965 ì›\n",
      "--------------------------------------------------------------------------------\n",
      "âœ… í•˜ë£¨ ì´ë§¤ì¶œ(1~10í•©): ğŸ’°      295,950 ì›\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# =========================\n",
    "# ì„¤ì •\n",
    "# =========================\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "SERVICE_KEY = os.getenv(\"RAIN_ID\")  # ê¸°ìƒì²­ ë‹¨ê¸°ì˜ˆë³´ ì„œë¹„ìŠ¤í‚¤\n",
    "\n",
    "MODELS_ROOT = \"data/model\"\n",
    "BEST_MAP_PATH = os.path.join(MODELS_ROOT, \"results_by_hour.csv\")\n",
    "BUNDLE_PATH_TEMPLATE = os.path.join(MODELS_ROOT, \"HOUR_{hour:02d}\", \"best_model.joblib\")\n",
    "\n",
    "# âœ… AMT(ë§¤ì¶œ) í•™ìŠµ ë•Œ ë¡œê·¸ íƒ€ê²Ÿ(y=log1p(AMT))ì´ë©´ True\n",
    "USE_LOG_TARGET = False\n",
    "\n",
    "# (í•™ìŠµ ë•Œì™€ ë™ì¼í•´ì•¼ í•¨) ì•ˆì „ì¥ì¹˜ë¡œ drop\n",
    "DROP_COLS = [\"UNIT\", \"DATE\", \"AMT\", \"CNT\"]\n",
    "\n",
    "# ê²©ì ì—‘ì…€\n",
    "GRID_XLSX_PATH = \"data/ìˆ˜ì›ì‹œ ê²©ì.xlsx\"\n",
    "_GRID_CACHE = None  # ë°˜ë³µ ë¡œë”© ë°©ì§€ ìºì‹œ\n",
    "\n",
    "# =========================\n",
    "# í•­ëª©ìš”ì•½ ì‹œê°„ëŒ€(01~10)\n",
    "# =========================\n",
    "HOUR_BINS = {\n",
    "    1: list(range(0, 7)),     # 00~06\n",
    "    2: [7, 8],\n",
    "    3: [9, 10],\n",
    "    4: [11, 12],\n",
    "    5: [13, 14],\n",
    "    6: [15, 16],\n",
    "    7: [17, 18],\n",
    "    8: [19, 20],\n",
    "    9: [21, 22],\n",
    "    10: [23],\n",
    "}\n",
    "\n",
    "# ì‹œê°„ëŒ€ ë¼ë²¨(ë³´ê¸° ì¢‹ê²Œ)\n",
    "HOUR_LABELS = {\n",
    "    1: \"00~06\",\n",
    "    2: \"07~08\",\n",
    "    3: \"09~10\",\n",
    "    4: \"11~12\",\n",
    "    5: \"13~14\",\n",
    "    6: \"15~16\",\n",
    "    7: \"17~18\",\n",
    "    8: \"19~20\",\n",
    "    9: \"21~22\",\n",
    "    10: \"23\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# ë‚ ì§œ íŒŒìƒ + DAY(01=ì›”..07=ì¼)\n",
    "# =========================\n",
    "def compute_day_code(date_yyyymmdd: str) -> int:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return int(dt.weekday() + 1)\n",
    "\n",
    "def make_date_features(date_yyyymmdd: str) -> dict:\n",
    "    dt = pd.to_datetime(str(date_yyyymmdd), format=\"%Y%m%d\", errors=\"raise\")\n",
    "    return {\n",
    "        \"TA_YMD\": str(date_yyyymmdd),\n",
    "        \"YEAR\": int(dt.year),\n",
    "        \"MONTH\": int(dt.month),\n",
    "        \"DAY_OF_MONTH\": int(dt.day),\n",
    "        \"WEEKOFYEAR\": int(dt.isocalendar().week),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# ê²©ì: DONG -> (nx, ny)\n",
    "# =========================\n",
    "def load_grid_table(path: str = GRID_XLSX_PATH) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    data/ìˆ˜ì›ì‹œ ê²©ì.xlsx\n",
    "    - Aì—´: ë™ì´ë¦„, Bì—´: nx, Cì—´: ny (ì‚¬ìš©ì ì„¤ëª… ê¸°ì¤€)\n",
    "    - headerê°€ ì—†ë‹¤ê³  ê°€ì •í•˜ê³  A/B/Cë¥¼ ê³ ì •ìœ¼ë¡œ ì½ìŒ\n",
    "      (ë§Œì•½ ì‹¤ì œ íŒŒì¼ì— í—¤ë”ê°€ ìˆìœ¼ë©´ header=None -> header=0 ìœ¼ë¡œ ë³€ê²½)\n",
    "    \"\"\"\n",
    "    global _GRID_CACHE\n",
    "    if _GRID_CACHE is not None:\n",
    "        return _GRID_CACHE\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"ê²©ì íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
    "\n",
    "    df = pd.read_excel(path, header=None)\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(f\"ê²©ì íŒŒì¼ ì»¬ëŸ¼ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤(A,B,C í•„ìš”): {path}\")\n",
    "\n",
    "    df = df.iloc[:, :3].copy()\n",
    "    df.columns = [\"DONG_NAME\", \"nx\", \"ny\"]\n",
    "\n",
    "    df[\"DONG_NAME\"] = df[\"DONG_NAME\"].astype(str).str.strip()\n",
    "    df[\"nx\"] = pd.to_numeric(df[\"nx\"], errors=\"coerce\")\n",
    "    df[\"ny\"] = pd.to_numeric(df[\"ny\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"nx\", \"ny\"]).copy()\n",
    "    df[\"nx\"] = df[\"nx\"].astype(int)\n",
    "    df[\"ny\"] = df[\"ny\"].astype(int)\n",
    "\n",
    "    _GRID_CACHE = df\n",
    "    return df\n",
    "\n",
    "def find_grid_by_dong(dong: str, path: str = GRID_XLSX_PATH) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ìê°€ ì…ë ¥í•œ dong(ì˜ˆ: 'í–‰ê¶ë™', 'ìˆ˜ì›ì‹œ íŒ”ë‹¬êµ¬ í–‰ê¶ë™')ìœ¼ë¡œ\n",
    "    ì—‘ì…€ Aì—´(DONG_NAME)ì—ì„œ ê²©ì(nx,ny) ì°¾ê¸°.\n",
    "    - ì™„ì „ì¼ì¹˜ ìš°ì„ , ì—†ìœ¼ë©´ í¬í•¨ê²€ìƒ‰(ì–‘ë°©í–¥)\n",
    "    - í›„ë³´ê°€ ì—¬ëŸ¬ ê°œë©´ DONG_NAME ê¸¸ì´ê°€ ì§§ì€ ê²ƒì„ ìš°ì„  ì„ íƒ\n",
    "    \"\"\"\n",
    "    dong = str(dong).strip()\n",
    "    if not dong:\n",
    "        raise ValueError(\"dong(ë™) ì…ë ¥ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    grid = load_grid_table(path)\n",
    "\n",
    "    # 1) ì™„ì „ì¼ì¹˜\n",
    "    exact = grid[grid[\"DONG_NAME\"] == dong]\n",
    "    if not exact.empty:\n",
    "        r = exact.iloc[0]\n",
    "        return int(r[\"nx\"]), int(r[\"ny\"])\n",
    "\n",
    "    # 2) í¬í•¨ê²€ìƒ‰(ì–‘ë°©í–¥)\n",
    "    a = grid[\"DONG_NAME\"].str.contains(dong, na=False)\n",
    "    b = grid[\"DONG_NAME\"].apply(lambda x: str(dong).find(str(x)) >= 0)\n",
    "    cand = grid[a | b].copy()\n",
    "\n",
    "    # fallback: ë§ˆì§€ë§‰ í† í°(ë³´í†µ 'OOë™')ë¡œ ì¬ì‹œë„\n",
    "    if cand.empty:\n",
    "        tok = dong.split()[-1]\n",
    "        cand = grid[grid[\"DONG_NAME\"].str.contains(tok, na=False)].copy()\n",
    "\n",
    "    if cand.empty:\n",
    "        raise ValueError(f\"'{dong}'ì— í•´ë‹¹í•˜ëŠ” ê²©ì(nx,ny)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì—‘ì…€ Aì—´ ë™ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    cand[\"name_len\"] = cand[\"DONG_NAME\"].str.len()\n",
    "    cand = cand.sort_values([\"name_len\"]).reset_index(drop=True)\n",
    "    r = cand.iloc[0]\n",
    "    return int(r[\"nx\"]), int(r[\"ny\"])\n",
    "\n",
    "# =========================\n",
    "# ê¸°ìƒì²­ ë‹¨ê¸°ì˜ˆë³´ TMP/PCP\n",
    "# =========================\n",
    "def _parse_pcp_to_mm(x):\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s in (\"ê°•ìˆ˜ì—†ìŒ\", \"ì—†ìŒ\", \"\", \"nan\", \"NaN\"):\n",
    "        return 0.0\n",
    "    if \"ë¯¸ë§Œ\" in s:\n",
    "        try:\n",
    "            num = float(s.replace(\"mm\", \"\").replace(\"ë¯¸ë§Œ\", \"\").strip())\n",
    "            return max(0.0, num * 0.5)\n",
    "        except:\n",
    "            return 0.0\n",
    "    if \"~\" in s:\n",
    "        try:\n",
    "            a, b = s.replace(\"mm\", \"\").split(\"~\")\n",
    "            return (float(a) + float(b)) / 2.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    try:\n",
    "        return float(s.replace(\"mm\", \"\"))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def fetch_vilage_fcst_json(service_key, base_date, base_time, nx, ny, num_rows=3000):\n",
    "    url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    params = {\n",
    "        \"serviceKey\": service_key,\n",
    "        \"pageNo\": 1,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"dataType\": \"JSON\",\n",
    "        \"base_date\": base_date,\n",
    "        \"base_time\": base_time,\n",
    "        \"nx\": nx,\n",
    "        \"ny\": ny,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def fetch_hourly_tmp_pcp_for_date(service_key, target_yyyymmdd, nx, ny):\n",
    "    base_times = [\"2300\", \"2000\", \"1700\", \"1400\", \"1100\", \"0800\", \"0500\", \"0200\"]\n",
    "    now = datetime.now()\n",
    "    today = now.strftime(\"%Y%m%d\")\n",
    "    yesterday = (now - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "    last_err = None\n",
    "    for bd in [today, yesterday]:\n",
    "        for bt in base_times:\n",
    "            try:\n",
    "                js = fetch_vilage_fcst_json(service_key, bd, bt, nx, ny)\n",
    "                items = js[\"response\"][\"body\"][\"items\"][\"item\"]\n",
    "                df = pd.DataFrame(items)\n",
    "\n",
    "                df = df[df[\"category\"].isin([\"TMP\", \"PCP\"])]\n",
    "                df = df[df[\"fcstDate\"].astype(str) == str(target_yyyymmdd)]\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                df[\"fcstTime\"] = df[\"fcstTime\"].astype(str).str.zfill(4)\n",
    "                piv = df.pivot_table(\n",
    "                    index=\"fcstTime\",\n",
    "                    columns=\"category\",\n",
    "                    values=\"fcstValue\",\n",
    "                    aggfunc=\"first\"\n",
    "                ).reset_index()\n",
    "\n",
    "                if \"TMP\" not in piv.columns:\n",
    "                    continue\n",
    "\n",
    "                piv[\"TMP\"] = piv[\"TMP\"].astype(float)\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].apply(_parse_pcp_to_mm) if \"PCP\" in piv.columns else 0.0\n",
    "                piv[\"PCP\"] = piv[\"PCP\"].astype(float)\n",
    "                piv[\"base_used\"] = f\"{bd}-{bt}\"\n",
    "                return piv.sort_values(\"fcstTime\").reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(f\"í•´ë‹¹ ë‚ ì§œì˜ ê¸°ìƒì²­ ì˜ˆë³´ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. last_err={last_err}\")\n",
    "\n",
    "def aggregate_weather_to_item_slots(hourly_df: pd.DataFrame) -> dict:\n",
    "    hourly_df = hourly_df.copy()\n",
    "    hourly_df[\"HOUR_OF_DAY\"] = hourly_df[\"fcstTime\"].str[:2].astype(int)\n",
    "\n",
    "    out = {}\n",
    "    for h, hours in HOUR_BINS.items():\n",
    "        sub = hourly_df[hourly_df[\"HOUR_OF_DAY\"].isin(hours)]\n",
    "        out[h] = {\n",
    "            \"TEMP\": float(sub[\"TMP\"].mean()) if not sub.empty else np.nan,\n",
    "            \"RAIN\": float(sub[\"PCP\"].sum()) if not sub.empty else 0.0,\n",
    "        }\n",
    "\n",
    "    temps = [v[\"TEMP\"] for v in out.values() if not pd.isna(v[\"TEMP\"])]\n",
    "    fill_temp = float(np.mean(temps)) if temps else 0.0\n",
    "    for h in out:\n",
    "        if pd.isna(out[h][\"TEMP\"]):\n",
    "            out[h][\"TEMP\"] = fill_temp\n",
    "\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# (ìœ í‹¸) ë™ ì…ë ¥ -> í•´ë‹¹ ë™ì˜ ê¸°ì˜¨ ë°˜í™˜\n",
    "# =========================\n",
    "def get_temperature_by_dong(\n",
    "    date_yyyymmdd: str,\n",
    "    dong: str,\n",
    "    slot_hour: int = 5,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> float:\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    if not (1 <= int(slot_hour) <= 10):\n",
    "        raise ValueError(\"slot_hourëŠ” 1~10 ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    temp = float(weather_by_hour[int(slot_hour)][\"TEMP\"])\n",
    "    return round(temp, 1)\n",
    "\n",
    "# =========================\n",
    "# ëª¨ë¸ ë¡œë“œ + \"ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì»¬ëŸ¼\" ìë™ ë§ì¶¤\n",
    "# =========================\n",
    "def load_best_model_map():\n",
    "    if not os.path.exists(BEST_MAP_PATH):\n",
    "        raise FileNotFoundError(f\"ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {BEST_MAP_PATH}\")\n",
    "    df = pd.read_csv(BEST_MAP_PATH)\n",
    "    required_cols = {\"HOUR\", \"BEST_MODEL\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"{BEST_MAP_PATH}ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í•„ìš”: {required_cols}, í˜„ì¬: {set(df.columns)}\")\n",
    "    return df.set_index(\"HOUR\")[\"BEST_MODEL\"].to_dict()\n",
    "\n",
    "def load_best_bundle(hour: int):\n",
    "    path = BUNDLE_PATH_TEMPLATE.format(hour=hour)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"ë²ˆë“¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
    "    bundle = joblib.load(path)\n",
    "    if not isinstance(bundle, dict) or \"preprocess\" not in bundle or \"model\" not in bundle:\n",
    "        raise ValueError(f\"ë²ˆë“¤ í¬ë§·ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {path}\")\n",
    "    return bundle\n",
    "\n",
    "def get_required_feature_names_from_preprocess(preprocess) -> list:\n",
    "    names = []\n",
    "    for _, _, cols in getattr(preprocess, \"transformers_\", []):\n",
    "        if cols is None:\n",
    "            continue\n",
    "        if isinstance(cols, (list, tuple, np.ndarray, pd.Index)):\n",
    "            names.extend(list(cols))\n",
    "    return sorted(set([str(x) for x in names]))\n",
    "\n",
    "def align_row_to_required_columns(X_row: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
    "    X = X_row.copy()\n",
    "\n",
    "    if required_cols:\n",
    "        drop_cols = [c for c in X.columns if c not in required_cols]\n",
    "        if drop_cols:\n",
    "            X = X.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        for c in required_cols:\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "\n",
    "        X = X[required_cols]\n",
    "\n",
    "    return X\n",
    "\n",
    "def predict_with_bundle(bundle: dict, X_new: pd.DataFrame) -> np.ndarray:\n",
    "    pre = bundle[\"preprocess\"]\n",
    "    model = bundle[\"model\"]\n",
    "    X_enc = pre.transform(X_new)\n",
    "    return model.predict(X_enc)\n",
    "\n",
    "# =========================\n",
    "# âœ… í•µì‹¬: ë‚ ì§œ 1ê°œ â†’ 1~10ì‹œê°„ëŒ€ \"ë§¤ì¶œ(AMT)\" ì˜ˆì¸¡\n",
    "# =========================\n",
    "def predict_sales_all_hours_with_weather(\n",
    "    date_yyyymmdd: str,\n",
    "    nx: int,\n",
    "    ny: int,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    if not service_key:\n",
    "        raise ValueError(\"SERVICE_KEY(RAIN_ID)ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    day_code = compute_day_code(date_yyyymmdd)\n",
    "\n",
    "    hourly_weather = fetch_hourly_tmp_pcp_for_date(service_key, date_yyyymmdd, nx, ny)\n",
    "    weather_by_hour = aggregate_weather_to_item_slots(hourly_weather)\n",
    "\n",
    "    best_map = load_best_model_map()\n",
    "    rows = []\n",
    "\n",
    "    for hour in range(1, 11):\n",
    "        if hour not in best_map:\n",
    "            continue\n",
    "\n",
    "        bundle = load_best_bundle(hour)\n",
    "        best_name = str(best_map[hour])\n",
    "\n",
    "        row = {}\n",
    "        row.update(make_date_features(date_yyyymmdd))\n",
    "        row.update({\n",
    "            \"HOUR\": int(hour),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "            \"DELIV_TEMP\": float(weather_by_hour[hour][\"TEMP\"]),\n",
    "            \"DELIV_RAIN\": float(weather_by_hour[hour][\"RAIN\"]),\n",
    "        })\n",
    "\n",
    "        X_row = pd.DataFrame([row])\n",
    "        X_row = X_row.drop(columns=[c for c in DROP_COLS if c in X_row.columns], errors=\"ignore\")\n",
    "\n",
    "        required = get_required_feature_names_from_preprocess(bundle[\"preprocess\"])\n",
    "        X_row_aligned = align_row_to_required_columns(X_row, required)\n",
    "\n",
    "        pred = float(predict_with_bundle(bundle, X_row_aligned)[0])\n",
    "        if USE_LOG_TARGET:\n",
    "            pred = float(np.expm1(pred))\n",
    "\n",
    "        pred = max(0.0, pred)  # ìŒìˆ˜ ë°©ì§€\n",
    "\n",
    "        rows.append({\n",
    "            \"TA_YMD\": str(date_yyyymmdd),\n",
    "            \"DAY\": int(day_code),\n",
    "            \"HOUR\": int(hour),\n",
    "            \"HOUR_LABEL\": HOUR_LABELS.get(int(hour), str(hour)),\n",
    "            \"BEST_MODEL\": best_name,\n",
    "            \"TEMP\": round(float(row[\"TEMP\"]), 2),\n",
    "            \"RAIN\": float(row[\"RAIN\"]),\n",
    "            \"PRED_AMT\": int(round(pred)),\n",
    "            \"KMA_BASE_USED\": str(hourly_weather.loc[0, \"base_used\"]),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"HOUR\").reset_index(drop=True)\n",
    "\n",
    "    # âœ… (ì¶”ê°€1) í•˜ë£¨ ì´ë§¤ì¶œ(1~10 í•©)\n",
    "    total_amt = int(df[\"PRED_AMT\"].sum()) if not df.empty else 0\n",
    "    df[\"PRED_AMT_TOTAL_DAY\"] = total_amt  # ëª¨ë“  í–‰ì— ë™ì¼ ê°’ìœ¼ë¡œ ë“¤ì–´ê°(ì¡°íšŒ í¸ì˜)\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# âœ… í•µì‹¬: ì‚¬ìš©ì DONG ì…ë ¥ ë²„ì „\n",
    "# =========================\n",
    "def predict_sales_all_hours_with_weather_by_dong(\n",
    "    date_yyyymmdd: str,\n",
    "    dong: str,\n",
    "    service_key: str = SERVICE_KEY\n",
    ") -> pd.DataFrame:\n",
    "    nx, ny = find_grid_by_dong(dong)\n",
    "    df_pred = predict_sales_all_hours_with_weather(\n",
    "        date_yyyymmdd=date_yyyymmdd,\n",
    "        nx=nx,\n",
    "        ny=ny,\n",
    "        service_key=service_key\n",
    "    )\n",
    "    df_pred.insert(1, \"DONG\", str(dong))\n",
    "    return df_pred\n",
    "\n",
    "# =========================\n",
    "# âœ… (ì¶”ê°€2) ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•˜ëŠ” í”„ë¦°íŠ¸ í•¨ìˆ˜\n",
    "# =========================\n",
    "def print_sales_prediction(df_pred: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    predict_sales_all_hours_with_weather_* ê²°ê³¼(DataFrame)ë¥¼\n",
    "    ì‹œê°„ëŒ€ë³„ + í•˜ë£¨ ì´ë§¤ì¶œê¹Œì§€ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    if df_pred is None or df_pred.empty:\n",
    "        print(\"ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    date_yyyymmdd = str(df_pred.loc[0, \"TA_YMD\"])\n",
    "    dong = str(df_pred.loc[0, \"DONG\"]) if \"DONG\" in df_pred.columns else \"-\"\n",
    "    day_code = int(df_pred.loc[0, \"DAY\"])\n",
    "    total_amt = int(df_pred.loc[0, \"PRED_AMT_TOTAL_DAY\"]) if \"PRED_AMT_TOTAL_DAY\" in df_pred.columns else int(df_pred[\"PRED_AMT\"].sum())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š ë§¤ì¶œ(AMT) ì˜ˆì¸¡ ê²°ê³¼\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸ“ì§€ì—­(ë™): {dong}\")\n",
    "    print(f\"ğŸ“…ë‚ ì§œ: {date_yyyymmdd} (DAYì½”ë“œ={day_code})\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # ì‹œê°„ëŒ€ë³„ í‘œ í˜•íƒœ ì¶œë ¥\n",
    "    # (ì‹œê°„ëŒ€ / ê¸°ì˜¨ / ê°•ìˆ˜ / ì˜ˆì¸¡ë§¤ì¶œ)\n",
    "    for _, r in df_pred.sort_values(\"HOUR\").iterrows():\n",
    "        hour = int(r[\"HOUR\"])\n",
    "        label = str(r.get(\"HOUR_LABEL\", hour))\n",
    "        temp = float(r.get(\"TEMP\", np.nan))\n",
    "        rain = float(r.get(\"RAIN\", 0.0))\n",
    "        pred_amt = int(r.get(\"PRED_AMT\", 0))\n",
    "\n",
    "        print(f\"â° {hour:02d} ({label}) | ğŸŒ¡ï¸ {temp:5.1f}Â°C | ğŸŒ§ï¸ {rain:6.1f}mm | ğŸ’° {pred_amt:>10,} ì›\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"âœ… í•˜ë£¨ ì´ë§¤ì¶œ(1~10í•©): ğŸ’° {total_amt:>12,} ì›\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# =========================\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df_pred = predict_sales_all_hours_with_weather_by_dong(\n",
    "        date_yyyymmdd=\"20251231\",\n",
    "        dong=\"ì •ì2ë™\"\n",
    "    )\n",
    "\n",
    "    # DataFrame ì¶œë ¥\n",
    "    print(df_pred)\n",
    "\n",
    "    # ë³´ê¸° ì¢‹ê²Œ ì½˜ì†” ì¶œë ¥\n",
    "    print_sales_prediction(df_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp (ipykernel)",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
